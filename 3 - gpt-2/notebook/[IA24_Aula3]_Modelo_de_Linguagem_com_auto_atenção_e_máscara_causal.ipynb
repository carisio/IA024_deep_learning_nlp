{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "# Exercício: Modelo de Linguagem com auto-atenção e máscaras causais\n",
        "\n",
        "Seguimos na mesma linha de treinar um modelo de linguagem a partir dos textos do livro \"O Guarani\", de José de Alencar.\n",
        "\n",
        "Neste exercício, vamos treinar um modelo de linguagem com auto-atenção e com máscara causal. A máscara causal é necessária para que o modelo não tenha acesso a palavras futuras, que é a abordagem usada por grandes modelos de linguagem, como o GPT.\n",
        "\n",
        "Use a implementação matricial de auto-atenção da aula passada.\n",
        "\n",
        "### Modificações necessárias\n",
        "\n",
        "* Adicione a máscara causal na função `forward` da cabeça de auto-atenção.\n",
        "* Modifique o nosso dataloader para retornar inputs (uma lista de tokens de tamanho $n$), targets (uma lista de tokens de tamanho $n$ deslocada para a esquerda em 1 token). Exemplo `input = [1, 2, 3, 4]`, `target = [2, 3, 4, 5]` para a sequência `[1, 2, 3, 4, 5]` com `seq_len=4`, por exemplo (Ver slide 50).\n",
        "\n",
        "### Extra\n",
        "* MultiHeadAttention: modifique a cabeça de auto-atenção para ter múltiplas cabeças. Isso não é obrigatório, mas pode ser interessante para ver como o modelo se comporta.\n",
        "* Diagrama da geração: fazer diagrama que mostre os passos da geração de tokens (conforme slide 47).\n",
        "\n",
        "### Dicas\n",
        "\n",
        "* Use como base o vídeo do Karpathy: https://www.youtube.com/watch?v=kCc8FmEb1nY. Observe que, no vídeo, ele primeiro implementa um modelo bi-grama, depois um modelo de linguagem com auto-atenção. O modelo de auto-atenção é implementado por volta do minuto 40, mas vale a pena assistir o vídeo todo.\n",
        "* Use esta implementação como base: https://colab.research.google.com/drive/1vFTg4MSXVJwNSzPjaCcvmqhxTP7gK7HA?usp=sharing. Observe como o modelo é organizado e como a máscara é implementada na classe MultiHeadAttention.\n",
        "* Use `context_size=9`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz4v3ubPrqk1"
      },
      "source": [
        "------------------------\n",
        "\n",
        "Observações: A resolução do exercício foi baseada nos notebooks implementados nas aulas anteriores e no vídeo do Karpathy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJnCrVMJhdra"
      },
      "source": [
        "## Parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uXjTy-hrhe_n"
      },
      "outputs": [],
      "source": [
        "# Livros (O Guarani)\n",
        "urls = [\"https://www.gutenberg.org/ebooks/67724.txt.utf-8\", \"https://www.gutenberg.org/ebooks/67725.txt.utf-8\"]\n",
        "\n",
        "# Dados do vocabulário\n",
        "UNK = \"<unk>\"\n",
        "vocab_size_desejado_sem_UNK = 10000 # Não considera o UNK\n",
        "vocab_size = vocab_size_desejado_sem_UNK + 1\n",
        "\n",
        "# Hiperparâmetros\n",
        "# Esse conjunto faz o stride (step_inputs) de 1 em 1 na hora de gerar os dados de treinamento\n",
        "# Entretanto, ele não usa train_test_split. Ele quebra o livro em um certo ponto e usa os 80% primeiros\n",
        "# para treinamento e o resto de teste, de forma que não tenha muita sobreposição de texto\n",
        "#\n",
        "# PPL = 71 na época 3 (VOCAB = 3000)\n",
        "# PPL = 145 na época 2 (VOCAB = 10000)\n",
        "usar_train_test_split = False # Se True, usa train_test_split. Se False, separa o conjunto de treino e teste como uma posição do livro\n",
        "context_size = 9  # número de palavras de entrada. O target é a próxima palavra\n",
        "step_inputs = 1 # Se colocar < context_size e usar_train_test_split = True, vai ter sobreposição de sequências longas no treinamento e validação\n",
        "num_epochs = 5\n",
        "test_size = 0.2\n",
        "batch_size=256\n",
        "n_embd = 64\n",
        "dropout = 0.2\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "lr = 1e-3\n",
        "seed = 18\n",
        "\n",
        "# Usa train_test_split e considera o step_inputs = context_size. Isso vai diminuir o\n",
        "# tamanho do conjunto de treinamento. É necessário reduzir o batch um pouco.\n",
        "# PPL = 182 na época 7 (VOCAB = 10000)\n",
        "# usar_train_test_split = True\n",
        "# context_size = 9\n",
        "# step_inputs = context_size\n",
        "# num_epochs = 10\n",
        "# test_size = 0.2\n",
        "# batch_size=64\n",
        "# n_embd = 64\n",
        "# dropout = 0.2\n",
        "# n_head = 4\n",
        "# n_layer = 6\n",
        "# lr = 1e-3\n",
        "# seed = 18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset\n",
        "\n",
        "Faz o download dos arquivos do projeto Gutenberg, extrai o conteúdo principal e armazena os parágrafos na variável `paragrafos` (lista). Em seguida, concatena os parágrafos com `\\n` em uma única variável `livro`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_UzC9pV091C",
        "outputId": "b9f76f6f-5305-406e-f56e-a7390a139db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- Livro 1 ---------------\n",
            "Publicando este livro em 1857, se disse ser aquella primeira edição uma prova typographica, que algum dia talvez o autor se dispuzesse a rever.\n",
            "Esta nova edição devia dar satisfação do empenho, que a extrema benevolencia do publico ledor, tão minguado ainda, mudou em bem para divida de reconhecimento.\n",
            "Mais do que podia fiou de si o autor. Relendo a obra depois de annos, achou elle tão mau e incorrecto quando escrevera, que para bem corrigir, fora mister escrever de novo. Para tanto lhe carece o tempo e sobra o tedio de um labor ingrato.\n",
            "Cingio-se pois ás pequenas emendas que toleravão o plano da obra e o desalinho de um estylo não castigado.\n",
            "PRIMEIRA PARTE\n",
            "OS AVENTUREIROS\n",
            "De um dos cabeços da _Serra dos Órgãos_ deslisa um fio d'agua que se dirige para norte, e engrossado com os mananciaes, que recebe no seu curso de dez leguas, torna-se rio caudal.\n",
            "É o _Paquequer_: soltando de cascata em cascata, enroscando-se como uma serpente, vai depois se espreguiçar na varzea e embeber no Parahyba, que rola magestosamente em seu vasto leito.\n",
            "Dir-se-hia que vassallo e tributario desse rei das aguas, o pequeno rio, altivo e sobranceiro contra os rochedos, curva-se humildemente aos pés do suzerano. Perde então a belleza selvatica; suas ondas são calmas e serenas como as de um lago, e não se revoltão contra os barcos e as canôas que resvalão sobre ellas: escravo submisso, soffre o latego do senhor.\n",
            "Não é neste lugar que elle deve ser visto; sim tres ou quatro leguas acima de sua foz, onde é livre ainda, como o filho indomito desta patria da liberdade.\n",
            "Ahi, o _Paquequer_ lança-se rapido sobre o seu leito, e atravessa as florestas como o tapir, espumando, deixando o pello esparso pelas pontas de rochedo, e enchendo a solidão com o estampido de sua carreira. De repente, falta-lhe o espaço, foge-lhe a terra; o soberbo rio recúa um momento para concentrar as suas forças e precipita-se de um só arremesso, como o tigre sobre a presa.\n",
            "Depois, fatigado do esforço supremo, se estende sobre a terra, e adormece n'uma linda bacia que a natureza formou, e onde o recebe como em um leito de noiva, sob as cortinas de trepadeiras e flores agrestes.\n",
            "A vegetação nessas paragens ostentava outr'ora todo o seu luxo e vigor; florestas virgens se estendião ao longo das margens do rio, que corria no meio das arcarias de verdura e dos capiteis formados pelos leques das palmeiras.\n",
            "Tudo era grande e pomposo no scenario que a natureza, sublime artista, tinha decorado para os dramas magestosos dos elementos, em que o homem é apenas um simples comparsa.\n",
            "No anno do graça de 1604, o lugar que acabamos de descrever estava deserto e inculto; a cidade do Rio de Janeiro tinha-se fundado havia menos de meio seculo, e a civilisação não tivera tempo de penetrar o interior.\n",
            "Entretanto, via-se á margem direita do rio uma casa larga e espaçosa, construida sobre uma eminencia, e protegida de todos os lados por uma muralha de rocha cortada a pique.\n",
            "A esplanada, sobre que estava assentado o edificio, formava um semicirculo irregular que teria quando muito cincoenta braças quadradas: do lado do norte havia uma especie de escada de lagedo feita metade pela natureza e metade pela arte.\n",
            "Descendo dous ou tres dos largos degráos de pedra da escada, encontrava-se uma ponte de madeira solidamente construida sobre uma fenda larga e profunda que se abria na rocha. Continuando a descer, chegava-se á beira do rio, que se curvava em seio gracioso; sombreado pelas grandes gameleiras e angelins que crescião ao longo das margens.\n",
            "Ahi, ainda a industria do homem tinha aproveitado habilmente a natureza para crear meios de segurança e defeza.\n",
            "De um e outro lado da escada seguião dous renques de arvores, que, alargando gradualmente, ião fechar como dous braços o seio do rio; entre o tronco dessas arvores, uma alta cerca de espinheiros tornava aquelle pequeno valle impenetravel.\n",
            "-------------- Livro 2 ---------------\n",
            "TERCEIRA PARTE\n",
            "Na segunda-feira, erão seis horas da manhã, quando D. Antonio de Mariz chamou seu filho.\n",
            "O velho fidalgo velara uma boa parte da noite; ou escrevendo ou reflectindo sobre os perigos que ameaçavão sua familia.\n",
            "Pery lhe havia contado todas as particularidades de seu encontro com os Aymorés; e o cavalheiro, que conhecia a ferocidade e espirito vingativo dessa raça selvagem, esperava a cada momento ser atacado.\n",
            "Por isso, de acordo com Alvaro, D. Diogo, com seu escudeiro Ayres Gomes, tinha tomado todas as medidas de precaução que as circumstancias e sua longa experiencia lhe aconselhavão.\n",
            "Quando seu filho entrou, o velho fidalgo acabava de sellar duas cartas que escrevêra na vespera.\n",
            "--Meu filho, disse elle com uma ligeira emoção, reflecti essa noite sobre o que nos pode acontecer, e assentei que deves partir hoje mesmo para S. Sebastião.\n",
            "--Não é possivel, senhor!... Afastais-me de vós justamente quando correis um perigo?\n",
            "--Sim! É justamente quando um grande perigo nos ameaça, que eu, chefe da casa, entendo ser do meu dever salvar o representante do meu nome e meu herdeiro legitimo, o protector de minha familia orphã.\n",
            "--Confio em Deos, meu pai, que vossos receios serão infundados; mas se elle nos quizesse submetter a tal provança, o unico lugar que compete a vosso filho e herdeiro de vosso nome é nesta casa ameaçada, ao vosso lado, para defender-vos e partilhar a vossa sorte, qualquer que ella seja.\n",
            "D. Antonio apertou seu filho ao peito.\n",
            "--Eu te reconheço; tu és meu filho; é o meu sangue juvenil que gyra em tuas veias, e o meu coração de moço que falla pelos teus labios. Deixa porém que os cincoenta annos de experiencia que desde então passárão sobre minha cabeça encanecida te ensinem o que vai da mocidade á velhice, o que vai do ardente cavalheiro ao pai de uma familia.\n",
            "--Eu vos escuto, senhor; mas pelo amor que vos consagro poupai-me a dôr e a vergonha de deixar-vos no momento em que mais precisais de um servidor fiel e dedicado.\n",
            "O fidalgo proseguio já calmo:\n",
            "--Não é uma espada, D. Diogo, que nos dará a victoria, fosse ella valente e forte como a vossa: entre quarenta combatentes que vão se medir talvez contra centenas e centenas de inimigos, um de mais ou de menos não importa ao resultado.\n",
            "--Que assim seja, respondeu o cavalheiro com energia; reclamo o meu posto de honra, e a minha parte do perigo; não vos ajudarei a vencer, porém morrerei junto dos meus.\n",
            "--E é por esse nobre mas esteril orgulho que quereis sacrificar o unico meio de salvação que talvez nos reste, se, como temo, as minhas previsões se realisarem?\n",
            "--Que dizeis, senhor?\n",
            "--Qualquer que seja a força e o numero de inimigos, conto que o valor portuguez e a posição desta casa me ajudarão a resistir-lhes por algum tempo, por vinte dias, mesmo por um mez; mas por fim teremos de succumbir.\n",
            "--Então?... exclamou D. Diogo pallido.\n",
            "-------------- -------------------------\n",
            "Total de parágrafos: 4576\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# Retorna True ou False dependendo se é um parágrafo válido ou não.\n",
        "# Usado para remover parágrafos muito curtos ou outros casos específicos\n",
        "# (por exemplo, os parágrafos do índice que tem vários pontos (.....))\n",
        "def paragrafo_valido(paragrafo):\n",
        "  return len(paragrafo) > 10 and '....' not in paragrafo and '***' not in paragrafo\n",
        "\n",
        "# Dada uma URL do projeto Gutenberg, baixa o arquivo e divide o texto\n",
        "# principal em parágrafos. Retorna um array de parágrafos\n",
        "def carregar_paragrafos_livro(url, n_linhas_para_print=20):\n",
        "  # Baixar o arquivo\n",
        "  response = requests.get(url)\n",
        "  texto = response.text\n",
        "\n",
        "  # Encontrar o início e o fim do conteúdo principal do livro\n",
        "  #inicio = texto.find(\"*** START OF THE PROJECT GUTENBERG EBOOK\")\n",
        "  str_inicio = \"Ficão reservados os direitos de propriedade.\"\n",
        "  str_fim = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "  inicio = texto.find(str_inicio)\n",
        "  fim = texto.find(str_fim)\n",
        "\n",
        "  # Extrair o conteúdo principal do livro\n",
        "  conteudo = texto[inicio+len(str_inicio):fim].replace('\\r','')\n",
        "\n",
        "  # Dividir o conteúdo em parágrafos e processar o conteúdo\n",
        "  paragrafos = []\n",
        "\n",
        "  # Cada parágrafo é separado por dois \\n\n",
        "  # Dentro de cada parágrafo, junta as linhas e remove espaços em branco no\n",
        "  # início e fim do prágrafo.\n",
        "  for paragrafo in conteudo.split(\"\\n\\n\"):\n",
        "    paragrafo = paragrafo.replace('\\n', ' ').strip()\n",
        "    if paragrafo_valido(paragrafo):\n",
        "      paragrafos.append(paragrafo)\n",
        "\n",
        "  # Imprime (usado pra debug)\n",
        "  for p in paragrafos[0:n_linhas_para_print]:\n",
        "    print(p)\n",
        "\n",
        "  return paragrafos\n",
        "\n",
        "paragrafos = []\n",
        "for i, url in enumerate(urls, 1):\n",
        "  print(f'-------------- Livro {i} ---------------')\n",
        "  paragrafos.extend(carregar_paragrafos_livro(url))\n",
        "\n",
        "print('-------------- -------------------------')\n",
        "print(f'Total de parágrafos: {len(paragrafos)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cakfIe7_hrL2",
        "outputId": "38861241-e797-4a9a-95ae-180dce877584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Publicando este livro em 1857, se disse ser aquella primeira edição uma prova typographica, que algum dia talvez o autor se dispuzesse a rever.\n",
            "Esta nova edição devia dar satisfação do empenho, que a extrema benevolencia do publico ledor, tão minguado ainda, mudou em bem para divida de reconhecimento.\n",
            "Mais do que podia fiou de si o autor. Relendo a obra depois de annos, achou elle tão mau e incorrecto quando escrevera, que para bem corrigir, fora mister escrever de novo. Para tanto lhe carece o tempo e sobra o tedio de um labor ingrato.\n",
            "Cingio-se pois ás pequenas emendas que toleravão o plano da obra e o desalinho de um estylo não castigado.\n",
            "PRIMEIRA PARTE\n",
            "OS AVENTUREIROS\n",
            "De um dos cabeços da _Serra dos Órgãos_ deslisa um fio d'agua que se dirige para norte, e engrossado com os mananciaes, que recebe no seu curso de dez leguas, torna-se rio caudal.\n",
            "É o _Paquequer_: soltando de cascata em cascata, enroscando-se como uma serpente, vai depois se espreguiçar na varzea e embeber no Parahyba\n"
          ]
        }
      ],
      "source": [
        "livro = \"\\n\".join(paragrafos)\n",
        "\n",
        "print(livro[0:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFVN2ihb33Rf"
      },
      "source": [
        "## Análise do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vusjPi_3jUeN"
      },
      "source": [
        "Primeiro define um tokenizador que vai separar as palavras para a entrada no dataset. A expressão regular deve capturar palavras, sinais de pontuação e quebras de linha. As quebras de linha são necessárias porque, ao final, quero que o modelo de linguagem também escreva parágrafos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUxGwQV_jUCM",
        "outputId": "504958d8-a274-4dca-86e6-193ae16b42c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['teste', '.', 'será', 'que', 'vai', 'manter', 'a', 'pontuação', '?', '\\n', 'e', 'os', 'espaços', 'em', 'branco', '?', '\\n', 'dúvidas', 'SUBSTITUIRPORTRESPONTOS']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def tokenizar(texto):\n",
        "  texto = texto.lower()\n",
        "\n",
        "  # Força os 3 pontos aparecerem juntos\n",
        "  texto = texto.replace('...', ' SUBSTITUIRPORTRESPONTOS')\n",
        "\n",
        "  # Define a expressão regular que captura palavras, sinais de pontuação e quebras de linha\n",
        "  padrao = r'\\b\\w+\\b|[^\\w\\s]|[\\n]'\n",
        "\n",
        "  # Usa o método findall para encontrar todas as ocorrências que se encaixam no padrão\n",
        "  tokens = re.findall(padrao, texto)\n",
        "\n",
        "  return tokens\n",
        "\n",
        "print(tokenizar('Teste. Será que vai manter a pontuação?\\nE os espaços em branco?\\nDúvidas...'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpkLQdklkV2b"
      },
      "source": [
        "Contador de palavras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "outputId": "41b4087e-2731-4719-bd04-a47927749f55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de tokens 11870\n",
            "1: , -> 7383\n",
            "2: - -> 5932\n",
            "3: . -> 5211\n",
            "4: \n",
            " -> 4575\n",
            "5: a -> 4461\n",
            "6: que -> 4336\n",
            "7: o -> 4055\n",
            "8: de -> 3951\n",
            "9: e -> 3605\n",
            "10: se -> 2398\n",
            "11: ; -> 2355\n",
            "12: um -> 1710\n",
            "13: do -> 1404\n",
            "14: não -> 1275\n",
            "15: uma -> 1249\n",
            "16: da -> 1133\n",
            "17: os -> 1115\n",
            "18: com -> 1015\n",
            "19: sua -> 924\n",
            "20: para -> 857\n",
            "21: seu -> 777\n",
            "22: ! -> 735\n",
            "23: em -> 724\n",
            "24: pery -> 718\n",
            "25: as -> 702\n",
            "26: no -> 640\n",
            "27: por -> 621\n",
            "28: ? -> 609\n",
            "29: ao -> 592\n",
            "30: como -> 587\n",
            "31: lhe -> 558\n",
            "32: d -> 491\n",
            "33: á -> 490\n",
            "34: tinha -> 478\n",
            "35: era -> 462\n",
            "36: cecilia -> 457\n",
            "37: na -> 452\n",
            "38: é -> 440\n",
            "39: : -> 432\n",
            "40: sobre -> 415\n",
            "41: mas -> 410\n",
            "42: elle -> 406\n",
            "43: dos -> 373\n",
            "44: indio -> 340\n",
            "45: seus -> 324\n",
            "46: me -> 324\n",
            "47: mais -> 318\n",
            "48: antonio -> 303\n",
            "49: quando -> 288\n",
            "50: SUBSTITUIRPORTRESPONTOS -> 281\n"
          ]
        }
      ],
      "source": [
        "# Conta as palavras no dataset\n",
        "from collections import Counter\n",
        "\n",
        "def count_words(text):\n",
        "  word_counts = Counter()\n",
        "  word_counts.update(tokenizar(text))\n",
        "  return word_counts\n",
        "\n",
        "word_counts = count_words(livro)\n",
        "\n",
        "print('Total de tokens', len(word_counts))\n",
        "\n",
        "top_tokens = word_counts.most_common(50)\n",
        "for i, token_num_ocorrencia in enumerate(top_tokens, 1):\n",
        "  print(f'{i}: {token_num_ocorrencia[0]} -> {token_num_ocorrencia[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y1ZWTeLllCLD"
      },
      "outputs": [],
      "source": [
        "def gerar_vocabulario(texto, vocab_size_sem_UNK):\n",
        "  counter = count_words(texto)\n",
        "\n",
        "  # Considera apenas as palavras mais frequentes. Adiciona, na posição 0, o token UNK\n",
        "  most_frequent_words = [UNK] + [word for word, count in counter.most_common(vocab_size_sem_UNK)]\n",
        "  # vocab é um mapa de palavras para o índice correspondente. O mapa leva a palavra para um índice entre [0, vocab_size]\n",
        "  # (o tamanho é vocab_size + 1), com o índice 0 apontando para UNK\n",
        "  vocab = {word: i for i, word in enumerate(most_frequent_words)}\n",
        "\n",
        "  return len(most_frequent_words), vocab, most_frequent_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkZ_3EkClf3F",
        "outputId": "7ef381a5-b957-440d-dad0-efc36cbc7cf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho do vocabulário (considera UNK):  10001\n",
            "Posição 0:  <unk>\n",
            "Índice do UNK:  0\n",
            "------------\n",
            "Posição 4:  \n",
            "\n",
            "Índice de \n",
            ":  4\n"
          ]
        }
      ],
      "source": [
        "vocab_size, vocab, most_frequent_words = gerar_vocabulario(livro, vocab_size_desejado_sem_UNK)\n",
        "\n",
        "print('Tamanho do vocabulário (considera UNK): ', vocab_size)\n",
        "\n",
        "print('Posição 0: ', most_frequent_words[0])\n",
        "print('Índice do UNK: ', vocab[UNK])\n",
        "print('------------')\n",
        "pos = 4 # a quarta maior ocorrência é o \\n\n",
        "print(f'Posição {pos}: ', most_frequent_words[pos])\n",
        "print(f'Índice de {most_frequent_words[pos]}: ', vocab[most_frequent_words[pos]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RhbhAsZbzJ_J"
      },
      "outputs": [],
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "  # Obs.: tem que usar o mesmo tokenizador que foi gerado o vocabulário\n",
        "  return [vocab.get(word, 0) for word in tokenizar(sentence)]\n",
        "\n",
        "def decode_sentence(sentence, most_frequent_words):\n",
        "  words = [most_frequent_words[code] for code in sentence]\n",
        "  decoded_sentence = ' '.join(words)\n",
        "  decoded_sentence = decoded_sentence.replace('SUBSTITUIRPORTRESPONTOS', '...')\n",
        "  decoded_sentence = decoded_sentence.replace(' \\n ', '\\n')\n",
        "  return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc1Su1PlmEma",
        "outputId": "772936de-92b4-46e1-beff-4fff8dfb4a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:\n",
            "Publicando este livro em 1857, se disse ser aquella primeira edição uma prova typographica, que algum dia talvez o autor se dispuzesse a rever.\n",
            "Esta nova edição devia dar satisfação do empenho, que a \n",
            "Encodada:\n",
            "[5872, 141, 4022, 23, 5873, 1, 10, 52, 111, 263, 264, 4023, 15, 981, 5874, 1, 6, 182, 131, 273, 7, 2040, 10, 5875, 5, 2434, 3, 4, 121, 1157, 4023, 173, 230, 917, 13, 5876, 1, 6, 5]\n",
            "Reconstruída:\n",
            "publicando este livro em 1857 , se disse ser aquella primeira edição uma prova typographica , que algum dia talvez o autor se dispuzesse a rever .\n",
            "esta nova edição devia dar satisfação do empenho , que a\n",
            "--------------------------------------\n",
            "Original:\n",
            "pery disse que amava cecilia\n",
            "Encodada:\n",
            "[24, 52, 6, 548, 36]\n",
            "Reconstruída:\n",
            "pery disse que amava cecilia\n"
          ]
        }
      ],
      "source": [
        "# Teste do encode/decode\n",
        "frase = livro[0:200]\n",
        "\n",
        "frase_encodada = encode_sentence(frase, vocab)\n",
        "frase_reconstruida = decode_sentence(frase_encodada, most_frequent_words)\n",
        "\n",
        "print('Original:')\n",
        "print(frase)\n",
        "print('Encodada:')\n",
        "print(frase_encodada)\n",
        "print('Reconstruída:')\n",
        "print(frase_reconstruida)\n",
        "print(\"--------------------------------------\")\n",
        "\n",
        "frase = \"pery disse que amava cecilia\"\n",
        "\n",
        "frase_encodada = encode_sentence(frase, vocab)\n",
        "frase_reconstruida = decode_sentence(frase_encodada, most_frequent_words)\n",
        "\n",
        "print('Original:')\n",
        "print(frase)\n",
        "print('Encodada:')\n",
        "print(frase_encodada)\n",
        "print('Reconstruída:')\n",
        "print(frase_reconstruida)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_QqmM5ZrnPt"
      },
      "source": [
        "Inicialmente, convertemos o livro para um array de tokens para, em seguida, gerar os conjuntos de treinamento e validação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g98yEfAkrQ8f",
        "outputId": "ace89776-22ef-4109-a91d-d93513101e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([139177]) torch.int64\n",
            "tensor([5872,  141, 4022,   23, 5873,    1,   10,   52,  111,  263,  264, 4023,\n",
            "          15,  981, 5874,    1,    6,  182,  131,  273,    7, 2040,   10, 5875,\n",
            "           5, 2434,    3,    4,  121, 1157, 4023,  173,  230,  917,   13, 5876,\n",
            "           1,    6,    5,  727, 3036,   13, 5877, 5878,    1,  108, 5879,   70,\n",
            "           1, 3037,   23,  134,   20, 2435,    8,  918,    3,    4,   47,   13,\n",
            "           6,  110, 5880,    8,  241,    7, 2040,    3, 5881,    5,  728,   64,\n",
            "           8,  471,    1, 1068,   42,  108, 5882,    9, 5883,   49, 5884,    1,\n",
            "           6,   20,  134, 5885,    1,  668, 5886, 4024,    8,  184,    3,   20,\n",
            "         308,   31, 5887,    7])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode_sentence(livro, vocab), dtype=torch.long)\n",
        "\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jchcN9oqt3Xj"
      },
      "source": [
        "Na construção do Dataset nos modelos de linguagem implementados nas aulas anteriores, cada frase de entrada (input) tinha apenas um target. Por exemplo, na frase \"eu gosto de pizza\", se a entrada for [\"eu\", \"gosto\", \"de\"], o target será \"pizza\".\n",
        "\n",
        "No caso dos Transformers, o target também é um array. Na frase \"eu gosto de pizza\", se o input for [\"eu\", \"gosto\", \"de\"], o target será [\"gosto\", \"de\", \"pizza\"]. Isso ocorre pois os inputs são variáveis. O target i se refere a todos os inputs de 0 a i. Assim, quando a máscara causal é inserida, \"gosto\" se refere a [\"eu\"], \"de\" se refere a [\"eu\", \"gosto\"] e assim por diante.\n",
        "\n",
        "A parte do janelamento continua. Dessa forma, em uma janela grande a gente tem algo mais ou menos assim:\n",
        "\n",
        "Frase: [\"eu\", \"gosto\", \"de\", pizza\", \"de\", \"calabresa\", \"e\", \"portuguesa\"]\n",
        "\n",
        "input -> target\n",
        "\n",
        "[\"eu\", \"gosto\", \"de\"] -> [pizza\", \"de\", \"calabresa\"]\n",
        "\n",
        "[\"gosto\", \"de\", \"pizza\"] -> [\"de\", \"calabresa\", \"e\"]\n",
        "\n",
        "[\"de\", \"pizza\", \"de\"] -> [\"calabresa\", \"e\", \"portuguesa\"]\n",
        "\n",
        "\n",
        "Nota: No código abaixo, isso é gerado dessa forma se step_inputs = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XXYPr66ys9HX"
      },
      "outputs": [],
      "source": [
        "def gera_inputs_e_targets_para_array(data: torch.Tensor, n):\n",
        "  max = len(data)-n\n",
        "  inputs = torch.stack([data[i:i+n] for i in range(0, max, step_inputs)])\n",
        "  targets = torch.stack([data[i:i+n] for i in range(1, max+1, step_inputs)])\n",
        "\n",
        "  return inputs, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrS-kEDov9Do",
        "outputId": "668d06d7-49e0-4038-a952-5545ae630a21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- decoded -------------------- \n",
            "publicando este livro -> este livro em\n",
            "este livro em -> livro em 1857\n",
            "livro em 1857 -> em 1857 ,\n",
            "em 1857 , -> 1857 , se\n",
            "1857 , se -> , se disse\n",
            ", se disse -> se disse ser\n",
            "se disse ser -> disse ser aquella\n",
            "disse ser aquella -> ser aquella primeira\n",
            "ser aquella primeira -> aquella primeira edição\n",
            "aquella primeira edição -> primeira edição uma\n",
            "primeira edição uma -> edição uma prova\n",
            "edição uma prova -> uma prova typographica\n",
            "uma prova typographica -> prova typographica ,\n",
            "prova typographica , -> typographica , que\n",
            "typographica , que -> , que algum\n",
            ", que algum -> que algum dia\n",
            "que algum dia -> algum dia talvez\n",
            "algum dia talvez -> dia talvez o\n",
            "dia talvez o -> talvez o autor\n",
            "talvez o autor -> o autor se\n",
            "o autor se -> autor se dispuzesse\n",
            "autor se dispuzesse -> se dispuzesse a\n",
            "se dispuzesse a -> dispuzesse a rever\n",
            "-------------------- encoded -------------------- \n",
            "[5872, 141, 4022] -> [141, 4022, 23]\n",
            "[141, 4022, 23] -> [4022, 23, 5873]\n",
            "[4022, 23, 5873] -> [23, 5873, 1]\n",
            "[23, 5873, 1] -> [5873, 1, 10]\n",
            "[5873, 1, 10] -> [1, 10, 52]\n",
            "[1, 10, 52] -> [10, 52, 111]\n",
            "[10, 52, 111] -> [52, 111, 263]\n",
            "[52, 111, 263] -> [111, 263, 264]\n",
            "[111, 263, 264] -> [263, 264, 4023]\n",
            "[263, 264, 4023] -> [264, 4023, 15]\n",
            "[264, 4023, 15] -> [4023, 15, 981]\n",
            "[4023, 15, 981] -> [15, 981, 5874]\n",
            "[15, 981, 5874] -> [981, 5874, 1]\n",
            "[981, 5874, 1] -> [5874, 1, 6]\n",
            "[5874, 1, 6] -> [1, 6, 182]\n",
            "[1, 6, 182] -> [6, 182, 131]\n",
            "[6, 182, 131] -> [182, 131, 273]\n",
            "[182, 131, 273] -> [131, 273, 7]\n",
            "[131, 273, 7] -> [273, 7, 2040]\n",
            "[273, 7, 2040] -> [7, 2040, 10]\n",
            "[7, 2040, 10] -> [2040, 10, 5875]\n",
            "[2040, 10, 5875] -> [10, 5875, 5]\n",
            "[10, 5875, 5] -> [5875, 5, 2434]\n"
          ]
        }
      ],
      "source": [
        "# Exemplo de uso da função gera_inputs_e_targets_para_array\n",
        "exemplo = encode_sentence(\"publicando este livro em 1857, se disse ser aquella primeira edição uma prova typographica, que algum dia talvez o autor se dispuzesse a rever\", vocab)\n",
        "\n",
        "inputs, targets = gera_inputs_e_targets_para_array(torch.tensor(exemplo), 3)\n",
        "\n",
        "print('-------------------- decoded -------------------- ')\n",
        "for input_target in zip(inputs, targets):\n",
        "  print(f'{decode_sentence(input_target[0].tolist(), most_frequent_words)} -> {decode_sentence(input_target[1].tolist(), most_frequent_words)}')\n",
        "\n",
        "print('-------------------- encoded -------------------- ')\n",
        "for input_target in zip(inputs, targets):\n",
        "  print(f'{input_target[0].tolist()} -> {input_target[1].tolist()}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if usar_train_test_split:\n",
        "  inputs, targets = gera_inputs_e_targets_para_array(data, context_size)\n",
        "  inputs_train, inputs_val, targets_train, targets_val = train_test_split(inputs, targets, test_size=test_size, random_state=seed)\n",
        "else:\n",
        "  n = (int)((1-test_size)*len(data))\n",
        "  inputs_train, targets_train = gera_inputs_e_targets_para_array(data[:n], context_size)\n",
        "  inputs_val, targets_val = gera_inputs_e_targets_para_array(data[n:], context_size)"
      ],
      "metadata": {
        "id": "5BSLZxRT08iE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs_train[0])\n",
        "print(targets_train[0])\n",
        "print(inputs_train[1])\n",
        "print(targets_train[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_trshI-A1HK5",
        "outputId": "402aab03-2675-4f48-fd5d-4fe528ec9f14"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5872,  141, 4022,   23, 5873,    1,   10,   52,  111])\n",
            "tensor([ 141, 4022,   23, 5873,    1,   10,   52,  111,  263])\n",
            "tensor([ 141, 4022,   23, 5873,    1,   10,   52,  111,  263])\n",
            "tensor([4022,   23, 5873,    1,   10,   52,  111,  263,  264])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1CVci2zJ_J",
        "outputId": "439ca89d-5014-4fac-845f-7beb01fcd596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.35 ms, sys: 25 ms, total: 28.4 ms\n",
            "Wall time: 90.2 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from typing import Union\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GptDataset(Dataset):\n",
        "  def __init__(self, inputs, targets):\n",
        "    # Salva o vocabulário\n",
        "    self.vocab = vocab\n",
        "\n",
        "    # Agora vamos gerar os dados de treinamento e manter em cache\n",
        "    self.inputs = inputs\n",
        "    self.targets = targets\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.targets)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.inputs[idx], self.targets[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNqBfHLczFbq",
        "outputId": "34cdabc4-bcf1-46b2-c1b9-bc01e2729813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imprimindo o dataset\n",
            "(tensor([5872,  141, 4022]), tensor([ 141, 4022,   23]))\n",
            "(tensor([ 141, 4022,   23]), tensor([4022,   23, 5873]))\n",
            "(tensor([4022,   23, 5873]), tensor([  23, 5873,    1]))\n",
            "(tensor([  23, 5873,    1]), tensor([5873,    1,   10]))\n",
            "(tensor([5873,    1,   10]), tensor([ 1, 10, 52]))\n",
            "(tensor([ 1, 10, 52]), tensor([ 10,  52, 111]))\n",
            "(tensor([ 10,  52, 111]), tensor([ 52, 111, 263]))\n",
            "(tensor([ 52, 111, 263]), tensor([111, 263, 264]))\n",
            "(tensor([111, 263, 264]), tensor([ 263,  264, 4023]))\n",
            "(tensor([ 263,  264, 4023]), tensor([ 264, 4023,   15]))\n",
            "(tensor([ 264, 4023,   15]), tensor([4023,   15,  981]))\n",
            "(tensor([4023,   15,  981]), tensor([  15,  981, 5874]))\n",
            "(tensor([  15,  981, 5874]), tensor([ 981, 5874,    1]))\n",
            "(tensor([ 981, 5874,    1]), tensor([5874,    1,    6]))\n",
            "(tensor([5874,    1,    6]), tensor([  1,   6, 182]))\n",
            "(tensor([  1,   6, 182]), tensor([  6, 182, 131]))\n",
            "(tensor([  6, 182, 131]), tensor([182, 131, 273]))\n",
            "(tensor([182, 131, 273]), tensor([131, 273,   7]))\n",
            "(tensor([131, 273,   7]), tensor([ 273,    7, 2040]))\n",
            "(tensor([ 273,    7, 2040]), tensor([   7, 2040,   10]))\n",
            "(tensor([   7, 2040,   10]), tensor([2040,   10, 5875]))\n",
            "(tensor([2040,   10, 5875]), tensor([  10, 5875,    5]))\n",
            "(tensor([  10, 5875,    5]), tensor([5875,    5, 2434]))\n"
          ]
        }
      ],
      "source": [
        "# Testa a implementação do Dataset\n",
        "\n",
        "exemplo = \"publicando este livro em 1857, se disse ser aquella primeira edição uma prova typographica, que algum dia talvez o autor se dispuzesse a rever\"\n",
        "inputs_ex, targets_ex = gera_inputs_e_targets_para_array(torch.tensor(encode_sentence(exemplo, vocab)), 3)\n",
        "teste_dataset = GptDataset(inputs_ex, targets_ex)\n",
        "\n",
        "print('Imprimindo o dataset')\n",
        "for dados in teste_dataset:\n",
        "  print(dados)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia38vYFtzpzz"
      },
      "source": [
        "### Instâncias vocabulário de treino, dataset e dataloader\n",
        "\n",
        "Gera datasets e dataloader de treinamento e de teste. Para facilitar, vou continuar considerando que o vocabulário é o vocabulário do livro inteiro. Não vou gerar um vocabulário específico para o treinamento. Como o vocabulário é muito grande, não deve nem fazer diferença."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CCT0R4Rw0Q7K"
      },
      "outputs": [],
      "source": [
        "# Gera os dataset de treino e validação\n",
        "train_dataset = GptDataset(inputs_train, targets_train)\n",
        "val_dataset = GptDataset(inputs_val, targets_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC0C5qn2zJ_J",
        "outputId": "fe48ef1b-eddf-41b0-b1b8-825c5619ec20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(inputs_train): 111332\n",
            "len(inputs_val): 27827\n",
            "Proporção de teste (deve ser próximo de 0.2): 0.8000344929181727\n"
          ]
        }
      ],
      "source": [
        "print(f'len(inputs_train): {len(inputs_train)}')\n",
        "print(f'len(inputs_val): {len(inputs_val)}')\n",
        "print(f'Proporção de teste (deve ser próximo de {test_size}): {len(inputs_train)/(len(inputs_train)+len(inputs_val))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "l9WkzAkYBH9r"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7unUq_JvBXRV"
      },
      "outputs": [],
      "source": [
        "# TODO O CÓDIGO DAQUI VEIO DO VÍDEO DO KARPATHY: https://www.youtube.com/watch?v=kCc8FmEb1nY&ab_channel=AndrejKarpathy\n",
        "import torch.nn as nn\n",
        "\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, n_embd, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "    # register_buffer fará com que tril seja um parâmetro não treinado pelo\n",
        "    # modelo, mas persistente\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # O que o Karpathy chama de B, T, C é:\n",
        "    # B - batch size\n",
        "    # T - context_size\n",
        "    # C - head_size -> Para uma cabeça, o head_size costuma ser n_embd. Mas pode passar valor diferente também\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)   # (B,T,C)\n",
        "    q = self.query(x) # (B,T,C)\n",
        "\n",
        "    # Multiplica q pela transposta de k.\n",
        "    # A transposta é feita só nas duas últimas dimensões\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "\n",
        "    # Isso daqui é a implementação da máscara causal\n",
        "    # self.tril é uma matriz triangular de tamanho context_size (T):\n",
        "    # 1 0 0 0\n",
        "    # 1 1 0 0\n",
        "    # 1 1 1 0\n",
        "    # 1 1 1 1\n",
        "    # O resultado de wei.masket_fill vai ser o seguinte:\n",
        "    # x -inf -inf -inf\n",
        "    # x   x  -inf -inf    => Onde x são os valores que já estavam em wei\n",
        "    # x   x    x  -inf\n",
        "    # x   x    x    x\n",
        "    # E o softmax vai transformar isso nos pesos para calcular uma média.\n",
        "    # Vamos supor que wei seja uma matriz de 1. Após a máscara, isso vai ficar:\n",
        "    # 1 -inf -inf -inf\n",
        "    # 1   1  -inf -inf\n",
        "    # 1   1    1  -inf\n",
        "    # 1   1    1    1\n",
        "    # E, depois do softmax:\n",
        "    # 1.00 0.00 0.00 0.00\n",
        "    # 0.50 0.50 0.00 0.00\n",
        "    # 0.33 0.33 0.33 0.00\n",
        "    # 0.25 0.25 0.25 0.25\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "    wei = self.dropout(wei)\n",
        "    # Calcula o v\n",
        "    v = self.value(x) # (B,T,C)\n",
        "    # O cálculo abaixo faz wei @ v, o que na prática calcula uma média\n",
        "    # ponderada das linhas anteriores. Por exemplo, no caso de wei (q @ k.T)\n",
        "    # ter gerado uma matriz de uns, wei, nesse ponto terá os pesos equivalentes\n",
        "    # a uma média simples:\n",
        "    # 1.00 0.00 0.00 0.00\n",
        "    # 0.50 0.50 0.00 0.00\n",
        "    # 0.33 0.33 0.33 0.00\n",
        "    # 0.25 0.25 0.25 0.25\n",
        "    #     @\n",
        "    # 1  2  3  4\n",
        "    # 5  6  7  8\n",
        "    # 9  10 11 12\n",
        "    # 13 14 15 16\n",
        "    #\n",
        "    # vira:\n",
        "    # 1  2  3  4   -> média da linha 1\n",
        "    # 3  4  5  6   -> média das linhas 1 e 2\n",
        "    # 5  6  7  8   -> média das linhas 1, 2 e 3\n",
        "    # 7  8  9  10  -> média das linhas 1, 2, 3 e 4\n",
        "    out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, n_embd, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    # Define o multi head como um conjunto de heads\n",
        "    self.heads = nn.ModuleList([Head(n_embd, head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out)) # A projeção WO é aplicada apenas no caso multihead\n",
        "    return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    # Cria uma rede nos mesmos moldes do artigo do Bengio:\n",
        "    # (linear + ReLU (no Bengio é tanh) + Linear)\n",
        "    # e faz um dropout\n",
        "    # No caso do Bengio, o tamanho da rede é um hiperparâmetro (h). Nesse caso,\n",
        "    # é calculado como 4x o total de embeddings. Isso veio do artigo\n",
        "    # \"Attention is all you need\", pag. 5: \"The dimensionality of input and output\n",
        "    # is d_model = 512, and the inner-layer has dimensionality dff = 2048\"\n",
        "    # No artigo, d_model é o total de embeddings\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_embd, n_head, head_size)\n",
        "    self.ffwd = FeedFoward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # É um bloco Transformer.\n",
        "    # Começa com uma normalização da entrada (LayerNorm), passa para o\n",
        "    # bloco de auto-atenção e adiciona a entrada (desvio na Fig. 1 do artigo\n",
        "    # \"Attention is all you need\").\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    # Depois, normaliza de novo (LayerNorm), passa pelo bloco FeedForward e\n",
        "    # diciona a entrada anterior (desvio na Fig. 1 do artigo\n",
        "    # \"Attention is all you need\").\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "  def __init__(self, n_embd, context_size, n_head, n_layer, vocab_size):\n",
        "    super().__init__()\n",
        "    self.context_size = context_size\n",
        "\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    # Nesse modelo os embeddings de posição também são aprendidos:\n",
        "    self.position_embedding_table = nn.Embedding(context_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "    tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = tok_emb + pos_emb # (B,T,C)\n",
        "    x = self.blocks(x) # (B,T,C)\n",
        "    x = self.ln_f(x) # (B,T,C)\n",
        "    logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      # C, aqui, é o vocab_size.\n",
        "      # logits tem o formato assim:\n",
        "      # 0 0 0 .... 0.3 ... 0.7 ... 0\n",
        "      #    ..... BT linhas .....\n",
        "      # 1 0 0 ..................... 0\n",
        "      # E targets, assim:\n",
        "      # 78 ... 12 ... 32 (Total de BT elementos)\n",
        "      #\n",
        "      # F.cross_entropy vai verificar os índices do logits e comparar com o target.\n",
        "      # Por exemplo, o primeiro target é 78. É desejado que o logito na posição 78 seja 1 e o resto 0\n",
        "      # O último target é 32. Logo, é desejado que o logito na posição 32 seja 1 e o resto 0\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to the last block_size tokens\n",
        "      idx_cond = idx[:, -self.context_size:]\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx_cond)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brz8cF4w5F6L"
      },
      "source": [
        "Funções para calcular e imprimir a loss e a perplexidade de um DataLoader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "F-tlhAItrap6"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "@torch.no_grad()\n",
        "def calcula_loss_e_perplexidade(model, loader):\n",
        "  model.eval()  # Coloca o modelo no modo de avaliação (não treinamento)\n",
        "  loss = 0.0\n",
        "  acc = 0\n",
        "  for inputs, targets in tqdm(loader, desc='Calculando loss e perplexidade'):\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "    # Forward pass\n",
        "    logits, loss_batch = model(inputs, targets)\n",
        "    # Acumula a perda\n",
        "    loss += loss_batch*len(targets)\n",
        "    acc += len(targets)\n",
        "\n",
        "  loss /= acc\n",
        "  ppl = math.exp(loss)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  return loss, ppl\n",
        "\n",
        "def print_loss_ppl(msg, loss, ppl):\n",
        "  print(f'{msg}. Loss: {loss:.2f}. Perplexidade: {ppl:.2f}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz_6AIC85NIo"
      },
      "source": [
        "Instancia um modelo e imprime o total de parâmetros, loss e perplexidade antes do treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wntaV50nzJ_L",
        "outputId": "0e15cc09-41b4-4f51-c128-b181d559ab98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- Total de parâmetros do modelo ----------------\n",
            "1.490001 M parameters\n",
            "------------------ ANTES DE INICIAR O TREINAMENTO ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 435/435 [00:07<00:00, 54.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN]. Loss: 9.37. Perplexidade: 11678.98\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Como esse é o modelo que será treinado, seta a seed aqui\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "model = LanguageModel(n_embd, context_size, n_head, n_layer, vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print('---------------- Total de parâmetros do modelo ----------------')\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "print(f'------------------ ANTES DE INICIAR O TREINAMENTO ------------------')\n",
        "loss, ppl = calcula_loss_e_perplexidade(model, train_loader)\n",
        "print_loss_ppl('\\n[TRAIN]', loss, ppl)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjjuY3Ar5cMf"
      },
      "source": [
        "Treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ySPAcoSM5eqi"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "def treina_modelo(model, optimizer, train_loader, val_loader, num_epochs=num_epochs):\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    loss_train_epoch = 0\n",
        "    acc = 0\n",
        "    print(f'------------------ [ÉPOCA {epoch+1}/{num_epochs}] ------------------')\n",
        "    for inputs, targets in tqdm(train_loader, desc='Treinando modelo'):\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      logits, loss_batch = model(inputs, targets)\n",
        "      optimizer.zero_grad()\n",
        "      loss_batch.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_train_epoch += loss_batch*len(targets)\n",
        "      acc += len(targets)\n",
        "\n",
        "    loss_train_epoch /= acc\n",
        "    print_loss_ppl(f'[TRAIN Estimativa] Época {epoch+1}', loss_train_epoch, math.exp(loss_train_epoch))\n",
        "    loss_val, ppl_val = calcula_loss_e_perplexidade(model, val_loader)\n",
        "    print_loss_ppl(f'[EVAL] Época {epoch+1}', loss_val, ppl_val)\n",
        "\n",
        "    # Salva os modelos intermediários\n",
        "    checkpoint_path = f\"modelo_epoca_{epoch+1}.pth\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRwSPiwizJ_L",
        "outputId": "80a0fece-0e49-414a-ac15-89e122271f14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------ [ÉPOCA 1/5] ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinando modelo: 100%|██████████| 435/435 [00:11<00:00, 37.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN Estimativa] Época 1. Loss: 5.71. Perplexidade: 302.27\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 109/109 [00:00<00:00, 113.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL] Época 1. Loss: 5.11. Perplexidade: 165.35\n",
            "\n",
            "------------------ [ÉPOCA 2/5] ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinando modelo: 100%|██████████| 435/435 [00:11<00:00, 38.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN Estimativa] Época 2. Loss: 4.62. Perplexidade: 101.69\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 109/109 [00:00<00:00, 109.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL] Época 2. Loss: 4.98. Perplexidade: 145.60\n",
            "\n",
            "------------------ [ÉPOCA 3/5] ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinando modelo: 100%|██████████| 435/435 [00:11<00:00, 39.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN Estimativa] Época 3. Loss: 4.17. Perplexidade: 64.43\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 109/109 [00:00<00:00, 113.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL] Época 3. Loss: 5.04. Perplexidade: 154.48\n",
            "\n",
            "------------------ [ÉPOCA 4/5] ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinando modelo: 100%|██████████| 435/435 [00:11<00:00, 37.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN Estimativa] Época 4. Loss: 3.86. Perplexidade: 47.66\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 109/109 [00:00<00:00, 109.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL] Época 4. Loss: 5.15. Perplexidade: 172.54\n",
            "\n",
            "------------------ [ÉPOCA 5/5] ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinando modelo: 100%|██████████| 435/435 [00:11<00:00, 38.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN Estimativa] Época 5. Loss: 3.65. Perplexidade: 38.34\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 109/109 [00:00<00:00, 115.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL] Época 5. Loss: 5.28. Perplexidade: 195.40\n",
            "\n"
          ]
        }
      ],
      "source": [
        "treina_modelo(model, optimizer, train_loader, val_loader, num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSXfwYISDoPN"
      },
      "source": [
        "## Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recupera_modelo(model, epoca):\n",
        "  # Recupera o modelo salvo na época x\n",
        "  checkpoint_path = f\"modelo_epoca_{epoca}.pth\"\n",
        "  # Carregar o estado do checkpoint\n",
        "  checkpoint = torch.load(checkpoint_path)\n",
        "  # Aplicar o estado do modelo e otimizador carregados\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "id": "p2ml7lmOGVzA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoca_melhor_modelo = 3 if vocab_size_desejado_sem_UNK == 3000 else 2\n",
        "recupera_modelo(model, epoca=epoca_melhor_modelo)\n",
        "\n",
        "loss, ppl = calcula_loss_e_perplexidade(model, train_loader)\n",
        "print_loss_ppl(f'\\nLOSS e PPL de treinamento para o modelo da época {epoca_melhor_modelo}', loss, ppl)\n",
        "loss, ppl = calcula_loss_e_perplexidade(model, val_loader)\n",
        "print_loss_ppl(f'\\nLOSS e PPL de validação para o modelo da época {epoca_melhor_modelo}', loss, ppl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In3J87dJGbGC",
        "outputId": "0a161cd4-0346-48df-d9ec-ce79bf62d2fa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 435/435 [00:03<00:00, 109.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LOSS e PPL de treinamento para o modelo da época 2. Loss: 4.24. Perplexidade: 69.16\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 109/109 [00:00<00:00, 109.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LOSS e PPL de validação para o modelo da época 2. Loss: 4.98. Perplexidade: 145.60\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checa o final do livro (o final está no conjunto de validação)\n",
        "print(livro[620000:623000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy9qnpNkAiva",
        "outputId": "6024fca9-9177-482b-814a-8d6cb5627ef6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " para elle a quem bastava o galho de uma arvore; mas para Cecilia.\n",
            "Seguindo pela margem para escolher o lugar mais favoravel, Pery soltou uma palavra de surpreza vendo a canôa que se tinha embaraçado numa dessas ilhas fluctuantes feitas pelas parasitas do rio que boião sobre as aguas.\n",
            "Era o melhor leito que podia ter a menina no meio do deserto; puxou a canôa, alcatifou o fundo com as folhas macias das palmeiras, e, tomando Cecilia nos braços, deitou-a no seu berço.\n",
            "A menina não consentio que Pery remasse; e a canôa deslisou docemente pelo leito do rio, apenas impellida pela correnteza.\n",
            "Cecilia brincava; debruçava-se sobre as aguas para colher uma flôr de passavem, para perseguir um peixe que beijava a face lisa das ondas, para ter o prazer de molhar as mãos nessa agua crystallina, para rever a sua imagem nesse espelho vacillante.\n",
            "Quando tinha brincado bastante, voltava-se para seu amigo e fallava-lhe com o gazeio argentino, mimoso chilrear dos labios travessos de uma linda menina, onde as cousas mais ligeiras e mais frivolas revestem encantos e graça suprema.\n",
            "Pery estava distrahido; seu olhar fitava-se no horizonte com uma attenção extraordinaria; a inquietação que se desenhava no seu semblante era indicio de algum perigo, embora ainda remoto.\n",
            "Sobre a linha azulada da cordilheira dos Orgãos, que se destacava n'um fundo de purpura e rosicler, amontoavão-se grossas nuvens escuras e pesadas, que, feridas pelos raios do occaso, lançavão reflexos acobreados.\n",
            "Dahi a pouco a serrania desappareceu envolta nesse manto côr de bronze, que se elevava como as columnas e abobadas de stalactites que se encontrão nas grutas das nossas montanhas. O azul puro e risonho que cobria o resto do firmamento contrastava com a cinta escura, que ia ennegrecendo gradualmente á medida que a noite cahia.\n",
            "Pery voltou-se.\n",
            "--Tu queres ir para terra, senhora?\n",
            "--Não; estou tão bem aqui! Não foste tu que me trouxeste?\n",
            "--Sim; mas...\n",
            "--Nada; pódes dormir sem receio!\n",
            "Elle tinha-se lembrado que entre dous perigos o melhor era preferir o mais remoto; aquelle que ainda estava longe e talvez não viesse.\n",
            "Por isso resolveu não dizer nada a Cecilia, e conservar-se attento e vigilante para salva-la, se o que elle temia se realisasse.\n",
            "Pery havia lutado com o tigre, os homens, com uma tribu de selvagens, com o veneno; e tinha vencido. Era chegada a occasião de lutar com os elementos; com a mesma confiança calma e impassivel, esperou prompto a aceitar o combate.\n",
            "O horizonte, sempre negro e fechado, se illuminava ás vezes com um lampejo phosphorescente: um tremor surdo parecia correr pelas entranhas da terra e fazia ondular a superficie das aguas, como o seio de uma vela enfunada pelo vento.\n",
            "Entretanto, ao redor tudo estava quieto; as estrellas recamavão o azul do céo; a viração aninhava-se nas folhas das arvores; os murmurios doces da solidão cantavão o hymno da noite.\n",
            "Cecilia adormeceu no seu berço, murmurando uma prece.\n",
            "Era alta noite; sombras espessas cobrião as margens do Parahyba.\n",
            "De repen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Essa função pega um texto grande, pega apenas os primeiros context_size tokens dele e tenta gerar o resto\n",
        "# Imprime o início e a continuação para comparar:\n",
        "def continua_frase_e_printa(model, texto_entrada, n_tokens):\n",
        "  # No código do Karpathy, a entrada tem tamanho (B, T)\n",
        "  # Cria uma lista de tokens de entrada\n",
        "  inicio_texto = ' '.join(tokenizar(texto_entrada)[:context_size])\n",
        "  tokens_entrada = encode_sentence(inicio_texto, vocab)\n",
        "\n",
        "  # Converte os tokens para tensor no tamanho (B=1, x). Se x > T, o generate trunca pra pegar só os últimos T tokens\n",
        "  tensor_tokens_entrada = torch.tensor(tokens_entrada, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "  print('-------------------------------')\n",
        "  print(f'Texto para completar:\\n\\t{inicio_texto}\\n')\n",
        "  print(f'Texto gerado:\\n\\t{decode_sentence(model.generate(tensor_tokens_entrada, max_new_tokens=n_tokens)[0].tolist(), most_frequent_words)}\\n')\n",
        "  print(f'Texto do livro:\\n\\t{texto_entrada}')\n",
        "\n",
        "continua_frase_e_printa(model, texto_entrada=\"Seguindo pela margem para escolher o lugar mais favoravel, Pery soltou uma palavra de surpreza vendo a canôa que se tinha embaraçado numa dessas ilhas fluctuantes feitas pelas parasitas do rio que boião sobre as aguas.\", n_tokens=100)\n",
        "continua_frase_e_printa(model, texto_entrada=\"Dahi a pouco os murmurios das aguas confundião-se com os accentos maviosos da voz de Cecilia que recitava o hymno christão repassado de tanta uncção e poesia.\", n_tokens=100)\n",
        "continua_frase_e_printa(model, texto_entrada=\"Logo que o sol chegou ao zenith, Pery procurou como na vespera um abrigo para passar as horas de calma.\", n_tokens=100)\n",
        "continua_frase_e_printa(model, texto_entrada=\"A canôa pojou n'um pequeno seio do rio, Cecilia saltou em terra; e seu companheiro escolheu uma sombra onde ella repousasse.\", n_tokens=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Hgznqr2AEaR",
        "outputId": "04fc7d7d-6780-4e70-efbd-cbcb385ce952"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------\n",
            "Texto para completar:\n",
            "\tseguindo pela margem para escolher o lugar mais favoravel\n",
            "\n",
            "Texto gerado:\n",
            "\tseguindo pela margem para escolher o lugar mais favoravel , nas costas , e delirante ? quantas obrigado a conversa e <unk> . eu o immovel nunca : mandai quanto a figura de pery , onde tudo da mulher para sua prima : d . tua marcha sempre alvaro .\n",
            "- - e eu furtar satisfazer , mas não me importa ; que tem o contacto , respondeu na sua existencia contra elles , e contrabando loredano dominou era mais melhor a parte do italiano ; quando a canto da cerca .\n",
            "ergueu foi nisto aymorés e cauan negras a uma palavra de promessa do menina tinha\n",
            "\n",
            "Texto do livro:\n",
            "\tSeguindo pela margem para escolher o lugar mais favoravel, Pery soltou uma palavra de surpreza vendo a canôa que se tinha embaraçado numa dessas ilhas fluctuantes feitas pelas parasitas do rio que boião sobre as aguas.\n",
            "-------------------------------\n",
            "Texto para completar:\n",
            "\tdahi a pouco os murmurios das aguas confundião -\n",
            "\n",
            "Texto gerado:\n",
            "\tdahi a pouco os murmurios das aguas confundião - vos ao fogo - se ao primeiro ferisse ; a pequena digressão ganhar o que se havia muito custo presa , segurando os indios levantou - se do que ellas tão imprevisto . além , acompanhava de sua senhora ; á meio insultar simplesmente estremeceu com a realisação , soltou que desmaios loredano , respondeu o fidalgo levantava a selvagem sorria ; esticou na folhagem ; o italiano admirado estava sua vida .\n",
            "aproximou - se de sobre o moço atirado de janeiro ; mas ergueu os emmudecêrão o perigo de motivo ; a flôr hespanhola o italiano selvagem\n",
            "\n",
            "Texto do livro:\n",
            "\tDahi a pouco os murmurios das aguas confundião-se com os accentos maviosos da voz de Cecilia que recitava o hymno christão repassado de tanta uncção e poesia.\n",
            "-------------------------------\n",
            "Texto para completar:\n",
            "\tlogo que o sol chegou ao zenith , pery\n",
            "\n",
            "Texto gerado:\n",
            "\tlogo que o sol chegou ao <unk> , pery em que abandones pery via a escrevi de encontro á arvore cameleão .\n",
            "todo a palavra disse cecilia descera da causa ! dispenso é possivel perder um corropio , como uma cousa pelo punhal que o indio esperavão sobre ella ; ahi o dr lançar - se por nós quizesse ao desengano um aspecto marcial para sempre tremeu .\n",
            "- - ha então ?\n",
            "- - já atirar o profunda contra a senhora corria o enorme do indio a ponto e sorveu a haste della um jasmim : nas faces .\n",
            "« fitou - se para\n",
            "\n",
            "Texto do livro:\n",
            "\tLogo que o sol chegou ao zenith, Pery procurou como na vespera um abrigo para passar as horas de calma.\n",
            "-------------------------------\n",
            "Texto para completar:\n",
            "\ta canôa pojou n ' um pequeno seio do\n",
            "\n",
            "Texto gerado:\n",
            "\ta canôa <unk> n ' um pequeno seio do rio , immovel preso em busca mal paralysou punição e o lindas com o momento de páo momentos supremos . o menor circulo .\n",
            "- - pretendo caminho .\n",
            "- - goytacaz ...\n",
            "- - preciso ! ... não respondeu :\n",
            "- - sou ! sim ! este papel ?\n",
            "- - não digo .\n",
            "- - oh !\n",
            "- - sim ! replicou o animal !\n",
            "- - _per ! disse ella me obrigues que vos seriamente um beijo .\n",
            "quando ouvio - se agitavão sobre si . que ainda\n",
            "\n",
            "Texto do livro:\n",
            "\tA canôa pojou n'um pequeno seio do rio, Cecilia saltou em terra; e seu companheiro escolheu uma sombra onde ella repousasse.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}