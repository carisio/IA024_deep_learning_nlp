{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aula 10 - RAGAS\n",
        "\n",
        "Leandro Carísio Fernandes"
      ],
      "metadata": {
        "id": "4uId9T_y6TVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "kpjeqqMf6eBF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "NY0Np6EN6S0j"
      },
      "outputs": [],
      "source": [
        "DIR_AULA10 = \"/content/drive/My Drive/IA024A-Redes_Neurais_NLP/Aula10-RAGAS/\"\n",
        "\n",
        "ARQUIVO_PARA_TESTAR = f'{DIR_AULA10}test_questions.json'\n",
        "\n",
        "GERAR_FAITHFULNESS = False\n",
        "GERAR_ANSWER_RELEVANCE = False\n",
        "GERAR_CONTEXT_RELEVANCE = False\n",
        "\n",
        "ARQUIVO_AVALIACAO_RAGAS = f'{DIR_AULA10}avaliacao_ragas.pickle'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "GROQ_API = getpass(\"API groq\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1UWHbKO6jrw",
        "outputId": "2298e28a-39ef-4ce6-9a84-c8c1d75b9c1f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API groq··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-r1cn8L6kqM",
        "outputId": "8b515990-22fd-43a7-b161-2d3a15baf6fb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Groq\n",
        "!pip install unidecode\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvi9OY1I6lpk",
        "outputId": "d7b2d959-f04a-4a66-c287-a3ec8a09137a"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Groq in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from Groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from Groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from Groq) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from Groq) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from Groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from Groq) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->Groq) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->Groq) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->Groq) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->Groq) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->Groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->Groq) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->Groq) (2.18.2)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.40.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download da base de dados\n",
        "\n",
        "Para esse caderno, vamos precisar apenas do arquivo de teste fornecido pelo professor."
      ],
      "metadata": {
        "id": "rrBSyq586peS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"{ARQUIVO_PARA_TESTAR}\" \"./test_questions.json\""
      ],
      "metadata": {
        "id": "2wXGE83yFdB9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "test_set = json.load(open('test_questions.json', 'r'))"
      ],
      "metadata": {
        "id": "nf2M5Uxw61ZJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entendendo o dataset"
      ],
      "metadata": {
        "id": "l9WxTI_R7DQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0 # i = 3 # Dá pra testar isso com qualquer tipo de resposta span. Para outros tipos, tem que adaptar\n",
        "print('Primeira pergunta do conjunto de teste:')\n",
        "print('Pergunta:', test_set[i]['question'])\n",
        "print('Resposta:', test_set[i]['answer']['answer_spans'][0]['text']) # Isso só funciona pois já sabemos que é do tipo span\n",
        "print('De onde veio a resposta:', test_set[i]['answer']['answer_spans'][0]['passage'])\n",
        "print('---------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0VVgLCd7ErC",
        "outputId": "c91ce807-81b0-48a3-a2cc-841b6cfaa4b3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeira pergunta do conjunto de teste:\n",
            "Pergunta: What is Zeus know for in Greek mythology?\n",
            "Resposta: sky and thunder god\n",
            "De onde veio a resposta: zeus\n",
            "---------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime o contexto\n",
        "t = test_set[10]\n",
        "contexto = [c['text'] for c in t['context']]\n",
        "print('. '.join(contexto))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zU5pjc77Ogw",
        "outputId": "8e62bac5-78e6-431f-d6dc-9f979c06cac7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and studied medicine at the University of Genoa in Italy..  It is located in the city of Genoa and regional Metropolitan City of Genoa, on the Italian Riviera in the Liguria region of northwestern Italy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Organizando os dados\n",
        "\n",
        "Organizando o conjunto de teste para gerar um array de pergunta/resposta/contexto."
      ],
      "metadata": {
        "id": "qpBFJSoPGa60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perg_resp_contexto = []\n",
        "\n",
        "for pr in test_set:\n",
        "  pergunta = pr['question']\n",
        "  resposta_obj = pr['answer']\n",
        "  tipo_resposta = resposta_obj['type']\n",
        "\n",
        "  if tipo_resposta == 'binary':\n",
        "    resposta = resposta_obj['answer_value']\n",
        "  elif tipo_resposta == 'value':\n",
        "    resposta = resposta_obj['answer_value'] + ' ' + resposta_obj['answer_unit']\n",
        "  elif tipo_resposta == 'span':\n",
        "    resposta = resposta_obj['answer_spans'][0]['text']\n",
        "  elif tipo_resposta == 'none':\n",
        "    resposta = 'none'\n",
        "  else:\n",
        "    resposta = 'Não pode chegar aqui, os tipos parece que são só binary/value/span/none!'\n",
        "    print(tipo_resposta)\n",
        "\n",
        "  contexto = [c['text'] for c in pr['context']]\n",
        "  perg_resp_contexto.append({\n",
        "      \"pergunta\": pergunta,\n",
        "      \"resposta\": resposta.strip(),\n",
        "      \"contexto\": contexto # Aqui está no formato de array. Depois tem que juntar para formar uma string.\n",
        "  })\n",
        "\n",
        "  # Printa as respostas:\n",
        "  print(resposta)"
      ],
      "metadata": {
        "id": "CkwHoN2j9ZGW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b4f945a-d2f0-4040-9245-9215a2e6e9ce"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sky and thunder god\n",
            "5 years\n",
            "30 years\n",
            "2 years\n",
            "26,688\n",
            "White Hart Lane\n",
            "Germany \n",
            "Kulm\n",
            "9 years\n",
            ", Friedrich Max Müller,\n",
            "Liguria\n",
            "Cuba\n",
            "1909\n",
            "2000\n",
            "1 year\n",
            "31 \n",
            "USA \n",
            "Bill de Blasio\n",
            "11 Years\n",
            "Herald Sun\n",
            "Iraq\n",
            "1897 years\n",
            "Costa Rica\n",
            "Colorado Buffaloes\n",
            "- Frank McPhee, Princeton\n",
            "- Bernie Flowers, Purdue\n",
            "- Eddie Bell, Pennsylvania\n",
            "- Tom Stolhandske, Texas\n",
            "- Tom Scott, Virginia\n",
            "- Joe Collier, Northwestern\n",
            "- Don Branby, Colorado\n",
            "- Buck Martin, Georgia Tech\n",
            "- Steve Mellinger, Kentucky\n",
            "- Ed Luke, Michigan State\n",
            "- Harry Babcock, Georgia\n",
            "\n",
            "106 years\n",
            "69 years\n",
            "7 May 1946\n",
            "Pulp Fiction\n",
            "David Austin\n",
            "Cardinals \n",
            "60 years\n",
            "Rochester, New York,\n",
            "Sam Chase\n",
            "Georgia\n",
            "5 appearances\n",
            "1964 \n",
            "George Pesut\n",
            "yes\n",
            "yes\n",
            " Guaranteed Rate Field\n",
            "20 years\n",
            "10 years\n",
            "marathon\n",
            "142.8 seconds\n",
            "1945\n",
            "1947\n",
            "no\n",
            " Donelson\n",
            "United States\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação RAGAS"
      ],
      "metadata": {
        "id": "q7NrAte57vFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import pandas as pd\n",
        "\n",
        "def salvar_df_avaliacao():\n",
        "  df_avaliacao.to_pickle(ARQUIVO_AVALIACAO_RAGAS)\n",
        "\n",
        "if os.path.isfile(ARQUIVO_AVALIACAO_RAGAS):\n",
        "  df_avaliacao = pd.read_pickle(ARQUIVO_AVALIACAO_RAGAS)\n",
        "else:\n",
        "  df_avaliacao = pd.DataFrame(perg_resp_contexto)\n",
        "  df_avaliacao['faithfulness'] = ''\n",
        "  df_avaliacao['answer_relevance'] = ''\n",
        "  df_avaliacao['context_relevance'] = ''\n",
        "  salvar_df_avaliacao()"
      ],
      "metadata": {
        "id": "NgpZxEJVl56V"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Faithfulness"
      ],
      "metadata": {
        "id": "AeJE2lUJ9Z4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como para esse dataset todas as respostas são bem curtas, podemos pular a parte do método de separar a resposta em sentenças, pois a resposta do dataset já é uma sentença única.\n",
        "\n",
        "Com isso, podemos adaptar ligeiramente o prompt proposto para essa métrica, já que não será necessário passar todas as frases.\n",
        "\n",
        "Obs.: Como temos só uma frase para avaliar, na prática a métrica é 0 ou 1. Ou ela está no contexto ou não está."
      ],
      "metadata": {
        "id": "lXyETllqKrY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message_faithfulness = \"\"\"\n",
        "Consider the given context and following statement, then determine whether it is supported by the information present in the context.\n",
        "\n",
        "The user will provide the context and statement in JSON format and you should also answer in JSON format with two attributes: \"reason\" and \"verdict\".\n",
        "\n",
        "Your answer should provide a brief explanation for the statement before arriving at the verdict. The veredict should be the string \"Yes\" or the string \"No\".\n",
        "\n",
        "Do not deviate from the specified format.\n",
        "\"\"\".strip()\n",
        "\n",
        "content_message_faithfulness = \"\"\"\n",
        "{{\n",
        "  \"context\": \"{context}\",\n",
        "  \"statement\": \"{answer}\"\n",
        "}}\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "X_aaMOJ24DGA"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key=GROQ_API)\n",
        "\n",
        "def get_faithfulneess_message(context, answer):\n",
        "  messages=[\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message_faithfulness\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": content_message_faithfulness.format(context=context, answer=answer)\n",
        "    }\n",
        "  ]\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"llama3-70b-8192\",\n",
        "    messages=messages,\n",
        "    temperature=1,\n",
        "    max_tokens=1024,\n",
        "    top_p=1,\n",
        "    stream=False,\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    stop=None,\n",
        "  )\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "WCPjUbFE3wue"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for idx, row in tqdm(df_avaliacao.iterrows(), total=len(df_avaliacao), desc=\"Calculando Faithfulness\"):\n",
        "  gerar_para_pergunta = GERAR_FAITHFULNESS or row['faithfulness'] == ''\n",
        "\n",
        "  if gerar_para_pergunta:\n",
        "    df_avaliacao.at[idx, 'faithfulness'] = get_faithfulneess_message('. '.join(row['contexto']), row['resposta'])\n",
        "    salvar_df_avaliacao()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF4KCYGNMx_n",
        "outputId": "c206eb4b-e267-4b37-a2c4-0b28e4ab0637"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando Faithfulness: 100%|██████████| 50/50 [00:00<00:00, 5552.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "total_yes = 0\n",
        "for _, row in tqdm(df_avaliacao.iterrows(), total=len(df_avaliacao), desc=\"Calculando Faithfulness\"):\n",
        "  json_msg_faithfulness = row['faithfulness']\n",
        "  msg_faithfulness = json.loads(json_msg_faithfulness)\n",
        "\n",
        "  total_yes += 1 if msg_faithfulness['verdict'] == 'Yes' else 0\n",
        "\n",
        "print(f'\\nFaithfulness: {total_yes/len(df_avaliacao):.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2MCBoEXBSS8",
        "outputId": "22f31199-915c-4386-b38a-552c5c4d6be9"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando Faithfulness: 100%|██████████| 50/50 [00:00<00:00, 6072.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Faithfulness: 0.68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer relevance"
      ],
      "metadata": {
        "id": "UIKCzZ-K9bXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message_answer_relevance = \"\"\"\n",
        "The user will provide an answer and your task is to generate a possible question for that answer.\n",
        "\n",
        "You should answer in JSON format with two attributes: \"reason\" and \"question\".\n",
        "\n",
        "Do not deviate from the specified format.\n",
        "\"\"\".strip()\n",
        "\n",
        "content_message_answer_relevance = \"\"\"\n",
        "Generate a question for the given answer.\n",
        "\n",
        "Answer: {answer}\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "NGoWM9MF9gxc"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key=GROQ_API)\n",
        "\n",
        "def get_answer_relevance_message(answer):\n",
        "  messages=[\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message_answer_relevance\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": content_message_answer_relevance.format(answer=answer)\n",
        "    }\n",
        "  ]\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"llama3-70b-8192\",\n",
        "    messages=messages,\n",
        "    temperature=1,\n",
        "    max_tokens=1024,\n",
        "    top_p=1,\n",
        "    stream=False,\n",
        "    #response_format={\"type\": \"json_object\"},\n",
        "    stop=None,\n",
        "  )\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "E6MqK40qDPQw"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for idx, row in tqdm(df_avaliacao.iterrows(), total=len(df_avaliacao), desc=\"Calculando Faithfulness\"):\n",
        "  gerar_para_pergunta = GERAR_ANSWER_RELEVANCE or row['answer_relevance'] == ''\n",
        "\n",
        "  if gerar_para_pergunta:\n",
        "    # Gera 10 perguntas\n",
        "    perguntas_possiveis = []\n",
        "    for i in range(10):\n",
        "      perguntas_possiveis.append(get_answer_relevance_message(row['resposta']))\n",
        "    df_avaliacao.at[idx, 'answer_relevance'] = perguntas_possiveis\n",
        "    salvar_df_avaliacao()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMvs_wy9DfvP",
        "outputId": "d438da53-57ff-4a8b-aa28-1a75d25602af"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando Faithfulness: 100%|██████████| 50/50 [00:00<00:00, 7515.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extrai as perguntas\n",
        "\n",
        "import re\n",
        "\n",
        "regex_question = r'\"question\"\\s*:\\s*\"((?:[^\"]|(?<=\\\\)\")*?)\"\\s*[,}]'\n",
        "perguntas_geradas = []\n",
        "\n",
        "for _, row in tqdm(df_avaliacao.iterrows(), total=len(df_avaliacao), desc=\"Calculando Answer Relevance\"):\n",
        "  lista_json_msg_answer_relevance = row['answer_relevance']\n",
        "\n",
        "  perguntas_para_essa_resposta = []\n",
        "  for json_msg_answer_relevance in lista_json_msg_answer_relevance:\n",
        "    # Considerando que nem sempre o LLM trouxe json bem formatados (tive que\n",
        "    # desabitar o retorno para JSON), vou pegar o conteúdo da propriedade\n",
        "    # \"question\" com regex\n",
        "    question_str_match = re.search(regex_question, json_msg_answer_relevance)\n",
        "    if question_str_match:\n",
        "      # O json é mal formado. As vezes a string está assim \"pergunta gerada\",\n",
        "      # mas acontece de ter alguns problemas. Por exemplo, já chegou a ter\n",
        "      # resultado assim: \"pergunta gerada'.\n",
        "      # Por isso, guarda apenas as perguntas que dão match no padrão pesquisado\n",
        "      perguntas_para_essa_resposta.append(question_str_match.group(1))\n",
        "\n",
        "  perguntas_geradas.append(perguntas_para_essa_resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ33Y3AbENH6",
        "outputId": "3ad8c963-55a5-4766-ab71-1dcb1630dd01"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando Answer Relevance: 100%|██████████| 50/50 [00:00<00:00, 3842.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gera os embeddings e calcula a métrica\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "idx = 0\n",
        "AR_todas_perguntas = 0\n",
        "for _, row in tqdm(df_avaliacao.iterrows(), total=len(df_avaliacao), desc=\"Calculando Answer Relevance\"):\n",
        "  # Pergunta usada para se obter a resposta\n",
        "  pergunta_referencia = row['pergunta']\n",
        "  # Perguntas geradas pelo LLM\n",
        "  perguntas_geradas_para_pergunta_referencia = perguntas_geradas[idx]\n",
        "  # Agrupa tudo em uma única lista para gerar os embeddings de uma vez\n",
        "  todas_perguntas = [pergunta_referencia] + perguntas_geradas_para_pergunta_referencia\n",
        "  embeddings = model.encode(todas_perguntas)\n",
        "\n",
        "  # Cálculo do AR para essa pergunta\n",
        "  AR = 0\n",
        "  for i in range(1, len(embeddings)):\n",
        "    AR += cosine_similarity(np.array([embeddings[0]]), np.array([embeddings[i]]))[0][0]\n",
        "  AR /= len(perguntas_geradas_para_pergunta_referencia)\n",
        "\n",
        "  # Acumula o AR para o dataset\n",
        "  AR_todas_perguntas += AR\n",
        "\n",
        "  idx += 1\n",
        "\n",
        "# Termina o cálculo do AR para o dataset (média)\n",
        "AR_todas_perguntas /= len(df_avaliacao)\n",
        "\n",
        "# Imprime o resultado\n",
        "print(f'\\nAnswer relevance: {AR_todas_perguntas:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6dqlskfOVsH",
        "outputId": "c905e645-6a94-4351-f29d-2b8287e854d5"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando Answer Relevance: 100%|██████████| 50/50 [00:06<00:00,  8.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer relevance: 0.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context relevance"
      ],
      "metadata": {
        "id": "JwKQzy2w9bd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message_context_relevance = \"\"\"\n",
        "The user will provide a question and list of sentences that can potentially help answer the question.\n",
        "\n",
        "Your task is to identify the relevant sentences provided.\n",
        "\n",
        "Your answer must be in JSON format with one attribute: \"relevant_sentences\". This attribute must contain a list of sentences that are relevant to the question.\n",
        "\n",
        "While extracting candidate sentences you're not allowed to make any changes to the sentences provided.\n",
        "\n",
        "Do not deviate from the specified format.\n",
        "\"\"\".strip()\n",
        "\n",
        "def get_content_message_context_relevance(question, context):\n",
        "  msg = f\"Question: {question}\\n\\nSentences:\\n\"\n",
        "  for i, sentence in enumerate(context):\n",
        "    msg = msg + f\"{i} - {sentence}\\n\"\n",
        "\n",
        "  return msg"
      ],
      "metadata": {
        "id": "U9x_7P6YTnSo"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key=GROQ_API)\n",
        "\n",
        "def get_context_relevance_message(question, context):\n",
        "  messages=[\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message_context_relevance\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": get_content_message_context_relevance(question, context)\n",
        "    }\n",
        "  ]\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"llama3-8b-8192\",\n",
        "    messages=messages,\n",
        "    temperature=1,\n",
        "    max_tokens=1024,\n",
        "    top_p=1,\n",
        "    stream=False,\n",
        "    #response_format={\"type\": \"json_object\"},\n",
        "    stop=None,\n",
        "  )\n",
        "  return completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "dpOyOIRUUO34"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for idx, row in tqdm(df_avaliacao.iterrows(), total=len(df_avaliacao), desc=\"Calculando Context Relevance\"):\n",
        "  gerar_para_pergunta = GERAR_CONTEXT_RELEVANCE or row['context_relevance'] == ''\n",
        "\n",
        "  if gerar_para_pergunta:\n",
        "    df_avaliacao.at[idx, 'context_relevance'] = get_context_relevance_message(row['pergunta'], row['contexto'])\n",
        "    salvar_df_avaliacao()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sbK2yNoUs48",
        "outputId": "474d17f4-b787-436b-96b9-b2352f220fce"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando Context Relevance: 100%|██████████| 50/50 [00:44<00:00,  1.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extrai a lista gerada\n",
        "contexto = []\n",
        "sentencas_relevantes = []\n",
        "\n",
        "for _, row in tqdm(df_avaliacao.iterrows(), total=len(df_avaliacao), desc=\"Calculando Context Relevance\"):\n",
        "  contexto.append(row['contexto'])\n",
        "\n",
        "  resposta_llm_context_relevance = row['context_relevance']\n",
        "  inicio = resposta_llm_context_relevance.index('[')\n",
        "  fim = resposta_llm_context_relevance.rindex(']') + 1\n",
        "  str_lista = resposta_llm_context_relevance[inicio:fim]\n",
        "\n",
        "  # Conserta algumas questões do JSON de retorno (que não veio relacionado ao padrão)\n",
        "  for i in range(10):\n",
        "    # Em alguns casos a informação do número da sentença veio fora da lista. Apaga isso\n",
        "    str_lista = str_lista.replace(f\"{i} - \", \"\")\n",
        "  # Em alguns casos, os elementos da lista veio separado apenas por uma quebra.\n",
        "  # Precisa colocar vírgula entre os termos\n",
        "  str_lista = str_lista.replace(\"\\\"\\n\\\"\", \"\\\",\\n\\\"\")\n",
        "\n",
        "  sentencas_relevantes.append(json.loads(str_lista))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7wtU8c4YIpY",
        "outputId": "8f47463d-e96e-492c-e33b-6dffb2d2fca5"
      },
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando Context Relevance: 100%|██████████| 50/50 [00:00<00:00, 5150.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CR = [len(sentencas_relevantes[i]) / len(contexto[i]) for i in range(len(contexto))]\n",
        "print(f'Context relevance: {sum(CR)/len(CR):.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwSTWGJsZcUp",
        "outputId": "e9991628-70ed-443a-e480-855f5b3edce2"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context relevance: 0.65\n"
          ]
        }
      ]
    }
  ]
}