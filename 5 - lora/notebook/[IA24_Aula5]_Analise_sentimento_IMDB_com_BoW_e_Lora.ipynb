{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Análise de sentimentos na base do IMDB usando BoW e LORA\n",
        "\n",
        "Leandro Carísio Fernandes"
      ],
      "metadata": {
        "id": "RG14e03lu4jH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parâmetros"
      ],
      "metadata": {
        "id": "c0ScMiEPvR9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_VOCAB_SIZE = 20000  # Máximo de palavras no vocabulário\n",
        "VOCAB_SIZE = -1         # Será ajustado automaticamente depois do cálculo do vocabulário\n",
        "\n",
        "HIDDEN_SIZE = 200       # Tamanho da camada oculta\n",
        "\n",
        "LR = 0.1                # Learning rate\n",
        "NUM_EPOCHS = 7          # Número de épocas para rodar o treinamento completo. Após 7 épocas já não tem melhoras significativas no dataset de validação\n",
        "NUM_EPOCHS_LORA = 10     # Número de épocas para rodar fine-tuning com LoRA (será executado após apenas uma época de treinamento completo)\n",
        "RANK = 1                # Rank que será usado. Quanto menor, menos parâmetros serão treinados no fine-tuning\n",
        "ALPHA = 1               # Usado pro cálculo do fator de escala na matriz delta_w. Escala = alpha/rank"
      ],
      "metadata": {
        "id": "125pyW-kvZpa"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "    print('GPU:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "    print('using CPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-ArYTafDtF8",
        "outputId": "6d8f083b-0b22-4c27-d786-ea6627ee01d1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seed"
      ],
      "metadata": {
        "id": "mFb7YhkivUEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def inicializa_seeds():\n",
        "  random.seed(123)\n",
        "  np.random.seed(123)\n",
        "  torch.manual_seed(123)\n",
        "\n",
        "inicializa_seeds()"
      ],
      "metadata": {
        "id": "0EXRU7oxvVcj"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download dos dados"
      ],
      "metadata": {
        "id": "lqMQ8UORvALq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gGiL0YCu2yu",
        "outputId": "5d917486-208c-47a6-e3e1-c90e58728876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘aclImdb.tgz’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz\n",
        "!tar -xzf aclImdb.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separação em treino/validação/teste"
      ],
      "metadata": {
        "id": "poFyothnvGQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "max_valid = 5000\n",
        "\n",
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "x_valid = x_train[-max_valid:]\n",
        "y_valid = y_train[-max_valid:]\n",
        "x_train = x_train[:-max_valid]\n",
        "y_train = y_train[:-max_valid]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('\\n3 primeiras amostras treino:')\n",
        "for x, y in zip(x_train[:3], y_train[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('\\n3 últimas amostras treino:')\n",
        "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('\\n3 primeiras amostras validação:')\n",
        "for x, y in zip(x_test[:3], y_test[:3]):    # No código original, estava x_valid\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('\\n3 últimas amostras validação:')\n",
        "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
        "    print(y, x[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXjBm_PCvIpu",
        "outputId": "11646f5e-dd5f-4127-faee-d136e51f5f91"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "\n",
            "3 primeiras amostras treino:\n",
            "False After some internet surfing, I found the \"Homefront\" series on DVD at ioffer.com. Before anyone gets\n",
            "False Think \"stage play\". This is worth seeing once for the performances of Lionel Atwill and Dwight Frye.\n",
            "True I've read a few of the reviews and I'm kinda sad that a lot of the Story seems glossed over. Its eas\n",
            "\n",
            "3 últimas amostras treino:\n",
            "False Fantastic Mr. Fox is a comedy based on the classic Roald Dahl book. Wes Anderson directs, and respec\n",
            "True **Attention Spoilers**<br /><br />First of all, let me say that Rob Roy is one of the best films of \n",
            "True The 14 year-old in me is immensely happy that they're now able to make really good looking fantasy m\n",
            "\n",
            "3 primeiras amostras validação:\n",
            "True One of the best sitcoms to run on Indian television along with Dekh bhai dekh and Idhar udhar. Great\n",
            "True Iron Eagle may not be the most believable film plot-wise, but the characters are well written and ve\n",
            "True This film stars Peter Lorre as an exceptionally nice guy who immigrates to America. Unfortunately, s\n",
            "\n",
            "3 últimas amostras validação:\n",
            "True I am sad to say that I disagree with other people on this Columbo episode. Death Lends a Hand is fra\n",
            "True This picture in 1935 walked away with all kinds of Ocars for Best Director, John Ford, Actor Victor \n",
            "True There is no denying that Ealing comedies are good, but for me this film stands out as one of the bes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizador e encoder"
      ],
      "metadata": {
        "id": "chgWKz5avohq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Função gerada com ajuda do ChatGPT\n",
        "import re\n",
        "\n",
        "def tokenizar(frase):\n",
        "    # return frase.split() # Usar esse return para uma tokenização simples (apenas split)\n",
        "    # Converter a frase para minúsculo\n",
        "    frase = frase.lower()\n",
        "    # Remover caracteres especiais e pontuação usando expressões regulares\n",
        "    frase = re.sub(r'[^\\w\\s]', '', frase)\n",
        "    # Dividir a frase em palavras\n",
        "    palavras = frase.split()\n",
        "    return palavras\n",
        "\n",
        "# Exemplo de uso\n",
        "frase = \"Olá! Como você está? Eu estou bem, obrigado.\"\n",
        "palavras = tokenizar(frase)\n",
        "print(palavras)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWk4r_ZSvqbA",
        "outputId": "72c3af15-e99a-4de6-f540-2aaf904e4e15"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['olá', 'como', 'você', 'está', 'eu', 'estou', 'bem', 'obrigado']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "idx_amostras = list(range(len(x_train)))\n",
        "\n",
        "counter_palavras = Counter()\n",
        "for review in x_train:\n",
        "    # Com tokenizador:\n",
        "    counter_palavras.update(tokenizar(review))\n",
        "\n",
        "# Cria um vocabulário com os tokens mais frequentes\n",
        "most_frequent_words = [\"<UNK>\"] + sorted(counter_palavras, key=counter_palavras.get, reverse=True)[:MAX_VOCAB_SIZE-1]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words)} # word é indexado de 0 (UNK) até MAX_VOCAB_SIZE\n",
        "VOCAB_SIZE = len(vocab.keys()) # Se MAX_VOCAB_SIZE for muito grande, é possível que o vocabulário não tenha palavras suficientes. Então ajusta pro tamanho correto"
      ],
      "metadata": {
        "id": "vlmakPK9vx8N"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_frequent_words[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9FJlZwLw6ia",
        "outputId": "3a25a364-12d2-4177-e31f-bfd8716cdea7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<UNK>', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "  return [vocab.get(word, 0) for word in tokenizar(sentence)] # 0 for OOV\n",
        "\n",
        "encode_sentence(\"I like Pizza PALAVRAINEXISTENTE\", vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05dysGCJxZiZ",
        "outputId": "5ba7ed5a-9567-4461-e251-f3245c8ff412"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9, 38, 8134, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definição da classe Dataset"
      ],
      "metadata": {
        "id": "h2iDcu2gvewV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import List\n",
        "\n",
        "class ImdbDataset(Dataset):\n",
        "  def __init__(self, x_data: List[str], y_data: List[bool], vocab) -> None:\n",
        "    self.vocab = vocab\n",
        "\n",
        "    tamanho_vocab = len(vocab.keys()) # isso dá igual VOCAB_SIZE. Pra não pegar da variável global\n",
        "\n",
        "    # Como as bases cabem em memória, faz cache\n",
        "    self.x = []\n",
        "    for review in x_data:\n",
        "        x = torch.zeros(tamanho_vocab)\n",
        "        x[encode_sentence(review, self.vocab)] = 1\n",
        "        self.x.append(x)\n",
        "\n",
        "    self.y = None if y_data is None else [torch.tensor(1 if y_is_true else 0) for y_is_true in y_data]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "BdX510FKvhFZ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checa a implementação do dataset:\n",
        "\n",
        "x_temp, y_temp = x_train[:10], y_train[0:10]\n",
        "dataset_temp = ImdbDataset(x_temp, y_temp, vocab)\n",
        "\n",
        "for i in range(len(x_temp)):\n",
        "  encoded_x = encode_sentence(x_temp[i], vocab)\n",
        "  x = torch.zeros(VOCAB_SIZE)\n",
        "  x[encoded_x] = 1\n",
        "  print(torch.all(x == dataset_temp[i][0]))\n",
        "  print(torch.tensor(y_temp[i]) == dataset_temp[i][1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eTLjBC9zii-",
        "outputId": "430391c1-fdfa-4562-eb20-474595b92f3e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader"
      ],
      "metadata": {
        "id": "4PU1eUs6A8dQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "batch_size = 128\n",
        "# define dataloaders\n",
        "train_data = ImdbDataset(x_train, y_train, vocab)\n",
        "test_data = ImdbDataset(x_test, y_test, vocab)\n",
        "val_data = ImdbDataset(x_valid, y_valid, vocab)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUAx3z_SA9eq",
        "outputId": "00e73070-1c94-4a8a-f919-bcdf35424bfb"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9.78 s, sys: 1.17 s, total: 10.9 s\n",
            "Wall time: 11.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementação camada LoRA"
      ],
      "metadata": {
        "id": "A7N5oBq6UGUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificando qual o operador mais eficiente (torch.matmul, torch.mm, @):"
      ],
      "metadata": {
        "id": "0bWUJ59grbhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "n_vezes = 20000 if device.type == 'cuda' else 2000\n",
        "\n",
        "temp_A = nn.Parameter(torch.Tensor(200, 1), requires_grad=False)\n",
        "temp_B = nn.Parameter(torch.Tensor(1, 3000), requires_grad=False)\n",
        "\n",
        "nn.init.normal_(temp_A, mean=0.0)\n",
        "nn.init.zeros_(temp_B)\n",
        "\n",
        "start_time = time.time()\n",
        "for i in range(n_vezes):\n",
        "  x = torch.matmul(temp_A, temp_B)\n",
        "end_time = time.time()\n",
        "print(f'matmul: {end_time - start_time}')\n",
        "\n",
        "start_time = time.time()\n",
        "for i in range(n_vezes):\n",
        "  x = torch.mm(temp_A, temp_B)\n",
        "end_time = time.time()\n",
        "print(f'mm: {end_time - start_time}')\n",
        "\n",
        "start_time = time.time()\n",
        "for i in range(n_vezes):\n",
        "  x = temp_A @ temp_B\n",
        "end_time = time.time()\n",
        "print(f'@: {end_time - start_time}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp3A5OnwqWIx",
        "outputId": "f31da3a1-6250-413d-8cb7-4c417ea53da5"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matmul: 2.756267786026001\n",
            "mm: 2.6500258445739746\n",
            "@: 2.6752097606658936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class LoRALayer(nn.Module):\n",
        "  def inicializa_pesos_linear(self):\n",
        "    # Parâmetros originais da camada linear\n",
        "    # Note que o nn.Parameter está no formato output_dim x input_dim,\n",
        "    # e não input_dim x output_dim como era de se esperar\n",
        "    # Isso é feito pois o PyTorch implementa os pesos da camada linear\n",
        "    # dessa forma. Em vez de calcular x*A + b, ele faz x * A_T + b,\n",
        "    # onde _T é a transposta. Por isso é necessário inverter isso\n",
        "    self.weight = nn.Parameter(torch.Tensor(self.output_dim, self.input_dim))\n",
        "    self.bias = nn.Parameter(torch.Tensor(self.output_dim))\n",
        "    nn.init.xavier_normal_(self.weight)\n",
        "    nn.init.uniform_(self.bias, 0, 0)\n",
        "\n",
        "  def inicializa_pesos_LoRA(self):\n",
        "    # Parâmetros LoRA\n",
        "    self.A = nn.Parameter(torch.Tensor(self.output_dim, self.rank), requires_grad=self.use_lora)\n",
        "    self.B = nn.Parameter(torch.Tensor(self.rank, self.input_dim), requires_grad=self.use_lora)\n",
        "\n",
        "    nn.init.normal_(self.A, mean=0.0)\n",
        "    nn.init.zeros_(self.B)\n",
        "\n",
        "  def __init__(self, input_dim, output_dim, rank, alpha=1.0, use_lora=False):\n",
        "    super(LoRALayer, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.rank = rank\n",
        "    self.alpha = alpha\n",
        "    self.use_lora = use_lora\n",
        "\n",
        "    self.scale = self.alpha/self.rank\n",
        "\n",
        "    self.inicializa_pesos_linear()\n",
        "    self.inicializa_pesos_LoRA()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Se estiver usando LoRA, é necessário considerar os pesos modificados\n",
        "    if self.use_lora:\n",
        "      delta_w = self.scale * torch.mm(self.A, self.B)\n",
        "      modified_weight = self.weight + delta_w\n",
        "      return nn.functional.linear(x, modified_weight, self.bias)\n",
        "    else:\n",
        "      # Caso contrário, usa os pesos originais\n",
        "      return nn.functional.linear(x, self.weight, self.bias)\n",
        "      # Obs.: O controle dos gradientes é feito dentro do toggle_lora\n",
        "\n",
        "\n",
        "  def toggle_lora(self, use_lora=False):\n",
        "    self.use_lora = use_lora\n",
        "    # Desabilita os gradientes de weight e bias se estiver usando a camada LoRA\n",
        "    self.weight.requires_grad = not self.use_lora\n",
        "    self.bias.requires_grad = not self.use_lora\n",
        "    # Habilita os gradientes de A e B se estiver usando a camada LoRA\n",
        "    self.A.requires_grad = self.use_lora\n",
        "    self.B.requires_grad = self.use_lora\n",
        "\n",
        "  def desabilita_lora_e_transfere_para_modelo(self):\n",
        "    # A ideia desse método é desabilitar o LoRA e alterar o peso original\n",
        "    # para considerar o treinamento usando LoRA\n",
        "    delta_w = self.scale * torch.mm(self.A, self.B)\n",
        "    self.weight.data += delta_w.data\n",
        "\n",
        "    self.toggle_lora(False)\n",
        "    self.inicializa_pesos_LoRA()"
      ],
      "metadata": {
        "id": "VGUtufOHUFyq"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando as dimensões\n",
        "# Pega um batch e passa o primeiro conjunto de inputs numa camada LoRA ativada:\n",
        "batch_teste = next(iter(test_loader))\n",
        "camada_lora = LoRALayer(VOCAB_SIZE, HIDDEN_SIZE, rank=10, use_lora=True)\n",
        "o1 = camada_lora(batch_teste[0][0])\n",
        "print('Cria uma camada LoRA e ativa o uso da camada (B é inicializado com zeros)')\n",
        "print('Faz o forward usando a camada => o1\\n')\n",
        "\n",
        "# Desativa a camada LoRA e passa de novo o mesmo input:\n",
        "camada_lora.toggle_lora(False)\n",
        "o2 = camada_lora(batch_teste[0][0])\n",
        "print('Desativa o uso da camada LoRA (faz a conta só com os pesos normais)')\n",
        "print('Faz o forward => o2')\n",
        "print(f'Testa de o1 == o2. Esperado: True. Resultado: {torch.all(o1 == o2)}\\n')\n",
        "\n",
        "# Na inicialização, B é tudo zero.\n",
        "# Altera diretamente o B com dados aleatórios para que a camada LoRA mude o resultado\n",
        "# Entretanto, mantém a camada LoRA desativada\n",
        "nn.init.normal_(camada_lora.B, mean=0.0)\n",
        "camada_lora.toggle_lora(False)\n",
        "o3 = camada_lora(batch_teste[0][0])\n",
        "print('Altera B para algum valor aleatório e mantém a camada LoRA desativada')\n",
        "print('Faz o forward => o3')\n",
        "print(f'Testa de o2 == o3. Esperado: True. Resultado: {torch.all(o2 == o3)}\\n')\n",
        "\n",
        "# Agora abilita a camada LoRA. Mantém B populado com algum valor\n",
        "camada_lora.toggle_lora(True)\n",
        "o4 = camada_lora(batch_teste[0][0])\n",
        "print('Mantém B com valores e ativa a camada LoRA')\n",
        "print('Faz o forward => o4')\n",
        "print(f'Testa de o2 == o4. Esperado: False. Resultado: {torch.all(o2 == o4)}\\n')\n",
        "\n",
        "# Agora desabilita a camada LoRA transferindo os resultados para os pesos originais\n",
        "# e calcula novamente\n",
        "camada_lora.desabilita_lora_e_transfere_para_modelo()\n",
        "o5 = camada_lora(batch_teste[0][0])\n",
        "print('Desabilita a camada LoRA, transferindo os pesos para a camada linear')\n",
        "print('Faz o forward => o5')\n",
        "print(f'Testa de o4 == o5. Esperado: True. Resultado: {torch.all(o4 == o5)}. Dif máxima: {torch.max((o4-o5).abs())}\\n')\n",
        "\n",
        "# Gera qualquer coisa na camada LoRA, mas não habilita ela. Não tem que fazer diferença\n",
        "nn.init.normal_(camada_lora.B, mean=0)\n",
        "o6 = camada_lora(batch_teste[0][0])\n",
        "print('Gera qualquer coisa na camada LoRA, mas não habilita ela')\n",
        "print('Faz o forward => o6')\n",
        "print(f'Testa de o5 == o6. Esperado: True. Resultado: {torch.all(o5 == o6)}\\n')\n",
        "\n",
        "# Agora habilita a camada LoRA sem inicializá-la. Tem lixo lá por conta da\n",
        "# inicialização dos pesos de B com dados (teste anterior)\n",
        "camada_lora.toggle_lora(True)\n",
        "o7 = camada_lora(batch_teste[0][0])\n",
        "print('Habilita a camada LoRA mantendo lixo em B')\n",
        "print('Faz o forward => o7')\n",
        "print(f'Testa de o6 == o7. Esperado: False. Resultado: {torch.all(o6 == o7)}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4keqE7K-YXVY",
        "outputId": "c3a093e6-1deb-4be8-ee15-6562969144b7"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cria uma camada LoRA e ativa o uso da camada (B é inicializado com zeros)\n",
            "Faz o forward usando a camada => o1\n",
            "\n",
            "Desativa o uso da camada LoRA (faz a conta só com os pesos normais)\n",
            "Faz o forward => o2\n",
            "Testa de o1 == o2. Esperado: True. Resultado: True\n",
            "\n",
            "Altera B para algum valor aleatório e mantém a camada LoRA desativada\n",
            "Faz o forward => o3\n",
            "Testa de o2 == o3. Esperado: True. Resultado: True\n",
            "\n",
            "Mantém B com valores e ativa a camada LoRA\n",
            "Faz o forward => o4\n",
            "Testa de o2 == o4. Esperado: False. Resultado: False\n",
            "\n",
            "Desabilita a camada LoRA, transferindo os pesos para a camada linear\n",
            "Faz o forward => o5\n",
            "Testa de o4 == o5. Esperado: True. Resultado: True. Dif máxima: 0.0\n",
            "\n",
            "Gera qualquer coisa na camada LoRA, mas não habilita ela\n",
            "Faz o forward => o6\n",
            "Testa de o5 == o6. Esperado: True. Resultado: True\n",
            "\n",
            "Habilita a camada LoRA mantendo lixo em B\n",
            "Faz o forward => o7\n",
            "Testa de o6 == o7. Esperado: False. Resultado: False\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo"
      ],
      "metadata": {
        "id": "F4H_KXv1BZ3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class OneHotMLP(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size, rank, alpha, n_logitos=2):\n",
        "    super(OneHotMLP, self).__init__()\n",
        "\n",
        "    self.n_logitos = n_logitos\n",
        "\n",
        "    #self.fc1 = nn.Linear(vocab_size, hidden_size)\n",
        "    # No início do treinamento é feito sem o uso da camada LoRA, por isso começa com use_lora=False\n",
        "    self.fc1 = LoRALayer(vocab_size, hidden_size, rank, alpha, use_lora=False)\n",
        "    self.fc2 = nn.Linear(hidden_size, n_logitos)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x tem tamanho [B, VOCAB]\n",
        "\n",
        "    # Após a primeira camada, o tamanho será [B, HIDDEN_SIZE]\n",
        "    o = self.fc1(x)\n",
        "\n",
        "    # Após ReLU, o tamanho continua [B, HIDDEN_SIZE]\n",
        "    o = self.relu(o)\n",
        "\n",
        "    # Após a segunda camada, o tamanho será [B, n_logitos]\n",
        "    o = self.fc2(o)\n",
        "\n",
        "    return o\n",
        "\n",
        "  def desabilita_lora_e_transfere_para_modelo(self):\n",
        "    self.fc1.desabilita_lora_e_transfere_para_modelo()\n",
        "\n",
        "  def toggle_lora(self, use_lora):\n",
        "    self.fc1.toggle_lora(use_lora)\n"
      ],
      "metadata": {
        "id": "tRXyYqc6BayH"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laço de treinamento"
      ],
      "metadata": {
        "id": "H7uCtKk-DpRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcula_loss_e_acuracia(model, loader):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Garante que nenhum gradiente seja calculado\n",
        "  with torch.no_grad():\n",
        "    # Coloca o modelo no modo de avaliação (não treinamento)\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    total_predicoes_corretas = 0.0\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      # Calcula a perda\n",
        "      probabilities = nn.functional.softmax(outputs, dim=1)\n",
        "      loss = criterion(probabilities, labels)\n",
        "      # Conta as predições corretas (para o cálculo da acurácia)\n",
        "      _, predicoes = torch.max(probabilities, 1)\n",
        "      total_predicoes_corretas += (predicoes == labels).sum().item()\n",
        "\n",
        "      # Acumula a perda e o número total de amostras\n",
        "      total_loss += loss.item() * inputs.size(0)\n",
        "      total_samples += inputs.size(0)\n",
        "\n",
        "  loss = total_loss / total_samples\n",
        "  acuracia = total_predicoes_corretas / total_samples\n",
        "\n",
        "  return loss, acuracia\n",
        "\n",
        "def print_loss_acuracia(msg, model, loader):\n",
        "  loss, acc = calcula_loss_e_acuracia(model, loader)\n",
        "  print(f\"{msg}. Loss: {loss}. Acc: {acc}\")\n",
        "  return loss, acc"
      ],
      "metadata": {
        "id": "YpcpoM2ODqlK"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testa o cálculo da loss em um dataloader. Ele deve ser aproximadamente ln(2):\n",
        "import math\n",
        "\n",
        "# Model instantiation\n",
        "model = OneHotMLP(VOCAB_SIZE, HIDDEN_SIZE, RANK, ALPHA, 2).to(device)\n",
        "\n",
        "print(f\"Valor esperado da loss: {math.log(2)}\")\n",
        "print_loss_acuracia(\"Antes de treinar o modelo [VAL]: \", model, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bQmSCXIF3g7",
        "outputId": "d73e6021-33ab-45d0-e719-335b75ad9206"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valor esperado da loss: 0.6931471805599453\n",
            "Antes de treinar o modelo [VAL]: . Loss: 0.6935397658348084. Acc: 0.4912\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6935397658348084, 0.4912)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp1 = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "temp2 = list(filter(lambda p: True, model.parameters()))"
      ],
      "metadata": {
        "id": "6S_cgfsdOrgS"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def train_model(model, lr=LR, num_epochs=NUM_EPOCHS):\n",
        "  model = model.to(device)\n",
        "  # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "  # Usa só os parâmetros que devem ser atualizados\n",
        "  optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  total_parametros_treinaveis = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  total_parametros = sum(p.numel() for p in model.parameters())\n",
        "  print(f'Total de parâmetros treináveis: {total_parametros_treinaveis}/{total_parametros}')\n",
        "  print_loss_acuracia(\"[TRAIN] Antes de iniciar o treinamento\", model, train_loader)\n",
        "  # Training loop\n",
        "  total_tempo_treinamento = 0\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    print(f'************************ EPOCH {epoch} ************************')\n",
        "    start_time = time.time()  # Start time of the epoch\n",
        "    model.train()\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch: {epoch}\"):\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(nn.functional.softmax(outputs, dim=1), labels)\n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    end_time = time.time()  # End time of the epoch\n",
        "    epoch_duration = end_time - start_time  # Duration of epoch\n",
        "    total_tempo_treinamento += epoch_duration\n",
        "\n",
        "    print(f'Resultados da época {epoch}')\n",
        "    print(f\"Elapsed Time: {epoch_duration:.2f} sec\")\n",
        "    print_loss_acuracia('[TRAIN]', model, train_loader)\n",
        "    print_loss_acuracia('[VAL]', model, val_loader)\n",
        "  print('*'*20)\n",
        "  print(f'Duração média durante o treino, por época: {total_tempo_treinamento/num_epochs:.2f}')"
      ],
      "metadata": {
        "id": "j7dXrtw9pDeZ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inicializa_seeds()\n",
        "model = OneHotMLP(VOCAB_SIZE, HIDDEN_SIZE, RANK, ALPHA, 2)\n",
        "train_model(model, lr=LR, num_epochs=NUM_EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBtwN3GzzxdU",
        "outputId": "44d56b42-b283-47b6-9e9a-c5e63a628aaf"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de parâmetros treináveis: 4000602/4020802\n",
            "[TRAIN] Antes de iniciar o treinamento. Loss: 0.6934093474388122. Acc: 0.50345\n",
            "************************ EPOCH 1 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1: 100%|██████████| 157/157 [00:00<00:00, 162.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 1\n",
            "Elapsed Time: 0.97 sec\n",
            "[TRAIN]. Loss: 0.5483880427360535. Acc: 0.8184\n",
            "[VAL]. Loss: 0.5536890678405761. Acc: 0.809\n",
            "************************ EPOCH 2 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 2: 100%|██████████| 157/157 [00:00<00:00, 166.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 2\n",
            "Elapsed Time: 0.95 sec\n",
            "[TRAIN]. Loss: 0.49788768944740297. Acc: 0.8201\n",
            "[VAL]. Loss: 0.505357151556015. Acc: 0.8122\n",
            "************************ EPOCH 3 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 3: 100%|██████████| 157/157 [00:01<00:00, 150.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 3\n",
            "Elapsed Time: 1.06 sec\n",
            "[TRAIN]. Loss: 0.4642197876930237. Acc: 0.85355\n",
            "[VAL]. Loss: 0.47608505234718324. Acc: 0.8414\n",
            "************************ EPOCH 4 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 4: 100%|██████████| 157/157 [00:00<00:00, 162.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 4\n",
            "Elapsed Time: 0.98 sec\n",
            "[TRAIN]. Loss: 0.4302668736457825. Acc: 0.8986\n",
            "[VAL]. Loss: 0.451534544801712. Acc: 0.8654\n",
            "************************ EPOCH 5 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 5: 100%|██████████| 157/157 [00:00<00:00, 163.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 5\n",
            "Elapsed Time: 0.97 sec\n",
            "[TRAIN]. Loss: 0.41854159569740296. Acc: 0.90735\n",
            "[VAL]. Loss: 0.44328261017799375. Acc: 0.8702\n",
            "************************ EPOCH 6 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 6: 100%|██████████| 157/157 [00:00<00:00, 164.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 6\n",
            "Elapsed Time: 0.96 sec\n",
            "[TRAIN]. Loss: 0.409752996635437. Acc: 0.917\n",
            "[VAL]. Loss: 0.4387821786403656. Acc: 0.875\n",
            "************************ EPOCH 7 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 7: 100%|██████████| 157/157 [00:00<00:00, 164.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 7\n",
            "Elapsed Time: 0.96 sec\n",
            "[TRAIN]. Loss: 0.4036974662780762. Acc: 0.92205\n",
            "[VAL]. Loss: 0.4357549529075623. Acc: 0.8794\n",
            "********************\n",
            "Duração média durante o treino, por época: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com esses resultados, dá pra imaginar que é possível ter aproximadamente 87~88% de acurácia na base de testes:"
      ],
      "metadata": {
        "id": "0MVkGRY0GO7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_loss_acuracia(\"[TESTE]\", model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5bvYFhx2AZh",
        "outputId": "5c17565b-8150-485e-9c22-4c43cbe79294"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TESTE]. Loss: 0.43823531591415404. Acc: 0.87756\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.43823531591415404, 0.87756)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testando LoRA"
      ],
      "metadata": {
        "id": "LRUoi7NVGUez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos fazer o seguinte:\n",
        "\n",
        "1. Reiniciar a seed e reinicializar o modelo.\n",
        "2. Simular ele completo com apenas uma época => Esse será o modelo pré-treinado (tem uns 78% de acurácia no conjunto de validação => esse valor inicial irá mudar dependendo da inicialização do modelo, mas basta que ele seja alguns pontos abaixo do target de 87-88% pra testarmos)\n",
        "3. Ativar as camadas LoRA.\n",
        "4. Simular novamente e verificar melhorias no tempo e se é possível chegar numa acurácia próxima a de treinar o modelo completo."
      ],
      "metadata": {
        "id": "ljKqjrqk1jJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PASSOS 1 e 2. Reiniciar a seed, reinicializar o modelo, simular com uma época"
      ],
      "metadata": {
        "id": "DlKtuPGQPAhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inicializa_seeds()\n",
        "model = OneHotMLP(VOCAB_SIZE, HIDDEN_SIZE, RANK, ALPHA, 2)\n",
        "train_model(model, lr=LR, num_epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWnXsGNo1beo",
        "outputId": "c74a16e5-ec93-4384-d4fc-986b0481b334"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de parâmetros treináveis: 4000602/4020802\n",
            "[TRAIN] Antes de iniciar o treinamento. Loss: 0.6934093474388122. Acc: 0.50345\n",
            "************************ EPOCH 1 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1: 100%|██████████| 157/157 [00:00<00:00, 158.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 1\n",
            "Elapsed Time: 1.00 sec\n",
            "[TRAIN]. Loss: 0.5483880427360535. Acc: 0.8184\n",
            "[VAL]. Loss: 0.5536890678405761. Acc: 0.809\n",
            "********************\n",
            "Duração média durante o treino, por época: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_loss_acuracia(\"[TESTE]\", model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxJfteFY2PmT",
        "outputId": "70806a23-6a74-4b64-aedd-71f8a79fcaca"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TESTE]. Loss: 0.5552642659378052. Acc: 0.80752\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5552642659378052, 0.80752)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PASSOS 3 e 4. Ativar as camadas LoRA e simular\n",
        "\n",
        "Mas antes de ativar, vamos ver como estão os pesos da camada fc1 do modelo (weight e bias). Depois de treinar os pesos devem ficar iguais. Vamos checar também as matrizes A e B."
      ],
      "metadata": {
        "id": "2t5P-xcqpo4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guarda para comparar depois\n",
        "fc1_0 = model.fc1.weight[0, 1:10].detach().clone()\n",
        "fc1_1 = model.fc1.weight[1, 1:10].detach().clone()\n",
        "b_0 = model.fc1.bias[1:10].detach().clone()\n",
        "\n",
        "print('w[0, 1:10]', model.fc1.weight[0, 1:10], '\\n')\n",
        "print('w[1, 1:10]', model.fc1.weight[1, 1:10], '\\n')\n",
        "print('b[1:10]', model.fc1.bias[1:10], '\\n')\n",
        "print('A[0: 1:10]', model.fc1.A[0, 1:10], '\\n') # Inicialização gausiana, média 0\n",
        "print('A[1: 1:10]', model.fc1.A[1, 1:10], '\\n') # Inicialização gausiana, média 0\n",
        "print('B[0: 1:10]', model.fc1.B[0, 1:10], '\\n') # Tem que ter só zeros aqui"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKslhoIwp5jA",
        "outputId": "50cd8085-eb0f-4b30-9bda-1414ea1b3085"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w[0, 1:10] tensor([-0.0016, -0.0029, -0.0058,  0.0036,  0.0067, -0.0021, -0.0037,  0.0078,\n",
            "        -0.0117], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "w[1, 1:10] tensor([ 0.0043,  0.0160,  0.0090, -0.0186,  0.0127, -0.0036,  0.0069, -0.0049,\n",
            "        -0.0183], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "b[1:10] tensor([ 0.0005, -0.0009,  0.0034, -0.0020,  0.0026,  0.0037,  0.0009,  0.0024,\n",
            "        -0.0006], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "A[0: 1:10] tensor([], device='cuda:0') \n",
            "\n",
            "A[1: 1:10] tensor([], device='cuda:0') \n",
            "\n",
            "B[0: 1:10] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.toggle_lora(True)\n",
        "train_model(model, lr=LR, num_epochs=NUM_EPOCHS_LORA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGdLjuc92R0h",
        "outputId": "7005a763-ad4e-497d-8e33-0fc4d752ffb0"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de parâmetros treináveis: 20602/4020802\n",
            "[TRAIN] Antes de iniciar o treinamento. Loss: 0.548388037109375. Acc: 0.8184\n",
            "************************ EPOCH 1 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1: 100%|██████████| 157/157 [00:00<00:00, 164.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 1\n",
            "Elapsed Time: 0.96 sec\n",
            "[TRAIN]. Loss: 0.475860825920105. Acc: 0.8482\n",
            "[VAL]. Loss: 0.48563785510063173. Acc: 0.838\n",
            "************************ EPOCH 2 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 2: 100%|██████████| 157/157 [00:00<00:00, 164.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 2\n",
            "Elapsed Time: 0.96 sec\n",
            "[TRAIN]. Loss: 0.43129615206718447. Acc: 0.88355\n",
            "[VAL]. Loss: 0.45073085746765135. Acc: 0.8574\n",
            "************************ EPOCH 3 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 3: 100%|██████████| 157/157 [00:00<00:00, 158.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 3\n",
            "Elapsed Time: 1.00 sec\n",
            "[TRAIN]. Loss: 0.4048342874526977. Acc: 0.9095\n",
            "[VAL]. Loss: 0.4346654232978821. Acc: 0.8742\n",
            "************************ EPOCH 4 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 4: 100%|██████████| 157/157 [00:00<00:00, 163.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 4\n",
            "Elapsed Time: 0.97 sec\n",
            "[TRAIN]. Loss: 0.5343850283622742. Acc: 0.76165\n",
            "[VAL]. Loss: 0.5537757751464844. Acc: 0.742\n",
            "************************ EPOCH 5 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 5: 100%|██████████| 157/157 [00:00<00:00, 158.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 5\n",
            "Elapsed Time: 1.00 sec\n",
            "[TRAIN]. Loss: 0.3852088430404663. Acc: 0.92805\n",
            "[VAL]. Loss: 0.42792939639091493. Acc: 0.8796\n",
            "************************ EPOCH 6 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 6: 100%|██████████| 157/157 [00:01<00:00, 140.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 6\n",
            "Elapsed Time: 1.13 sec\n",
            "[TRAIN]. Loss: 0.4021162752151489. Acc: 0.90795\n",
            "[VAL]. Loss: 0.440475150680542. Acc: 0.8656\n",
            "************************ EPOCH 7 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 7: 100%|██████████| 157/157 [00:00<00:00, 162.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 7\n",
            "Elapsed Time: 0.98 sec\n",
            "[TRAIN]. Loss: 0.382212971496582. Acc: 0.93105\n",
            "[VAL]. Loss: 0.4328409121990204. Acc: 0.874\n",
            "************************ EPOCH 8 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 8: 100%|██████████| 157/157 [00:00<00:00, 164.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 8\n",
            "Elapsed Time: 0.96 sec\n",
            "[TRAIN]. Loss: 0.48127295837402345. Acc: 0.8187\n",
            "[VAL]. Loss: 0.5025337201595307. Acc: 0.8014\n",
            "************************ EPOCH 9 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 9: 100%|██████████| 157/157 [00:00<00:00, 157.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 9\n",
            "Elapsed Time: 1.00 sec\n",
            "[TRAIN]. Loss: 0.401526517534256. Acc: 0.90645\n",
            "[VAL]. Loss: 0.4480236240386963. Acc: 0.8588\n",
            "************************ EPOCH 10 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 10: 100%|██████████| 157/157 [00:00<00:00, 157.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 10\n",
            "Elapsed Time: 1.01 sec\n",
            "[TRAIN]. Loss: 0.3656315544128418. Acc: 0.9484\n",
            "[VAL]. Loss: 0.43123267207145694. Acc: 0.8758\n",
            "********************\n",
            "Duração média durante o treino, por época: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_loss_acuracia(\"[TESTE]\", model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPCUetV6Im-p",
        "outputId": "b4a7999e-4739-41f5-b2ea-36b55a5fb4c0"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TESTE]. Loss: 0.433113478307724. Acc: 0.87492\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.433113478307724, 0.87492)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checa se os parâmetros weight e bias da camada LoRA mudaram depois do fine-tuning ou se apenas ajustaram as camadas A e B da camada LoRA:"
      ],
      "metadata": {
        "id": "cEkrBJDJq9UQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.all(fc1_0 == model.fc1.weight[0, 1:10])) # Tem que ser True\n",
        "print(torch.all(fc1_1 == model.fc1.weight[1, 1:10])) # Tem que ser True\n",
        "print(torch.all(b_0 == model.fc1.bias[1:10])) # Tem que ser True\n",
        "\n",
        "print('w[0, 1:10]', model.fc1.weight[0, 1:10], '\\n')\n",
        "print('w[1, 1:10]', model.fc1.weight[1, 1:10], '\\n')\n",
        "print('b[1:10]', model.fc1.bias[1:10], '\\n')\n",
        "print('A[0: 1:10]', model.fc1.A[0, 1:10], '\\n') # Nesse ponto essa matriz já foi treinada, tem que ter alguma coisa diferente\n",
        "print('A[1: 1:10]', model.fc1.A[1, 1:10], '\\n') # Nesse ponto essa matriz já foi treinada, tem que ter alguma coisa diferente\n",
        "print('B[0: 1:10]', model.fc1.B[0, 1:10], '\\n') # Nesse ponto essa matriz já foi treinada, não pode continuar zerada"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7NmepZ8rJuw",
        "outputId": "3f90f96d-8359-4871-e01f-05f9923fb016"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "w[0, 1:10] tensor([-0.0016, -0.0029, -0.0058,  0.0036,  0.0067, -0.0021, -0.0037,  0.0078,\n",
            "        -0.0117], device='cuda:0') \n",
            "\n",
            "w[1, 1:10] tensor([ 0.0043,  0.0160,  0.0090, -0.0186,  0.0127, -0.0036,  0.0069, -0.0049,\n",
            "        -0.0183], device='cuda:0') \n",
            "\n",
            "b[1:10] tensor([ 0.0005, -0.0009,  0.0034, -0.0020,  0.0026,  0.0037,  0.0009,  0.0024,\n",
            "        -0.0006], device='cuda:0') \n",
            "\n",
            "A[0: 1:10] tensor([], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "A[1: 1:10] tensor([], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "B[0: 1:10] tensor([-3.5144e-02, -5.4687e-03,  5.3679e-03,  6.3311e-05,  7.9117e-03,\n",
            "        -3.5592e-02,  4.2383e-02, -1.8123e-02, -2.9819e-02], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfere a camada LoRA para o modelo (altera weight para scale * A * B):"
      ],
      "metadata": {
        "id": "y31wevtCrjaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.desabilita_lora_e_transfere_para_modelo()"
      ],
      "metadata": {
        "id": "WNpDTMStr7Fq"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.all(fc1_0 == model.fc1.weight[0, 1:10])) # Tem que ser False\n",
        "print(torch.all(fc1_1 == model.fc1.weight[1, 1:10])) # Tem que ser False\n",
        "print(torch.all(b_0 == model.fc1.bias[1:10])) # Tem que ser True\n",
        "\n",
        "print('w[0, 1:10]', model.fc1.weight[0, 1:10], '\\n')\n",
        "print('w[1, 1:10]', model.fc1.weight[1, 1:10], '\\n')\n",
        "print('b[1:10]', model.fc1.bias[1:10], '\\n')\n",
        "print('A[0: 1:10]', model.fc1.A[0, 1:10], '\\n')\n",
        "print('A[1: 1:10]', model.fc1.A[1, 1:10], '\\n')\n",
        "print('B[0: 1:10]', model.fc1.B[0, 1:10], '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEleJl_EryjJ",
        "outputId": "b1150eb6-673f-4199-9b11-335cf954f28b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(False, device='cuda:0')\n",
            "tensor(False, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "w[0, 1:10] tensor([ 0.0265,  0.0014, -0.0101,  0.0036,  0.0004,  0.0264, -0.0376,  0.0223,\n",
            "         0.0122], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "w[1, 1:10] tensor([-0.0787,  0.0030,  0.0216, -0.0185,  0.0313, -0.0877,  0.1070, -0.0477,\n",
            "        -0.0887], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "b[1:10] tensor([ 0.0005, -0.0009,  0.0034, -0.0020,  0.0026,  0.0037,  0.0009,  0.0024,\n",
            "        -0.0006], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "A[0: 1:10] tensor([]) \n",
            "\n",
            "A[1: 1:10] tensor([]) \n",
            "\n",
            "B[0: 1:10] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora que já jogou os dados de A e B para weight, calcula novamente a acurácia (deve estar igual o último cálculo)"
      ],
      "metadata": {
        "id": "IIMIRMM1tTJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_loss_acuracia(\"[TESTE]\", model, test_loader)"
      ],
      "metadata": {
        "id": "3SCJdey2-b2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "796b055c-9b5b-4e2f-a17a-2fe7665b4f65"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TESTE]. Loss: 0.433113478307724. Acc: 0.87492\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.433113478307724, 0.87492)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    }
  ]
}