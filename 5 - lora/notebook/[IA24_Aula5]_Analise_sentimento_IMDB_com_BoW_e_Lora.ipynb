{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Análise de sentimentos na base do IMDB usando BoW e LORA\n",
        "\n",
        "Leandro Carísio Fernandes"
      ],
      "metadata": {
        "id": "RG14e03lu4jH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parâmetros"
      ],
      "metadata": {
        "id": "c0ScMiEPvR9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_VOCAB_SIZE = 20000  # Máximo de palavras no vocabulário\n",
        "VOCAB_SIZE = -1         # Será ajustado automaticamente depois do cálculo do vocabulário\n",
        "\n",
        "HIDDEN_SIZE = 200       # Tamanho da camada oculta\n",
        "\n",
        "LR = 0.1                # Learning rate\n",
        "NUM_EPOCHS = 7          # Número de épocas para rodar o treinamento completo. Após 7 épocas já não tem melhoras significativas no dataset de validação\n",
        "NUM_EPOCHS_LORA = 10    # Número de épocas para rodar fine-tuning com LoRA (será executado após apenas uma época de treinamento completo)\n",
        "RANK = 1                # Rank que será usado. Quanto menor, menos parâmetros serão treinados no fine-tuning\n",
        "ALPHA = 1               # Usado pro cálculo do fator de escala na matriz delta_w. Escala = alpha/rank"
      ],
      "metadata": {
        "id": "125pyW-kvZpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "    print('GPU:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "    print('using CPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-ArYTafDtF8",
        "outputId": "07a37fa7-5ef1-4010-d992-7aafed0a88e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seed"
      ],
      "metadata": {
        "id": "mFb7YhkivUEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def inicializa_seeds():\n",
        "  random.seed(123)\n",
        "  np.random.seed(123)\n",
        "  torch.manual_seed(123)\n",
        "\n",
        "inicializa_seeds()"
      ],
      "metadata": {
        "id": "0EXRU7oxvVcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download dos dados"
      ],
      "metadata": {
        "id": "lqMQ8UORvALq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gGiL0YCu2yu",
        "outputId": "52065e55-873d-47d2-f8d9-48fccec94230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-12 13:31:57--  http://files.fast.ai/data/aclImdb.tgz\n",
            "Resolving files.fast.ai (files.fast.ai)... 104.26.3.19, 172.67.69.159, 104.26.2.19, ...\n",
            "Connecting to files.fast.ai (files.fast.ai)|104.26.3.19|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://files.fast.ai/data/aclImdb.tgz [following]\n",
            "--2024-04-12 13:31:57--  https://files.fast.ai/data/aclImdb.tgz\n",
            "Connecting to files.fast.ai (files.fast.ai)|104.26.3.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 145982645 (139M) [application/x-gtar-compressed]\n",
            "Saving to: ‘aclImdb.tgz’\n",
            "\n",
            "aclImdb.tgz         100%[===================>] 139.22M  98.4MB/s    in 1.4s    \n",
            "\n",
            "2024-04-12 13:31:59 (98.4 MB/s) - ‘aclImdb.tgz’ saved [145982645/145982645]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz\n",
        "!tar -xzf aclImdb.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separação em treino/validação/teste"
      ],
      "metadata": {
        "id": "poFyothnvGQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "max_valid = 5000\n",
        "\n",
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "x_valid = x_train[-max_valid:]\n",
        "y_valid = y_train[-max_valid:]\n",
        "x_train = x_train[:-max_valid]\n",
        "y_train = y_train[:-max_valid]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('\\n3 primeiras amostras treino:')\n",
        "for x, y in zip(x_train[:3], y_train[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('\\n3 últimas amostras treino:')\n",
        "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('\\n3 primeiras amostras validação:')\n",
        "for x, y in zip(x_test[:3], y_test[:3]):    # No código original, estava x_valid\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('\\n3 últimas amostras validação:')\n",
        "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
        "    print(y, x[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXjBm_PCvIpu",
        "outputId": "b82af340-3fd7-4749-c34f-caf52ab691b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "\n",
            "3 primeiras amostras treino:\n",
            "False Some of the filmmakers who are participating in this series have made some really great films but th\n",
            "False The original Boogeyman was a silly but entertaining supernatural slasher flick. It was by no means a\n",
            "True This tender beautifully crafted production delved deep down bitter sweet into my being. The irrevere\n",
            "\n",
            "3 últimas amostras treino:\n",
            "False Bathebo, you big dope.<br /><br />This is the WORST piece of crap I've seen in a long time. I have j\n",
            "True They filmed this movie out on long Island, where I grew up. My brother and his girlfriend were extra\n",
            "True I am amazed that movies like this can still be made. I watch all kinds of movies all the time with m\n",
            "\n",
            "3 primeiras amostras validação:\n",
            "True The Book of Life was rather like a short snack, whetting the appetite for Hartley's next full length\n",
            "True In the ever growing film genre of comic book adaptations, Blade is by far one of the best realised, \n",
            "True Octavio Paz, Mexican poet, writer, and diplomat, who received the Nobel Prize for Literature in 1990\n",
            "\n",
            "3 últimas amostras validação:\n",
            "True The star of this film is the screenplay. Attention to detail for the period in dress, language ,soci\n",
            "True Mukhsin is a beautiful movie about a first love story. Everyone probably has one, and this is writer\n",
            "True I too have gone thru very painful personal loss (Twice) and this movie portrays the gut wrenching re\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizador e encoder"
      ],
      "metadata": {
        "id": "chgWKz5avohq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Função gerada com ajuda do ChatGPT\n",
        "import re\n",
        "\n",
        "def tokenizar(frase):\n",
        "    # return frase.split() # Usar esse return para uma tokenização simples (apenas split)\n",
        "    # Converter a frase para minúsculo\n",
        "    frase = frase.lower()\n",
        "    # Remover caracteres especiais e pontuação usando expressões regulares\n",
        "    frase = re.sub(r'[^\\w\\s]', '', frase)\n",
        "    # Dividir a frase em palavras\n",
        "    palavras = frase.split()\n",
        "    return palavras\n",
        "\n",
        "# Exemplo de uso\n",
        "frase = \"Olá! Como você está? Eu estou bem, obrigado.\"\n",
        "palavras = tokenizar(frase)\n",
        "print(palavras)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWk4r_ZSvqbA",
        "outputId": "9be5cf80-eeb6-482b-84bc-0e8411ceccf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['olá', 'como', 'você', 'está', 'eu', 'estou', 'bem', 'obrigado']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "idx_amostras = list(range(len(x_train)))\n",
        "\n",
        "counter_palavras = Counter()\n",
        "for review in x_train:\n",
        "    # Com tokenizador:\n",
        "    counter_palavras.update(tokenizar(review))\n",
        "\n",
        "# Cria um vocabulário com os tokens mais frequentes\n",
        "most_frequent_words = [\"<UNK>\"] + sorted(counter_palavras, key=counter_palavras.get, reverse=True)[:MAX_VOCAB_SIZE-1]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words)} # word é indexado de 0 (UNK) até MAX_VOCAB_SIZE\n",
        "VOCAB_SIZE = len(vocab.keys()) # Se MAX_VOCAB_SIZE for muito grande, é possível que o vocabulário não tenha palavras suficientes. Então ajusta pro tamanho correto"
      ],
      "metadata": {
        "id": "vlmakPK9vx8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_frequent_words[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9FJlZwLw6ia",
        "outputId": "d7d81dbb-91af-47e3-fae5-e298ba61fac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<UNK>', 'the', 'a', 'and', 'of', 'to', 'is', 'in', 'it', 'i']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "  return [vocab.get(word, 0) for word in tokenizar(sentence)] # 0 for OOV\n",
        "\n",
        "encode_sentence(\"I like Pizza PALAVRAINEXISTENTE\", vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05dysGCJxZiZ",
        "outputId": "f6bd3f4d-5b12-4132-9d6b-e9aa017fb5e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9, 38, 8027, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definição da classe Dataset"
      ],
      "metadata": {
        "id": "h2iDcu2gvewV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import List\n",
        "\n",
        "class ImdbDataset(Dataset):\n",
        "  def __init__(self, x_data: List[str], y_data: List[bool], vocab) -> None:\n",
        "    self.vocab = vocab\n",
        "\n",
        "    tamanho_vocab = len(vocab.keys()) # isso dá igual VOCAB_SIZE. Pra não pegar da variável global\n",
        "\n",
        "    # Como as bases cabem em memória, faz cache\n",
        "    self.x = []\n",
        "    for review in x_data:\n",
        "        x = torch.zeros(tamanho_vocab)\n",
        "        x[encode_sentence(review, self.vocab)] = 1\n",
        "        self.x.append(x)\n",
        "\n",
        "    self.y = None if y_data is None else [torch.tensor(1 if y_is_true else 0) for y_is_true in y_data]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "BdX510FKvhFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checa a implementação do dataset:\n",
        "\n",
        "x_temp, y_temp = x_train[:10], y_train[0:10]\n",
        "dataset_temp = ImdbDataset(x_temp, y_temp, vocab)\n",
        "\n",
        "for i in range(len(x_temp)):\n",
        "  encoded_x = encode_sentence(x_temp[i], vocab)\n",
        "  x = torch.zeros(VOCAB_SIZE)\n",
        "  x[encoded_x] = 1\n",
        "  print(torch.all(x == dataset_temp[i][0]))\n",
        "  print(torch.tensor(y_temp[i]) == dataset_temp[i][1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eTLjBC9zii-",
        "outputId": "abfee5a9-52de-4857-806c-353123772f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader"
      ],
      "metadata": {
        "id": "4PU1eUs6A8dQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "batch_size = 128\n",
        "# define dataloaders\n",
        "train_data = ImdbDataset(x_train, y_train, vocab)\n",
        "test_data = ImdbDataset(x_test, y_test, vocab)\n",
        "val_data = ImdbDataset(x_valid, y_valid, vocab)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUAx3z_SA9eq",
        "outputId": "67bbb9d9-482b-4247-9936-2134ed39174f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8.43 s, sys: 2.19 s, total: 10.6 s\n",
            "Wall time: 10.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementação camada LoRA"
      ],
      "metadata": {
        "id": "A7N5oBq6UGUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class LoRALayer(nn.Module):\n",
        "  def inicializa_pesos_LoRA(self):\n",
        "    # Parâmetros LoRA\n",
        "    self.A = nn.Parameter(torch.Tensor(self.output_dim, self.rank), requires_grad=self.use_lora)\n",
        "    self.B = nn.Parameter(torch.Tensor(self.rank, self.input_dim), requires_grad=self.use_lora)\n",
        "\n",
        "    nn.init.normal_(self.A, mean=0.0)\n",
        "    nn.init.zeros_(self.B)\n",
        "\n",
        "  def __init__(self, input_dim, output_dim, rank, alpha=1.0, use_lora=False):\n",
        "    super(LoRALayer, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.rank = rank\n",
        "    self.alpha = alpha\n",
        "    self.use_lora = use_lora\n",
        "\n",
        "    self.scale = self.alpha/self.rank\n",
        "\n",
        "    # Parâmetros originais da camada linear\n",
        "    # Note que o nn.Parameter está no formato output_dim x input_dim,\n",
        "    # e não input_dim x output_dim como era de se esperar\n",
        "    # Isso é feito pois o PyTorch implementa os pesos da camada linear\n",
        "    # dessa forma. Em vez de calcular x*A + b, ele faz x * A_T + b,\n",
        "    # onde _T é a transposta. Por isso é necessário inverter isso\n",
        "    self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
        "    self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
        "\n",
        "    # Inicialização\n",
        "    nn.init.xavier_normal_(self.weight)\n",
        "    nn.init.uniform_(self.bias, 0, 0)\n",
        "    self.inicializa_pesos_LoRA()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Se estiver usando LoRA, é necessário considerar os pesos modificados\n",
        "    if self.use_lora:\n",
        "      delta_w = self.scale * (self.A @ self.B)\n",
        "      modified_weight = self.weight + delta_w\n",
        "      return nn.functional.linear(x, modified_weight, self.bias)\n",
        "    else:\n",
        "      # Caso contrário, usa os pesos originais\n",
        "      return nn.functional.linear(x, self.weight, self.bias)\n",
        "      # Obs.: O controle dos gradientes é feito dentro do toggle_lora\n",
        "\n",
        "\n",
        "  def toggle_lora(self, use_lora=False):\n",
        "    self.use_lora = use_lora\n",
        "    # Desabilita os gradientes de weight e bias se estiver usando a camada LoRA\n",
        "    self.weight.requires_grad = not self.use_lora\n",
        "    self.bias.requires_grad = not self.use_lora\n",
        "    # Habilita os gradientes de A e B se estiver usando a camada LoRA\n",
        "    self.A.requires_grad = self.use_lora\n",
        "    self.B.requires_grad = self.use_lora\n",
        "\n",
        "  def desabilita_lora_e_transfere_para_modelo(self):\n",
        "    # A ideia desse método é desabilitar o LoRA e alterar o peso original\n",
        "    # para considerar o treinamento usando LoRA\n",
        "    delta_w = self.scale * self.A @ self.B\n",
        "    self.weight.data += delta_w.data\n",
        "\n",
        "    self.toggle_lora(False)\n",
        "    self.inicializa_pesos_LoRA()"
      ],
      "metadata": {
        "id": "VGUtufOHUFyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando as dimensões\n",
        "# Pega um batch e passa o primeiro conjunto de inputs numa camada LoRA ativada:\n",
        "batch_teste = next(iter(test_loader))\n",
        "camada_lora = LoRALayer(VOCAB_SIZE, HIDDEN_SIZE, rank=10, use_lora=True)\n",
        "o1 = camada_lora(batch_teste[0][0])\n",
        "print('Cria uma camada LoRA e ativa o uso da camada (B é inicializado com zeros)')\n",
        "print('Faz o forward usando a camada => o1\\n')\n",
        "\n",
        "# Desativa a camada LoRA e passa de novo o mesmo input:\n",
        "camada_lora.toggle_lora(False)\n",
        "o2 = camada_lora(batch_teste[0][0])\n",
        "print('Desativa o uso da camada LoRA (faz a conta só com os pesos normais)')\n",
        "print('Faz o forward => o2')\n",
        "print(f'Testa de o1 == o2. Esperado: True. Resultado: {torch.all(o1 == o2)}\\n')\n",
        "\n",
        "# Na inicialização, B é tudo zero.\n",
        "# Altera diretamente o B com dados aleatórios para que a camada LoRA mude o resultado\n",
        "# Entretanto, mantém a camada LoRA desativada\n",
        "nn.init.normal_(camada_lora.B, mean=0.0)\n",
        "camada_lora.toggle_lora(False)\n",
        "o3 = camada_lora(batch_teste[0][0])\n",
        "print('Altera B para algum valor aleatório e mantém a camada LoRA desativada')\n",
        "print('Faz o forward => o3')\n",
        "print(f'Testa de o2 == o3. Esperado: True. Resultado: {torch.all(o2 == o3)}\\n')\n",
        "\n",
        "# Agora abilita a camada LoRA. Mantém B populado com algum valor\n",
        "camada_lora.toggle_lora(True)\n",
        "o4 = camada_lora(batch_teste[0][0])\n",
        "print('Mantém B com valores e ativa a camada LoRA')\n",
        "print('Faz o forward => o4')\n",
        "print(f'Testa de o2 == o4. Esperado: False. Resultado: {torch.all(o2 == o4)}\\n')\n",
        "\n",
        "# Agora desabilita a camada LoRA transferindo os resultados para os pesos originais\n",
        "# e calcula novamente\n",
        "camada_lora.desabilita_lora_e_transfere_para_modelo()\n",
        "o5 = camada_lora(batch_teste[0][0])\n",
        "print('Desabilita a camada LoRA, transferindo os pesos para a camada linear')\n",
        "print('Faz o forward => o5')\n",
        "print(f'Testa de o4 == o5. Esperado: True. Resultado: {torch.all(o4 == o5)}. Dif máxima: {torch.max((o4-o5).abs())}\\n')\n",
        "\n",
        "# Gera qualquer coisa na camada LoRA, mas não habilita ela. Não tem que fazer diferença\n",
        "nn.init.normal_(camada_lora.B, mean=0)\n",
        "o6 = camada_lora(batch_teste[0][0])\n",
        "print('Gera qualquer coisa na camada LoRA, mas não habilita ela')\n",
        "print('Faz o forward => o6')\n",
        "print(f'Testa de o5 == o6. Esperado: True. Resultado: {torch.all(o5 == o6)}\\n')\n",
        "\n",
        "# Agora habilita a camada LoRA sem inicializá-la. Tem lixo lá por conta da\n",
        "# inicialização dos pesos de B com dados (teste anterior)\n",
        "camada_lora.toggle_lora(True)\n",
        "o7 = camada_lora(batch_teste[0][0])\n",
        "print('Habilita a camada LoRA mantendo lixo em B')\n",
        "print('Faz o forward => o7')\n",
        "print(f'Testa de o6 == o7. Esperado: False. Resultado: {torch.all(o6 == o7)}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4keqE7K-YXVY",
        "outputId": "a986b946-3923-41de-d8c2-6efe5b76a0a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cria uma camada LoRA e ativa o uso da camada (B é inicializado com zeros)\n",
            "Faz o forward usando a camada => o1\n",
            "\n",
            "Desativa o uso da camada LoRA (faz a conta só com os pesos normais)\n",
            "Faz o forward => o2\n",
            "Testa de o1 == o2. Esperado: True. Resultado: True\n",
            "\n",
            "Altera B para algum valor aleatório e mantém a camada LoRA desativada\n",
            "Faz o forward => o3\n",
            "Testa de o2 == o3. Esperado: True. Resultado: True\n",
            "\n",
            "Mantém B com valores e ativa a camada LoRA\n",
            "Faz o forward => o4\n",
            "Testa de o2 == o4. Esperado: False. Resultado: False\n",
            "\n",
            "Desabilita a camada LoRA, transferindo os pesos para a camada linear\n",
            "Faz o forward => o5\n",
            "Testa de o4 == o5. Esperado: True. Resultado: False. Dif máxima: 9.5367431640625e-07\n",
            "\n",
            "Gera qualquer coisa na camada LoRA, mas não habilita ela\n",
            "Faz o forward => o6\n",
            "Testa de o5 == o6. Esperado: True. Resultado: True\n",
            "\n",
            "Habilita a camada LoRA mantendo lixo em B\n",
            "Faz o forward => o7\n",
            "Testa de o6 == o7. Esperado: False. Resultado: False\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo"
      ],
      "metadata": {
        "id": "F4H_KXv1BZ3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class OneHotMLP(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size, rank, alpha, n_logitos=2):\n",
        "    super(OneHotMLP, self).__init__()\n",
        "\n",
        "    self.n_logitos = n_logitos\n",
        "\n",
        "    #self.fc1 = nn.Linear(vocab_size, hidden_size)\n",
        "    # No início do treinamento é feito sem o uso da camada LoRA, por isso começa com use_lora=False\n",
        "    self.fc1 = LoRALayer(vocab_size, hidden_size, rank, alpha, use_lora=False)\n",
        "    self.fc2 = nn.Linear(hidden_size, n_logitos)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x tem tamanho [B, VOCAB]\n",
        "\n",
        "    # Após a primeira camada, o tamanho será [B, HIDDEN_SIZE]\n",
        "    o = self.fc1(x)\n",
        "\n",
        "    # Após ReLU, o tamanho continua [B, HIDDEN_SIZE]\n",
        "    o = self.relu(o)\n",
        "\n",
        "    # Após a segunda camada, o tamanho será [B, n_logitos]\n",
        "    o = self.fc2(o)\n",
        "\n",
        "    return o\n",
        "\n",
        "  def desabilita_lora_e_transfere_para_modelo(self):\n",
        "    self.fc1.desabilita_lora_e_transfere_para_modelo()\n",
        "\n",
        "  def toggle_lora(self, use_lora):\n",
        "    self.fc1.toggle_lora(use_lora)\n"
      ],
      "metadata": {
        "id": "tRXyYqc6BayH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laço de treinamento"
      ],
      "metadata": {
        "id": "H7uCtKk-DpRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcula_loss_e_acuracia(model, loader):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Garante que nenhum gradiente seja calculado\n",
        "  with torch.no_grad():\n",
        "    # Coloca o modelo no modo de avaliação (não treinamento)\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    total_predicoes_corretas = 0.0\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      # Calcula a perda\n",
        "      probabilities = nn.functional.softmax(outputs, dim=1)\n",
        "      loss = criterion(probabilities, labels)\n",
        "      # Conta as predições corretas (para o cálculo da acurácia)\n",
        "      _, predicoes = torch.max(probabilities, 1)\n",
        "      total_predicoes_corretas += (predicoes == labels).sum().item()\n",
        "\n",
        "      # Acumula a perda e o número total de amostras\n",
        "      total_loss += loss.item() * inputs.size(0)\n",
        "      total_samples += inputs.size(0)\n",
        "\n",
        "  loss = total_loss / total_samples\n",
        "  acuracia = total_predicoes_corretas / total_samples\n",
        "\n",
        "  return loss, acuracia\n",
        "\n",
        "def print_loss_acuracia(msg, model, loader):\n",
        "  loss, acc = calcula_loss_e_acuracia(model, loader)\n",
        "  print(f\"{msg}. Loss: {loss}. Acc: {acc}\")\n",
        "  return loss, acc"
      ],
      "metadata": {
        "id": "YpcpoM2ODqlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testa o cálculo da loss em um dataloader. Ele deve ser aproximadamente ln(2):\n",
        "import math\n",
        "\n",
        "# Model instantiation\n",
        "model = OneHotMLP(VOCAB_SIZE, HIDDEN_SIZE, RANK, ALPHA, 2).to(device)\n",
        "\n",
        "print(f\"Valor esperado da loss: {math.log(2)}\")\n",
        "print_loss_acuracia(\"Antes de treinar o modelo [VAL]: \", model, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bQmSCXIF3g7",
        "outputId": "9cbd950d-236c-41c7-9e55-f36cacdcb62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valor esperado da loss: 0.6931471805599453\n",
            "Antes de treinar o modelo [VAL]: . Loss: 0.692955906677246. Acc: 0.5194\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.692955906677246, 0.5194)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp1 = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "temp2 = list(filter(lambda p: True, model.parameters()))"
      ],
      "metadata": {
        "id": "6S_cgfsdOrgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def train_model(model, lr=LR, num_epochs=NUM_EPOCHS):\n",
        "  model = model.to(device)\n",
        "  # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "  # Usa só os parâmetros que devem ser atualizados\n",
        "  optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  total_parametros_treinaveis = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  total_parametros = sum(p.numel() for p in model.parameters())\n",
        "  print(f'Total de parâmetros treináveis: {total_parametros_treinaveis}/{total_parametros}')\n",
        "  print_loss_acuracia(\"[TRAIN] Antes de iniciar o treinamento\", model, train_loader)\n",
        "  # Training loop\n",
        "  total_tempo_treinamento = 0\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    print(f'************************ EPOCH {epoch} ************************')\n",
        "    start_time = time.time()  # Start time of the epoch\n",
        "    model.train()\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch: {epoch}\"):\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(nn.functional.softmax(outputs, dim=1), labels)\n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    end_time = time.time()  # End time of the epoch\n",
        "    epoch_duration = end_time - start_time  # Duration of epoch\n",
        "    total_tempo_treinamento += epoch_duration\n",
        "\n",
        "    print(f'Resultados da época {epoch}')\n",
        "    print(f\"Elapsed Time: {epoch_duration:.2f} sec\")\n",
        "    print_loss_acuracia('[TRAIN]', model, train_loader)\n",
        "    print_loss_acuracia('[VAL]', model, val_loader)\n",
        "  print('*'*20)\n",
        "  print(f'Duração média durante o treino, por época: {total_tempo_treinamento/num_epochs:.2f}')"
      ],
      "metadata": {
        "id": "HplzBXoLEYJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inicializa_seeds()\n",
        "model = OneHotMLP(VOCAB_SIZE, HIDDEN_SIZE, RANK, ALPHA, 2)\n",
        "train_model(model, lr=LR, num_epochs=NUM_EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBtwN3GzzxdU",
        "outputId": "a80dea97-3334-4796-f0d6-1499b325c046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de parâmetros treináveis: 4000602/4020802\n",
            "[TRAIN] Antes de iniciar o treinamento. Loss: 0.6927644582748413. Acc: 0.51385\n",
            "************************ EPOCH 1 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1: 100%|██████████| 157/157 [00:01<00:00, 146.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 1\n",
            "Elapsed Time: 1.08 sec\n",
            "[TRAIN]. Loss: 0.5444812890052796. Acc: 0.82305\n",
            "[VAL]. Loss: 0.5499884818077088. Acc: 0.8136\n",
            "************************ EPOCH 2 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 2: 100%|██████████| 157/157 [00:01<00:00, 152.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 2\n",
            "Elapsed Time: 1.04 sec\n",
            "[TRAIN]. Loss: 0.48147224526405336. Acc: 0.84685\n",
            "[VAL]. Loss: 0.49002674050331113. Acc: 0.837\n",
            "************************ EPOCH 3 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 3: 100%|██████████| 157/157 [00:00<00:00, 171.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 3\n",
            "Elapsed Time: 0.92 sec\n",
            "[TRAIN]. Loss: 0.44719675750732424. Acc: 0.88325\n",
            "[VAL]. Loss: 0.4653233910560608. Acc: 0.8542\n",
            "************************ EPOCH 4 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 4: 100%|██████████| 157/157 [00:00<00:00, 158.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 4\n",
            "Elapsed Time: 0.99 sec\n",
            "[TRAIN]. Loss: 0.42865587148666384. Acc: 0.89935\n",
            "[VAL]. Loss: 0.4511946942329407. Acc: 0.8688\n",
            "************************ EPOCH 5 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 5: 100%|██████████| 157/157 [00:00<00:00, 170.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 5\n",
            "Elapsed Time: 0.93 sec\n",
            "[TRAIN]. Loss: 0.4206233283996582. Acc: 0.90465\n",
            "[VAL]. Loss: 0.44712929768562315. Acc: 0.8708\n",
            "************************ EPOCH 6 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 6: 100%|██████████| 157/157 [00:00<00:00, 160.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 6\n",
            "Elapsed Time: 0.98 sec\n",
            "[TRAIN]. Loss: 0.41281407041549684. Acc: 0.91285\n",
            "[VAL]. Loss: 0.44706996631622314. Acc: 0.8654\n",
            "************************ EPOCH 7 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 7: 100%|██████████| 157/157 [00:00<00:00, 171.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 7\n",
            "Elapsed Time: 0.92 sec\n",
            "[TRAIN]. Loss: 0.4009817371368408. Acc: 0.9268\n",
            "[VAL]. Loss: 0.43852525539398196. Acc: 0.8754\n",
            "********************\n",
            "Duração média durante o treino, por época: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com esses resultados, dá pra imaginar que é possível ter aproximadamente 87~88% de acurácia na base de testes:"
      ],
      "metadata": {
        "id": "0MVkGRY0GO7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_loss_acuracia(\"[TESTE]\", model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5bvYFhx2AZh",
        "outputId": "ac9e9ab7-c672-4e21-d696-2c565e81285d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TESTE]. Loss: 0.4384763907623291. Acc: 0.87576\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4384763907623291, 0.87576)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testando LoRA"
      ],
      "metadata": {
        "id": "LRUoi7NVGUez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos fazer o seguinte:\n",
        "\n",
        "1. Reiniciar a seed e reinicializar o modelo.\n",
        "2. Simular ele completo com apenas uma época => Esse será o modelo pré-treinado (tem uns 78% de acurácia no conjunto de validação => esse valor inicial irá mudar dependendo da inicialização do modelo, mas basta que ele seja alguns pontos abaixo do target de 87-88% pra testarmos)\n",
        "3. Ativar as camadas LoRA.\n",
        "4. Simular novamente e verificar melhorias no tempo e se é possível chegar numa acurácia próxima a de treinar o modelo completo."
      ],
      "metadata": {
        "id": "ljKqjrqk1jJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PASSOS 1 e 2. Reiniciar a seed, reinicializar o modelo, simular com uma época"
      ],
      "metadata": {
        "id": "DlKtuPGQPAhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inicializa_seeds()\n",
        "model = OneHotMLP(VOCAB_SIZE, HIDDEN_SIZE, RANK, ALPHA, 2)\n",
        "train_model(model, lr=LR, num_epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWnXsGNo1beo",
        "outputId": "555769d4-faef-4415-c364-036ec4f2367b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de parâmetros treináveis: 4000602/4020802\n",
            "[TRAIN] Antes de iniciar o treinamento. Loss: 0.6927644582748413. Acc: 0.51385\n",
            "************************ EPOCH 1 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1: 100%|██████████| 157/157 [00:00<00:00, 166.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 1\n",
            "Elapsed Time: 0.95 sec\n",
            "[TRAIN]. Loss: 0.5444812890052796. Acc: 0.82305\n",
            "[VAL]. Loss: 0.5499884818077088. Acc: 0.8136\n",
            "********************\n",
            "Duração média durante o treino, por época: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_loss_acuracia(\"[TESTE]\", model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxJfteFY2PmT",
        "outputId": "7697d9d0-c203-41c9-e471-196736f6d556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TESTE]. Loss: 0.5512490636062622. Acc: 0.81256\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5512490636062622, 0.81256)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PASSOS 3 e 4. Ativar as camadas LoRA e simular\n",
        "\n",
        "Mas antes de ativar, vamos ver como estão os pesos da camada fc1 do modelo (weight e bias). Depois de treinar os pesos devem ficar iguais. Vamos checar também as matrizes A e B."
      ],
      "metadata": {
        "id": "2t5P-xcqpo4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guarda para comparar depois\n",
        "fc1_0 = model.fc1.weight[0, 1:10].detach().clone()\n",
        "fc1_1 = model.fc1.weight[1, 1:10].detach().clone()\n",
        "b_0 = model.fc1.bias[1:10].detach().clone()\n",
        "\n",
        "print('w[0, 1:10]', model.fc1.weight[0, 1:10], '\\n')\n",
        "print('w[1, 1:10]', model.fc1.weight[1, 1:10], '\\n')\n",
        "print('b[1:10]', model.fc1.bias[1:10], '\\n')\n",
        "print('A[0: 1:10]', model.fc1.A[0, 1:10], '\\n') # Inicialização gausiana, média 0\n",
        "print('A[1: 1:10]', model.fc1.A[1, 1:10], '\\n') # Inicialização gausiana, média 0\n",
        "print('B[0: 1:10]', model.fc1.B[0, 1:10], '\\n') # Tem que ter só zeros aqui"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKslhoIwp5jA",
        "outputId": "ae74b7b8-af5e-4aba-f969-da27aedc300a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w[0, 1:10] tensor([-0.0018, -0.0030, -0.0058,  0.0035,  0.0066, -0.0022, -0.0037,  0.0076,\n",
            "        -0.0119], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "w[1, 1:10] tensor([ 0.0075,  0.0194,  0.0114, -0.0156,  0.0159, -0.0014,  0.0096, -0.0025,\n",
            "        -0.0152], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "b[1:10] tensor([ 3.5952e-03, -4.8239e-04,  2.0265e-03,  3.6117e-03,  4.5666e-03,\n",
            "         1.0681e-03,  1.7175e-03,  3.7140e-03, -4.0021e-05], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "A[0: 1:10] tensor([], device='cuda:0') \n",
            "\n",
            "A[1: 1:10] tensor([], device='cuda:0') \n",
            "\n",
            "B[0: 1:10] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.toggle_lora(True)\n",
        "train_model(model, lr=LR, num_epochs=NUM_EPOCHS_LORA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGdLjuc92R0h",
        "outputId": "67948b43-904e-4bf0-9f6e-38b74729543c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de parâmetros treináveis: 20602/4020802\n",
            "[TRAIN] Antes de iniciar o treinamento. Loss: 0.544481286239624. Acc: 0.82305\n",
            "************************ EPOCH 1 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1: 100%|██████████| 157/157 [00:00<00:00, 164.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 1\n",
            "Elapsed Time: 0.97 sec\n",
            "[TRAIN]. Loss: 0.5190781641960144. Acc: 0.7954\n",
            "[VAL]. Loss: 0.5323013854026795. Acc: 0.7778\n",
            "************************ EPOCH 2 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 2: 100%|██████████| 157/157 [00:00<00:00, 169.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 2\n",
            "Elapsed Time: 0.93 sec\n",
            "[TRAIN]. Loss: 0.46575151739120485. Acc: 0.8414\n",
            "[VAL]. Loss: 0.48098870248794556. Acc: 0.8228\n",
            "************************ EPOCH 3 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 3: 100%|██████████| 157/157 [00:00<00:00, 164.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 3\n",
            "Elapsed Time: 0.96 sec\n",
            "[TRAIN]. Loss: 0.40685406131744384. Acc: 0.90825\n",
            "[VAL]. Loss: 0.43710347862243654. Acc: 0.8758\n",
            "************************ EPOCH 4 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 4: 100%|██████████| 157/157 [00:01<00:00, 131.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 4\n",
            "Elapsed Time: 1.20 sec\n",
            "[TRAIN]. Loss: 0.41395979180336. Acc: 0.8963\n",
            "[VAL]. Loss: 0.4490214967727661. Acc: 0.855\n",
            "************************ EPOCH 5 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 5: 100%|██████████| 157/157 [00:00<00:00, 172.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 5\n",
            "Elapsed Time: 0.92 sec\n",
            "[TRAIN]. Loss: 0.41186986780166623. Acc: 0.89895\n",
            "[VAL]. Loss: 0.45581795625686644. Acc: 0.848\n",
            "************************ EPOCH 6 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 6: 100%|██████████| 157/157 [00:00<00:00, 168.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 6\n",
            "Elapsed Time: 0.94 sec\n",
            "[TRAIN]. Loss: 0.4060087486743927. Acc: 0.9054\n",
            "[VAL]. Loss: 0.45302961044311524. Acc: 0.8512\n",
            "************************ EPOCH 7 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 7: 100%|██████████| 157/157 [00:00<00:00, 170.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 7\n",
            "Elapsed Time: 0.93 sec\n",
            "[TRAIN]. Loss: 0.3738481011390686. Acc: 0.94075\n",
            "[VAL]. Loss: 0.43178628997802737. Acc: 0.8764\n",
            "************************ EPOCH 8 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 8: 100%|██████████| 157/157 [00:00<00:00, 163.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 8\n",
            "Elapsed Time: 0.97 sec\n",
            "[TRAIN]. Loss: 0.43763634238243104. Acc: 0.8659\n",
            "[VAL]. Loss: 0.48059727602005003. Acc: 0.8218\n",
            "************************ EPOCH 9 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 9: 100%|██████████| 157/157 [00:00<00:00, 166.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 9\n",
            "Elapsed Time: 0.95 sec\n",
            "[TRAIN]. Loss: 0.36940426173210145. Acc: 0.9444\n",
            "[VAL]. Loss: 0.43479630875587466. Acc: 0.8736\n",
            "************************ EPOCH 10 ************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 10: 100%|██████████| 157/157 [00:01<00:00, 141.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da época 10\n",
            "Elapsed Time: 1.12 sec\n",
            "[TRAIN]. Loss: 0.36321702942848205. Acc: 0.95065\n",
            "[VAL]. Loss: 0.43189150218963623. Acc: 0.877\n",
            "********************\n",
            "Duração média durante o treino, por época: 0.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_loss_acuracia(\"[TESTE]\", model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPCUetV6Im-p",
        "outputId": "ae8ec34a-1099-41a6-e3f4-0ceae2ae5cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TESTE]. Loss: 0.4307401481819153. Acc: 0.87744\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4307401481819153, 0.87744)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checa se os parâmetros weight e bias da camada LoRA mudaram depois do fine-tuning ou se apenas ajustaram as camadas A e B da camada LoRA:"
      ],
      "metadata": {
        "id": "cEkrBJDJq9UQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.all(fc1_0 == model.fc1.weight[0, 1:10])) # Tem que ser True\n",
        "print(torch.all(fc1_1 == model.fc1.weight[1, 1:10])) # Tem que ser True\n",
        "print(torch.all(b_0 == model.fc1.bias[1:10])) # Tem que ser True\n",
        "\n",
        "print('w[0, 1:10]', model.fc1.weight[0, 1:10], '\\n')\n",
        "print('w[1, 1:10]', model.fc1.weight[1, 1:10], '\\n')\n",
        "print('b[1:10]', model.fc1.bias[1:10], '\\n')\n",
        "print('A[0: 1:10]', model.fc1.A[0, 1:10], '\\n') # Nesse ponto essa matriz já foi treinada, tem que ter alguma coisa diferente\n",
        "print('A[1: 1:10]', model.fc1.A[1, 1:10], '\\n') # Nesse ponto essa matriz já foi treinada, tem que ter alguma coisa diferente\n",
        "print('B[0: 1:10]', model.fc1.B[0, 1:10], '\\n') # Nesse ponto essa matriz já foi treinada, não pode continuar zerada"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7NmepZ8rJuw",
        "outputId": "5743bab7-31c6-462f-c2ce-2bc661cd7b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "w[0, 1:10] tensor([-0.0018, -0.0030, -0.0058,  0.0035,  0.0066, -0.0022, -0.0037,  0.0076,\n",
            "        -0.0119], device='cuda:0') \n",
            "\n",
            "w[1, 1:10] tensor([ 0.0075,  0.0194,  0.0114, -0.0156,  0.0159, -0.0014,  0.0096, -0.0025,\n",
            "        -0.0152], device='cuda:0') \n",
            "\n",
            "b[1:10] tensor([ 3.5952e-03, -4.8239e-04,  2.0265e-03,  3.6117e-03,  4.5666e-03,\n",
            "         1.0681e-03,  1.7175e-03,  3.7140e-03, -4.0021e-05], device='cuda:0') \n",
            "\n",
            "A[0: 1:10] tensor([], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "A[1: 1:10] tensor([], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "B[0: 1:10] tensor([-0.0331,  0.0474,  0.0306,  0.0086,  0.0205, -0.0656,  0.0111, -0.0618,\n",
            "        -0.0693], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfere a camada LoRA para o modelo (altera weight para scale * A * B):"
      ],
      "metadata": {
        "id": "y31wevtCrjaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.desabilita_lora_e_transfere_para_modelo()"
      ],
      "metadata": {
        "id": "WNpDTMStr7Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.all(fc1_0 == model.fc1.weight[0, 1:10])) # Tem que ser False\n",
        "print(torch.all(fc1_1 == model.fc1.weight[1, 1:10])) # Tem que ser False\n",
        "print(torch.all(b_0 == model.fc1.bias[1:10])) # Tem que ser True\n",
        "\n",
        "print('w[0, 1:10]', model.fc1.weight[0, 1:10], '\\n')\n",
        "print('w[1, 1:10]', model.fc1.weight[1, 1:10], '\\n')\n",
        "print('b[1:10]', model.fc1.bias[1:10], '\\n')\n",
        "print('A[0: 1:10]', model.fc1.A[0, 1:10], '\\n')\n",
        "print('A[1: 1:10]', model.fc1.A[1, 1:10], '\\n')\n",
        "print('B[0: 1:10]', model.fc1.B[0, 1:10], '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEleJl_EryjJ",
        "outputId": "8bbf14fa-9021-4aad-d06a-661197f7ddc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(False, device='cuda:0')\n",
            "tensor(False, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "w[0, 1:10] tensor([ 0.0248, -0.0410, -0.0304, -0.0035, -0.0099,  0.0505, -0.0126,  0.0572,\n",
            "         0.0437], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "w[1, 1:10] tensor([-0.0707,  0.1315,  0.0838,  0.0049,  0.0643, -0.1567,  0.0357, -0.1487,\n",
            "        -0.1791], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "b[1:10] tensor([ 3.5952e-03, -4.8239e-04,  2.0265e-03,  3.6117e-03,  4.5666e-03,\n",
            "         1.0681e-03,  1.7175e-03,  3.7140e-03, -4.0021e-05], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "A[0: 1:10] tensor([]) \n",
            "\n",
            "A[1: 1:10] tensor([]) \n",
            "\n",
            "B[0: 1:10] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora que já jogou os dados de A e B para weight, calcula novamente a acurácia (deve estar igual o último cálculo)"
      ],
      "metadata": {
        "id": "IIMIRMM1tTJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_loss_acuracia(\"[TESTE]\", model, test_loader)"
      ],
      "metadata": {
        "id": "3SCJdey2-b2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dda03ee2-6f99-418f-9019-0564846ee198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TESTE]. Loss: 0.4307401481819153. Acc: 0.87744\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4307401481819153, 0.87744)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}