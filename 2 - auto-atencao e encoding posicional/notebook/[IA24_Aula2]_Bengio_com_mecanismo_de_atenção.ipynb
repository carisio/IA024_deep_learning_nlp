{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "# Exercício: Modelo de Linguagem com auto-atenção\n",
        "\n",
        "Este exercício é similar ao da aula passada, mas iremos agora treinar uma rede neural *com auto-atenção* para prever a próxima palavra de um texto, data as palavras anteriores como entrada.\n",
        "\n",
        "Na camada de auto-atenção, deve-se implementar (vide slide 34):\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "Instrucões:\n",
        "- É necessário fazer duas implementações da camada de auto-atenção: uma usando laços (ineficiente, mas fácil de entender) e outra matricial (eficiente mas difícil de entender). Usar slide 36 como referência.\n",
        "\n",
        "- Fazer um assert para garantir que o resultado das duas implementações é exatamente igual.\n",
        "\n",
        "- No treinamento, usar apenas a implementação matricial."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parâmetros"
      ],
      "metadata": {
        "id": "vkfDySL0HrhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Livros (O Guarani)\n",
        "urls = [\"https://www.gutenberg.org/ebooks/67724.txt.utf-8\", \"https://www.gutenberg.org/ebooks/67725.txt.utf-8\"]\n",
        "\n",
        "# Dados do vocabulário\n",
        "UNK = \"<unk>\"\n",
        "vocab_size_desejado_sem_UNK = 3000 # Não considera o UNK\n",
        "vocab_size = vocab_size_desejado_sem_UNK + 1\n",
        "\n",
        "# Dados de treinamento\n",
        "context_size = 9 # número de palavras de entrada. O target é a próxima palavra\n",
        "num_epochs = 10 # usado pra fazer overfit no modelo e ajudar na verificação do treinamento\n",
        "test_size = 0.2\n",
        "seed = 18\n",
        "batch_size=128\n",
        "m = 64 # tamanho dos embeddings\n",
        "h = 50 # tamanho da camada oculta\n",
        "lr = 0.05\n",
        "momentum = 0.9\n",
        "ativacao = 'relu'\n",
        "zerar_w = True\n",
        "\n",
        "comparacoes_loop = {\n",
        "    \"projecoes\": False, # Se colocar True o Colab costuma travar depois de executar\n",
        "    \"qkt\": True,\n",
        "    \"softmax\": True,\n",
        "    \"qkv\": True\n",
        "}"
      ],
      "metadata": {
        "id": "D2q4aw4MHz75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Retorna True ou False dependendo se é um parágrafo válido ou não.\n",
        "# Usado para remover parágrafos muito curtos ou outros casos específicos\n",
        "# (por exemplo, os parágrafos do índice que tem vários pontos (.....))\n",
        "def paragrafo_valido(paragrafo):\n",
        "  return len(paragrafo) > 10 and '....' not in paragrafo and '***' not in paragrafo\n",
        "\n",
        "# Dada uma URL do projeto Gutenberg, baixa o arquivo e divide o texto\n",
        "# principal em parágrafos. Retorna um array de parágrafos\n",
        "def carregar_paragrafos_livro(url, n_linhas_para_print=20):\n",
        "  # Baixar o arquivo\n",
        "  response = requests.get(url)\n",
        "  texto = response.text\n",
        "\n",
        "  # Encontrar o início e o fim do conteúdo principal do livro\n",
        "  inicio = texto.find(\"*** START OF THE PROJECT GUTENBERG EBOOK\")\n",
        "  fim = texto.find(\"*** END OF THE PROJECT GUTENBERG EBOOK\")\n",
        "\n",
        "  # Extrair o conteúdo principal do livro\n",
        "  conteudo = texto[inicio:fim].replace('\\r','')\n",
        "\n",
        "  # Dividir o conteúdo em parágrafos e processar o conteúdo\n",
        "  paragrafos = []\n",
        "\n",
        "  # Cada parágrafo é separado por dois \\n\n",
        "  # Dentro de cada parágrafo, junta as linhas e remove espaços em branco no\n",
        "  # início e fim do prágrafo.\n",
        "  for paragrafo in conteudo.split(\"\\n\\n\"):\n",
        "    paragrafo = paragrafo.replace('\\n', ' ').strip()\n",
        "    if paragrafo_valido(paragrafo):\n",
        "      paragrafos.append(paragrafo)\n",
        "\n",
        "  # Imprime (usado pra debug)\n",
        "  for p in paragrafos[0:n_linhas_para_print]:\n",
        "    print(p)\n",
        "\n",
        "  return paragrafos\n",
        "\n",
        "paragrafos = []\n",
        "for i, url in enumerate(urls, 1):\n",
        "  print(f'-------------- Livro {i} ---------------')\n",
        "  paragrafos.extend(carregar_paragrafos_livro(url))\n",
        "\n",
        "print('-------------- -------------------------')\n",
        "print(f'Total de parágrafos: {len(paragrafos)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJpvxMahIoSh",
        "outputId": "5288196c-060c-470e-91a9-1215a3de3fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- Livro 1 ---------------\n",
            "J. DE ALENCAR\n",
            "ROMANCE BRAZILEIRO\n",
            "QUINTA EDIÇÃO\n",
            "TOMO PRIMEIRO\n",
            "RIO DE JANEIRO\n",
            "B.-L. GARNIER, LIVREIRO-EDITOR\n",
            "71, RUA DO OUVIDOR, 71\n",
            "PARIS.--E. MELLIER, 17, RUA SÉGUIER.\n",
            "Ficão reservados os direitos de propriedade.\n",
            "Publicando este livro em 1857, se disse ser aquella primeira edição uma prova typographica, que algum dia talvez o autor se dispuzesse a rever.\n",
            "Esta nova edição devia dar satisfação do empenho, que a extrema benevolencia do publico ledor, tão minguado ainda, mudou em bem para divida de reconhecimento.\n",
            "Mais do que podia fiou de si o autor. Relendo a obra depois de annos, achou elle tão mau e incorrecto quando escrevera, que para bem corrigir, fora mister escrever de novo. Para tanto lhe carece o tempo e sobra o tedio de um labor ingrato.\n",
            "Cingio-se pois ás pequenas emendas que toleravão o plano da obra e o desalinho de um estylo não castigado.\n",
            "PRIMEIRA PARTE\n",
            "OS AVENTUREIROS\n",
            "De um dos cabeços da _Serra dos Órgãos_ deslisa um fio d'agua que se dirige para norte, e engrossado com os mananciaes, que recebe no seu curso de dez leguas, torna-se rio caudal.\n",
            "É o _Paquequer_: soltando de cascata em cascata, enroscando-se como uma serpente, vai depois se espreguiçar na varzea e embeber no Parahyba, que rola magestosamente em seu vasto leito.\n",
            "Dir-se-hia que vassallo e tributario desse rei das aguas, o pequeno rio, altivo e sobranceiro contra os rochedos, curva-se humildemente aos pés do suzerano. Perde então a belleza selvatica; suas ondas são calmas e serenas como as de um lago, e não se revoltão contra os barcos e as canôas que resvalão sobre ellas: escravo submisso, soffre o latego do senhor.\n",
            "Não é neste lugar que elle deve ser visto; sim tres ou quatro leguas acima de sua foz, onde é livre ainda, como o filho indomito desta patria da liberdade.\n",
            "Ahi, o _Paquequer_ lança-se rapido sobre o seu leito, e atravessa as florestas como o tapir, espumando, deixando o pello esparso pelas pontas de rochedo, e enchendo a solidão com o estampido de sua carreira. De repente, falta-lhe o espaço, foge-lhe a terra; o soberbo rio recúa um momento para concentrar as suas forças e precipita-se de um só arremesso, como o tigre sobre a presa.\n",
            "-------------- Livro 2 ---------------\n",
            "J. DE ALENCAR\n",
            "ROMANCE BRAZILEIRO\n",
            "QUINTA EDIÇÃO\n",
            "TOMO SEGUNDO\n",
            "RIO DE JANEIRO\n",
            "B.-L. GARNIER, LIVREIRO-EDITOR\n",
            "71, RUA DO OUVIDOR, 71\n",
            "PARIS.--E. MELLIER, 17, RUA SÉGUIER.\n",
            "Ficão reservados os direitos de propriedade.\n",
            "TERCEIRA PARTE\n",
            "Na segunda-feira, erão seis horas da manhã, quando D. Antonio de Mariz chamou seu filho.\n",
            "O velho fidalgo velara uma boa parte da noite; ou escrevendo ou reflectindo sobre os perigos que ameaçavão sua familia.\n",
            "Pery lhe havia contado todas as particularidades de seu encontro com os Aymorés; e o cavalheiro, que conhecia a ferocidade e espirito vingativo dessa raça selvagem, esperava a cada momento ser atacado.\n",
            "Por isso, de acordo com Alvaro, D. Diogo, com seu escudeiro Ayres Gomes, tinha tomado todas as medidas de precaução que as circumstancias e sua longa experiencia lhe aconselhavão.\n",
            "Quando seu filho entrou, o velho fidalgo acabava de sellar duas cartas que escrevêra na vespera.\n",
            "--Meu filho, disse elle com uma ligeira emoção, reflecti essa noite sobre o que nos pode acontecer, e assentei que deves partir hoje mesmo para S. Sebastião.\n",
            "--Não é possivel, senhor!... Afastais-me de vós justamente quando correis um perigo?\n",
            "--Sim! É justamente quando um grande perigo nos ameaça, que eu, chefe da casa, entendo ser do meu dever salvar o representante do meu nome e meu herdeiro legitimo, o protector de minha familia orphã.\n",
            "--Confio em Deos, meu pai, que vossos receios serão infundados; mas se elle nos quizesse submetter a tal provança, o unico lugar que compete a vosso filho e herdeiro de vosso nome é nesta casa ameaçada, ao vosso lado, para defender-vos e partilhar a vossa sorte, qualquer que ella seja.\n",
            "D. Antonio apertou seu filho ao peito.\n",
            "-------------- -------------------------\n",
            "Total de parágrafos: 4594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFVN2ihb33Rf"
      },
      "source": [
        "## Análise do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primeiro define um tokenizador que vai separar as palavras para a entrada no dataset."
      ],
      "metadata": {
        "id": "Yk70gpGULE21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenizar(texto):\n",
        "  texto = texto.lower()\n",
        "\n",
        "  # Força os 3 pontos aparecerem juntos\n",
        "  texto = texto.replace('...', 'SUBSTITUIRPORTRESPONTOS')\n",
        "\n",
        "  # Define a expressão regular que captura palavras e sinais de pontuação\n",
        "  padrao = r'\\w+|[^\\w\\s]'\n",
        "\n",
        "  # Usa o método findall para encontrar todas as ocorrências que se encaixam no padrão\n",
        "  tokens = re.findall(padrao, texto)\n",
        "\n",
        "  return tokens\n",
        "\n",
        "print(tokenizar('Teste. Será que vai manter a pontuação?'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6NzjbW2K9_3",
        "outputId": "0d6e953a-619a-4d77-bd62-d8584b696148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['teste', '.', 'será', 'que', 'vai', 'manter', 'a', 'pontuação', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contador de palavras:"
      ],
      "metadata": {
        "id": "6CCb_rsBLJeU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSRHqe3H4ZFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d921392-f669-4a2c-fd0d-c03ac4b7313a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11938"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Conta as palavras no dataset\n",
        "from collections import Counter\n",
        "\n",
        "def count_words(texts):\n",
        "  word_counts = Counter()\n",
        "  for text in texts:\n",
        "    word_counts.update(tokenizar(text))\n",
        "  return word_counts\n",
        "\n",
        "word_counts = count_words(paragrafos)\n",
        "\n",
        "len(word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "def gerar_vocabulario(paragrafos, vocab_size_sem_UNK):\n",
        "  counter = count_words(paragrafos)\n",
        "\n",
        "  # Considera apenas as palavras mais frequentes. Adiciona, na posição 0, o token UNK\n",
        "  most_frequent_words = [UNK] + sorted(counter, key=counter.get, reverse=True)[:vocab_size_sem_UNK]\n",
        "  # vocab é um mapa de palavras para o índice correspondente. O mapa leva a palavra para um índice entre [0, vocab_size]\n",
        "  # (o tamanho é vocab_size + 1), com o índice 0 apontando para UNK\n",
        "  vocab = {word: i for i, word in enumerate(most_frequent_words)}\n",
        "\n",
        "  return len(most_frequent_words), vocab, most_frequent_words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size, vocab, most_frequent_words = gerar_vocabulario(paragrafos, vocab_size_desejado_sem_UNK)\n",
        "\n",
        "print('Tamanho do vocabulário (considera UNK): ', vocab_size)\n",
        "\n",
        "print('Posição 0: ', most_frequent_words[0])\n",
        "print('Índice do UNK: ', vocab[UNK])\n",
        "print('------------')\n",
        "print('Posição 200: ', most_frequent_words[200])\n",
        "print(f'Índice de {most_frequent_words[200]}: ', vocab[most_frequent_words[200]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj4SEYbLLz0p",
        "outputId": "cb507842-4262-4397-c9a4-dcad9370cc57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho do vocabulário (considera UNK):  3001\n",
            "Posição 0:  <unk>\n",
            "Índice do UNK:  0\n",
            "------------\n",
            "Posição 200:  tambem\n",
            "Índice de tambem:  200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhbhAsZbzJ_J"
      },
      "outputs": [],
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "  # Obs.: tem que usar o mesmo tokenizador que foi gerado o vocabulário\n",
        "  return [vocab.get(word, 0) for word in tokenizar(sentence)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sentence(sentence, most_frequent_words):\n",
        "  words = [most_frequent_words[code] for code in sentence]\n",
        "  return ' '.join(words)"
      ],
      "metadata": {
        "id": "iRgb9RGXMDCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste do encode/decode\n",
        "frase = paragrafos[10]\n",
        "\n",
        "frase_encodada = encode_sentence(frase, vocab)\n",
        "frase_reconstruida = decode_sentence(frase_encodada, most_frequent_words)\n",
        "\n",
        "print('Original:')\n",
        "print(frase)\n",
        "print('Encodada:')\n",
        "print(frase_encodada)\n",
        "print('Reconstruída:')\n",
        "print(frase_reconstruida)\n",
        "print(\"--------------------------------------\")\n",
        "\n",
        "frase = \"pery disse que amava cecilia\"\n",
        "\n",
        "frase_encodada = encode_sentence(frase, vocab)\n",
        "frase_reconstruida = decode_sentence(frase_encodada, most_frequent_words)\n",
        "\n",
        "print('Original:')\n",
        "print(frase)\n",
        "print('Encodada:')\n",
        "print(frase_encodada)\n",
        "print('Reconstruída:')\n",
        "print(frase_reconstruida)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJhyepuyMGEX",
        "outputId": "28cfed84-41df-4634-e59e-d634e85ecb46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:\n",
            "Esta nova edição devia dar satisfação do empenho, que a extrema benevolencia do publico ledor, tão minguado ainda, mudou em bem para divida de reconhecimento.\n",
            "Encodada:\n",
            "[120, 1157, 2435, 172, 229, 916, 12, 0, 1, 5, 4, 725, 0, 12, 0, 0, 1, 107, 0, 69, 1, 0, 22, 136, 19, 2439, 7, 917, 3]\n",
            "Reconstruída:\n",
            "esta nova edição devia dar satisfação do <unk> , que a extrema <unk> do <unk> <unk> , tão <unk> ainda , <unk> em bem para divida de reconhecimento .\n",
            "--------------------------------------\n",
            "Original:\n",
            "pery disse que amava cecilia\n",
            "Encodada:\n",
            "[23, 50, 5, 546, 35]\n",
            "Reconstruída:\n",
            "pery disse que amava cecilia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para cada parágrafo, é necessário gerar os dados de treinamento. Supondo que a frase é \"eu gosto de pizza.\" e vamos usar uma janela de contexto igual a 2, a ideia é que essa frase gere o seguinte conjunto de treinamento:\n",
        "\n",
        "input -> target\n",
        "\n",
        "[UNK, \"eu\"] -> \"gosto\" (ESSE CASO NÃO SERÁ CONSIDERADO POR ENQUANTO)\n",
        "\n",
        "[\"eu\", \"gosto\"] -> \"de\"\n",
        "\n",
        "[\"gosto\", \"de\"] -> \"pizza\"\n",
        "\n",
        "[\"de\", \"pizza\"] -> \".\""
      ],
      "metadata": {
        "id": "oMv7ACEeMs6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gera_inputs_e_targets_para_array(array, n):\n",
        "  # Faz uma janela deslizante de tamanho n no array\n",
        "  janelas = []\n",
        "  targets = []\n",
        "\n",
        "  for i in range(len(array) - n):\n",
        "    janela = array[i:i+n]\n",
        "    janelas.append(janela)\n",
        "    targets.append(array[i+n])\n",
        "\n",
        "  return janelas, targets"
      ],
      "metadata": {
        "id": "Iy-elI1magRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de uso da função gera_inputs_e_targets_para_array\n",
        "exemplo = \"eu gosto de pizza .\".split()\n",
        "\n",
        "for n in range(1, 4):\n",
        "  print(f'Testando para janela de tamanho {n}')\n",
        "  inputs, targets = gera_inputs_e_targets_para_array(exemplo, n)\n",
        "\n",
        "  # Testa\n",
        "  for input_target in zip(inputs, targets):\n",
        "    print(f'{input_target[0]} -> {input_target[1]}')\n",
        "  print('------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWXndUTHNALN",
        "outputId": "c8a6b380-0ff9-41eb-cec6-1557b6414b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testando para janela de tamanho 1\n",
            "['eu'] -> gosto\n",
            "['gosto'] -> de\n",
            "['de'] -> pizza\n",
            "['pizza'] -> .\n",
            "------------------------------\n",
            "Testando para janela de tamanho 2\n",
            "['eu', 'gosto'] -> de\n",
            "['gosto', 'de'] -> pizza\n",
            "['de', 'pizza'] -> .\n",
            "------------------------------\n",
            "Testando para janela de tamanho 3\n",
            "['eu', 'gosto', 'de'] -> pizza\n",
            "['gosto', 'de', 'pizza'] -> .\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testa com um parágrafo real e o tamanho do contexto configurado\n",
        "i = 60\n",
        "inputs, targets = gera_inputs_e_targets_para_array(tokenizar(paragrafos[i]), context_size)\n",
        "print(paragrafos[i])\n",
        "for input_target in zip(inputs, targets):\n",
        "  print(f'{input_target[0]} -> {input_target[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzxNrXXWNJj1",
        "outputId": "b66c1708-e478-4b07-93db-78fdad24885e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descobrindo-se, curvou o joelho em terra, e estendeo a mão direita sobre o abysmo, cujos échos adormecidos repetirão ao longe a ultima phrase do juramento prestado sobre o altar da natureza, em face do sol que transmontava.\n",
            "['descobrindo', '-', 'se', ',', 'curvou', 'o', 'joelho', 'em', 'terra'] -> ,\n",
            "['-', 'se', ',', 'curvou', 'o', 'joelho', 'em', 'terra', ','] -> e\n",
            "['se', ',', 'curvou', 'o', 'joelho', 'em', 'terra', ',', 'e'] -> estendeo\n",
            "[',', 'curvou', 'o', 'joelho', 'em', 'terra', ',', 'e', 'estendeo'] -> a\n",
            "['curvou', 'o', 'joelho', 'em', 'terra', ',', 'e', 'estendeo', 'a'] -> mão\n",
            "['o', 'joelho', 'em', 'terra', ',', 'e', 'estendeo', 'a', 'mão'] -> direita\n",
            "['joelho', 'em', 'terra', ',', 'e', 'estendeo', 'a', 'mão', 'direita'] -> sobre\n",
            "['em', 'terra', ',', 'e', 'estendeo', 'a', 'mão', 'direita', 'sobre'] -> o\n",
            "['terra', ',', 'e', 'estendeo', 'a', 'mão', 'direita', 'sobre', 'o'] -> abysmo\n",
            "[',', 'e', 'estendeo', 'a', 'mão', 'direita', 'sobre', 'o', 'abysmo'] -> ,\n",
            "['e', 'estendeo', 'a', 'mão', 'direita', 'sobre', 'o', 'abysmo', ','] -> cujos\n",
            "['estendeo', 'a', 'mão', 'direita', 'sobre', 'o', 'abysmo', ',', 'cujos'] -> échos\n",
            "['a', 'mão', 'direita', 'sobre', 'o', 'abysmo', ',', 'cujos', 'échos'] -> adormecidos\n",
            "['mão', 'direita', 'sobre', 'o', 'abysmo', ',', 'cujos', 'échos', 'adormecidos'] -> repetirão\n",
            "['direita', 'sobre', 'o', 'abysmo', ',', 'cujos', 'échos', 'adormecidos', 'repetirão'] -> ao\n",
            "['sobre', 'o', 'abysmo', ',', 'cujos', 'échos', 'adormecidos', 'repetirão', 'ao'] -> longe\n",
            "['o', 'abysmo', ',', 'cujos', 'échos', 'adormecidos', 'repetirão', 'ao', 'longe'] -> a\n",
            "['abysmo', ',', 'cujos', 'échos', 'adormecidos', 'repetirão', 'ao', 'longe', 'a'] -> ultima\n",
            "[',', 'cujos', 'échos', 'adormecidos', 'repetirão', 'ao', 'longe', 'a', 'ultima'] -> phrase\n",
            "['cujos', 'échos', 'adormecidos', 'repetirão', 'ao', 'longe', 'a', 'ultima', 'phrase'] -> do\n",
            "['échos', 'adormecidos', 'repetirão', 'ao', 'longe', 'a', 'ultima', 'phrase', 'do'] -> juramento\n",
            "['adormecidos', 'repetirão', 'ao', 'longe', 'a', 'ultima', 'phrase', 'do', 'juramento'] -> prestado\n",
            "['repetirão', 'ao', 'longe', 'a', 'ultima', 'phrase', 'do', 'juramento', 'prestado'] -> sobre\n",
            "['ao', 'longe', 'a', 'ultima', 'phrase', 'do', 'juramento', 'prestado', 'sobre'] -> o\n",
            "['longe', 'a', 'ultima', 'phrase', 'do', 'juramento', 'prestado', 'sobre', 'o'] -> altar\n",
            "['a', 'ultima', 'phrase', 'do', 'juramento', 'prestado', 'sobre', 'o', 'altar'] -> da\n",
            "['ultima', 'phrase', 'do', 'juramento', 'prestado', 'sobre', 'o', 'altar', 'da'] -> natureza\n",
            "['phrase', 'do', 'juramento', 'prestado', 'sobre', 'o', 'altar', 'da', 'natureza'] -> ,\n",
            "['do', 'juramento', 'prestado', 'sobre', 'o', 'altar', 'da', 'natureza', ','] -> em\n",
            "['juramento', 'prestado', 'sobre', 'o', 'altar', 'da', 'natureza', ',', 'em'] -> face\n",
            "['prestado', 'sobre', 'o', 'altar', 'da', 'natureza', ',', 'em', 'face'] -> do\n",
            "['sobre', 'o', 'altar', 'da', 'natureza', ',', 'em', 'face', 'do'] -> sol\n",
            "['o', 'altar', 'da', 'natureza', ',', 'em', 'face', 'do', 'sol'] -> que\n",
            "['altar', 'da', 'natureza', ',', 'em', 'face', 'do', 'sol', 'que'] -> transmontava\n",
            "['da', 'natureza', ',', 'em', 'face', 'do', 'sol', 'que', 'transmontava'] -> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testa com um parágrafo real, mas agora usa ele encodado em vez de usar as palavras do texto\n",
        "inputs, targets = gera_inputs_e_targets_para_array(encode_sentence(paragrafos[i], vocab), context_size)\n",
        "print(paragrafos[i])\n",
        "for input_target in zip(inputs, targets):\n",
        "  print(f'{input_target[0]} -> {input_target[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y4DfOYKNPXp",
        "outputId": "88545a7c-b25c-41c0-ac46-ce4fcc19cdde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descobrindo-se, curvou o joelho em terra, e estendeo a mão direita sobre o abysmo, cujos échos adormecidos repetirão ao longe a ultima phrase do juramento prestado sobre o altar da natureza, em face do sol que transmontava.\n",
            "[0, 2, 9, 1, 1413, 6, 1414, 22, 131] -> 1\n",
            "[2, 9, 1, 1413, 6, 1414, 22, 131, 1] -> 8\n",
            "[9, 1, 1413, 6, 1414, 22, 131, 1, 8] -> 0\n",
            "[1, 1413, 6, 1414, 22, 131, 1, 8, 0] -> 4\n",
            "[1413, 6, 1414, 22, 131, 1, 8, 0, 4] -> 94\n",
            "[6, 1414, 22, 131, 1, 8, 0, 4, 94] -> 820\n",
            "[1414, 22, 131, 1, 8, 0, 4, 94, 820] -> 39\n",
            "[22, 131, 1, 8, 0, 4, 94, 820, 39] -> 6\n",
            "[131, 1, 8, 0, 4, 94, 820, 39, 6] -> 733\n",
            "[1, 8, 0, 4, 94, 820, 39, 6, 733] -> 1\n",
            "[8, 0, 4, 94, 820, 39, 6, 733, 1] -> 1587\n",
            "[0, 4, 94, 820, 39, 6, 733, 1, 1587] -> 2072\n",
            "[4, 94, 820, 39, 6, 733, 1, 1587, 2072] -> 2488\n",
            "[94, 820, 39, 6, 733, 1, 1587, 2072, 2488] -> 0\n",
            "[820, 39, 6, 733, 1, 1587, 2072, 2488, 0] -> 28\n",
            "[39, 6, 733, 1, 1587, 2072, 2488, 0, 28] -> 205\n",
            "[6, 733, 1, 1587, 2072, 2488, 0, 28, 205] -> 4\n",
            "[733, 1, 1587, 2072, 2488, 0, 28, 205, 4] -> 364\n",
            "[1, 1587, 2072, 2488, 0, 28, 205, 4, 364] -> 1171\n",
            "[1587, 2072, 2488, 0, 28, 205, 4, 364, 1171] -> 12\n",
            "[2072, 2488, 0, 28, 205, 4, 364, 1171, 12] -> 732\n",
            "[2488, 0, 28, 205, 4, 364, 1171, 12, 732] -> 0\n",
            "[0, 28, 205, 4, 364, 1171, 12, 732, 0] -> 39\n",
            "[28, 205, 4, 364, 1171, 12, 732, 0, 39] -> 6\n",
            "[205, 4, 364, 1171, 12, 732, 0, 39, 6] -> 0\n",
            "[4, 364, 1171, 12, 732, 0, 39, 6, 0] -> 15\n",
            "[364, 1171, 12, 732, 0, 39, 6, 0, 15] -> 288\n",
            "[1171, 12, 732, 0, 39, 6, 0, 15, 288] -> 1\n",
            "[12, 732, 0, 39, 6, 0, 15, 288, 1] -> 22\n",
            "[732, 0, 39, 6, 0, 15, 288, 1, 22] -> 255\n",
            "[0, 39, 6, 0, 15, 288, 1, 22, 255] -> 12\n",
            "[39, 6, 0, 15, 288, 1, 22, 255, 12] -> 175\n",
            "[6, 0, 15, 288, 1, 22, 255, 12, 175] -> 5\n",
            "[0, 15, 288, 1, 22, 255, 12, 175, 5] -> 0\n",
            "[15, 288, 1, 22, 255, 12, 175, 5, 0] -> 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ParagrafosDataset(Dataset):\n",
        "  def __init__(self, paragrafos, vocab, context_size):\n",
        "    # Salva o vocabulário\n",
        "    self.vocab = vocab\n",
        "    # Cria os inputs e target\n",
        "    inputs = []\n",
        "    targets = []\n",
        "\n",
        "    for p in paragrafos:\n",
        "      # O primeiro passo é pegar cada frase do parágrafo e encodar\n",
        "      p_tokenizado = encode_sentence(p, self.vocab)\n",
        "      # Só faz sentido considerar frases que tem no mínimo (context_size + 1) tokens\n",
        "      if (len(p_tokenizado) <= context_size):\n",
        "        continue\n",
        "\n",
        "      # Agora vamos gerar os dados de treinamento para esse parágrafo\n",
        "      p_inputs, p_targets = gera_inputs_e_targets_para_array(p_tokenizado, context_size)\n",
        "\n",
        "      # Adiciona independentemente se tiver UKN ou não no input ou target\n",
        "      inputs.extend(p_inputs)\n",
        "      targets.extend(p_targets)\n",
        "\n",
        "      # Apenas adiciona se o input ou o target não tiver nenhum UNK (código 0)\n",
        "      #for p_um_input, p_um_target in zip(p_inputs, p_targets):\n",
        "      #  if (0 not in p_um_input and p_um_target != 0):\n",
        "      #    inputs.append(p_um_input)\n",
        "      #    targets.append(p_um_target)\n",
        "\n",
        "    # Mantém em cache\n",
        "    self.inputs = torch.tensor(inputs)\n",
        "    self.targets = torch.tensor(targets)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.targets)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.inputs[idx], self.targets[idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaDheTeWNaMp",
        "outputId": "ff1f5517-4b5a-41c3-d11c-8fecf6297cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.89 s, sys: 376 ms, total: 2.27 s\n",
            "Wall time: 6.86 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testa a implementação do Dataset\n",
        "\n",
        "teste_paragrafos = [\"Depois, vendo que esta expedição não se realisava, e que seu braço e sua coragem de nada valião ao rei de Portugal\", \"Colocando uma segunda frase qualquer apenas para ver se está tudo certo.\"]\n",
        "teste_dataset = ParagrafosDataset(teste_paragrafos, vocab, context_size)\n",
        "\n",
        "print('Imprimindo o dataset')\n",
        "for dados in teste_dataset:\n",
        "  print(dados)\n",
        "\n",
        "print('-------------------------')\n",
        "print('Como deveria estar (testando se o dataset está considerando corretamente os parágrafos:')\n",
        "for p in teste_paragrafos:\n",
        "  # Faz o encode do parágrafo\n",
        "  p_encodado = encode_sentence(p, vocab)\n",
        "  inputs, targets = gera_inputs_e_targets_para_array(p_encodado, context_size)\n",
        "  for inputs_targets in zip(inputs, targets):\n",
        "    print(torch.tensor(inputs_targets[0]), torch.tensor(inputs_targets[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfKob8xqNpiR",
        "outputId": "5669b31b-76d2-4016-f18e-291511df93c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imprimindo o dataset\n",
            "(tensor([ 63,   1, 275,   5, 120, 994,  13,   9,   0]), tensor(1))\n",
            "(tensor([  1, 275,   5, 120, 994,  13,   9,   0,   1]), tensor(8))\n",
            "(tensor([275,   5, 120, 994,  13,   9,   0,   1,   8]), tensor(5))\n",
            "(tensor([  5, 120, 994,  13,   9,   0,   1,   8,   5]), tensor(20))\n",
            "(tensor([120, 994,  13,   9,   0,   1,   8,   5,  20]), tensor(204))\n",
            "(tensor([994,  13,   9,   0,   1,   8,   5,  20, 204]), tensor(8))\n",
            "(tensor([ 13,   9,   0,   1,   8,   5,  20, 204,   8]), tensor(18))\n",
            "(tensor([  9,   0,   1,   8,   5,  20, 204,   8,  18]), tensor(363))\n",
            "(tensor([  0,   1,   8,   5,  20, 204,   8,  18, 363]), tensor(7))\n",
            "(tensor([  1,   8,   5,  20, 204,   8,  18, 363,   7]), tensor(252))\n",
            "(tensor([  8,   5,  20, 204,   8,  18, 363,   7, 252]), tensor(0))\n",
            "(tensor([  5,  20, 204,   8,  18, 363,   7, 252,   0]), tensor(28))\n",
            "(tensor([ 20, 204,   8,  18, 363,   7, 252,   0,  28]), tensor(550))\n",
            "(tensor([204,   8,  18, 363,   7, 252,   0,  28, 550]), tensor(7))\n",
            "(tensor([  8,  18, 363,   7, 252,   0,  28, 550,   7]), tensor(1075))\n",
            "(tensor([  0,  14, 674,   0, 494, 118,  19, 145,   9]), tensor(352))\n",
            "(tensor([ 14, 674,   0, 494, 118,  19, 145,   9, 352]), tensor(90))\n",
            "(tensor([674,   0, 494, 118,  19, 145,   9, 352,  90]), tensor(329))\n",
            "(tensor([  0, 494, 118,  19, 145,   9, 352,  90, 329]), tensor(3))\n",
            "-------------------------\n",
            "Como deveria estar (testando se o dataset está considerando corretamente os parágrafos:\n",
            "tensor([ 63,   1, 275,   5, 120, 994,  13,   9,   0]) tensor(1)\n",
            "tensor([  1, 275,   5, 120, 994,  13,   9,   0,   1]) tensor(8)\n",
            "tensor([275,   5, 120, 994,  13,   9,   0,   1,   8]) tensor(5)\n",
            "tensor([  5, 120, 994,  13,   9,   0,   1,   8,   5]) tensor(20)\n",
            "tensor([120, 994,  13,   9,   0,   1,   8,   5,  20]) tensor(204)\n",
            "tensor([994,  13,   9,   0,   1,   8,   5,  20, 204]) tensor(8)\n",
            "tensor([ 13,   9,   0,   1,   8,   5,  20, 204,   8]) tensor(18)\n",
            "tensor([  9,   0,   1,   8,   5,  20, 204,   8,  18]) tensor(363)\n",
            "tensor([  0,   1,   8,   5,  20, 204,   8,  18, 363]) tensor(7)\n",
            "tensor([  1,   8,   5,  20, 204,   8,  18, 363,   7]) tensor(252)\n",
            "tensor([  8,   5,  20, 204,   8,  18, 363,   7, 252]) tensor(0)\n",
            "tensor([  5,  20, 204,   8,  18, 363,   7, 252,   0]) tensor(28)\n",
            "tensor([ 20, 204,   8,  18, 363,   7, 252,   0,  28]) tensor(550)\n",
            "tensor([204,   8,  18, 363,   7, 252,   0,  28, 550]) tensor(7)\n",
            "tensor([  8,  18, 363,   7, 252,   0,  28, 550,   7]) tensor(1075)\n",
            "tensor([  0,  14, 674,   0, 494, 118,  19, 145,   9]) tensor(352)\n",
            "tensor([ 14, 674,   0, 494, 118,  19, 145,   9, 352]) tensor(90)\n",
            "tensor([674,   0, 494, 118,  19, 145,   9, 352,  90]) tensor(329)\n",
            "tensor([  0, 494, 118,  19, 145,   9, 352,  90, 329]) tensor(3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instâncias vocabulário de treino, dataset e dataloader"
      ],
      "metadata": {
        "id": "BN-dbjNTc-YH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gera datasets de treinamento e de teste:\n",
        "\n",
        "- Vou fazer a consideração de que a proporção é no total de parágrafos, e não no total do conjunto de dados. Como cada parágrafo tem um total de frases/palavras diferentes, o conjunto final não ficará com a proporção exatamente conforme esperado inicialmente. Entretanto, pensando que em um texto as coisas são mais ou menos distribuídas, espera-se que, no final, a proporção seja mais ou menos conforme a desejada.\n",
        "\n",
        "- Depois de fazer isso, é necessário gerar novamente o vocabulário, mas considerando apenas o conjunto de treinamento."
      ],
      "metadata": {
        "id": "-yXb_qF6OKnE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD1CVci2zJ_J"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_paragrafos, val_paragrafos = train_test_split(paragrafos, test_size=test_size, random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gera novamente o vocabulário, mas agora usando apenas os parágrafos de treinamento.\n",
        "# Isso é necessário pois o treino deveria levar em consideração apenas o vocabulário de treinamento\n",
        "# Num exemplo real, com bases de dados grandes, é improvável que a base de validação tenha\n",
        "# palavras que não estão na base de treinamento.\n",
        "# Entretanto, no exercício, para bases pequenas, é comum de ocorrer.\n",
        "vocab_size, vocab, most_frequent_words = gerar_vocabulario(train_paragrafos, vocab_size_desejado_sem_UNK)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfXcDe1IOdDE",
        "outputId": "55061db0-f7ab-4404-efa3-58c8ccdc44c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gera os dataset de treino e validação\n",
        "train_data = ParagrafosDataset(train_paragrafos, vocab, context_size)\n",
        "val_data = ParagrafosDataset(val_paragrafos, vocab, context_size)"
      ],
      "metadata": {
        "id": "u0YlbywTO304"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'len(val_data): {len(val_data)}')\n",
        "print(f'len(train_data): {len(train_data)}')\n",
        "print(f'Proporção de teste (deve ser próximo de {test_size}): {len(val_data)/(len(train_data)+len(val_data))}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf4lmVGjO6XU",
        "outputId": "b809e7cc-d354-436d-bf07-d0296d6b24a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(val_data): 18881\n",
            "len(train_data): 76116\n",
            "Proporção de teste (deve ser próximo de 0.2): 0.1987536448519427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "XDoMwRnaPFmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testes de dimensões do que se espera do modelo"
      ],
      "metadata": {
        "id": "FKJVFBfdZRcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Cria um dataset com poucos dados e pega um batch\n",
        "dataset_pequeno = ParagrafosDataset(paragrafos[0:10], vocab, context_size)\n",
        "loader_pequeno = DataLoader(dataset_pequeno, batch_size=2, shuffle=False)\n",
        "inputs, targets = next(iter(loader_pequeno))\n",
        "\n",
        "print('O batch tem tamanho: ', inputs.shape)\n",
        "print(f'São {targets.shape} amostras e {inputs.shape[1]} entradas (tamando do contexto)')\n",
        "\n",
        "# Gera os embeddings\n",
        "print(f'Cria um nn.Embedding de tamanho ({vocab_size}, {m})')\n",
        "C = nn.Embedding(vocab_size, m)\n",
        "print(C, ' => (tamanho do vocabulário, dimensão dos embeddings) \\n')\n",
        "\n",
        "# Passa o batch pela matriz C\n",
        "print('x = C(inputs)')\n",
        "x = C(inputs)\n",
        "print(f'Dimensões de x: {x.shape}')\n",
        "print(f'Tamanho do batch:\\t\\tB = {x.shape[0]}')\n",
        "print(f'Tamanho da sequência:\\t\\tL = {x.shape[1]}')\n",
        "print(f'Dimensão dos embeddings:\\tD = {x.shape[2]}\\n')\n",
        "\n",
        "# Cria três matrizes WQ, WK e WV de dimensão\n",
        "print(f'Cria matrizes WQ, WK, WV de tamanhos ({m}, {m}) -> dimensões dos embeddings')\n",
        "WQ = nn.Parameter(torch.randn(m, m))\n",
        "WK = nn.Parameter(torch.randn(m, m))\n",
        "WV = nn.Parameter(torch.randn(m, m))\n",
        "print(f'WQ = {WQ.shape}')\n",
        "print(f'WK = {WK.shape}')\n",
        "print(f'WV = {WV.shape}\\n')\n",
        "\n",
        "# Aplica as matrizes para criar Q, K, V\n",
        "print(f'Aplica as matrizes WQ, WK e WV em x para criar Q, K e V')\n",
        "Q = torch.matmul(x, WQ)\n",
        "K = torch.matmul(x, WK)\n",
        "V = torch.matmul(x, WV)\n",
        "print(f'Q = {Q.shape}')\n",
        "print(f'K = {K.shape}')\n",
        "print(f'V = {V.shape}\\n')\n",
        "\n",
        "# MATMUL (Q, K) E SCALE\n",
        "print('Calcula matmul(Q, K.t())')\n",
        "print('As dimensões de Q e K são (batch, context_size, embeddings)')\n",
        "print('As dimensões de Q e K.t() são (batch, context_size, embeddings) x (batch, embeddings, context_size)')\n",
        "print('As dimensões finais devem ser (batch, context_size, context_size)')\n",
        "qkt = torch.bmm(Q, K.transpose(1, 2)) / math.sqrt(m)\n",
        "print(f'qkt = {qkt.shape}\\n')\n",
        "\n",
        "# SOFTMAX\n",
        "print('Aplica o softmax em qkt. Dimensões permanecem')\n",
        "qkt_softmax = torch.softmax(qkt, dim=2)\n",
        "print(f'qkt_softmax = {qkt_softmax.shape}\\n')\n",
        "\n",
        "# MATMUL (_, V)\n",
        "print('Calcula o QKV')\n",
        "print(f'As dimensões de qkt_softmax são (batch, context_size, context_size)')\n",
        "print(f'As dimensões de v são (batch, context_size, embeddings)')\n",
        "print(f'As dimensões resultantes devem ser (batch, context_size, embeddings)')\n",
        "qkv = torch.bmm(qkt_softmax, V)\n",
        "print(f'qkv = {qkv.shape}\\n')\n",
        "\n",
        "# Daqui pra frente, segue a lógica normal.\n",
        "# Esse qkv passa a ser a nova entrada\n",
        "# Diferentemente do exercício anterior, vou considerar, para facilitar, que a entrada será sempre em batch\n",
        "print('Achata a entrada:')\n",
        "x = qkv\n",
        "batch_size, _, _ = x.shape\n",
        "x = x.view(batch_size, -1)\n",
        "print(f'x = {x.shape}')\n",
        "\n",
        "# Cria a primeira camada\n",
        "print('Cria a primeira camada: d_plus_H')\n",
        "d_plus_H = nn.Linear(in_features=context_size*m, out_features=h, bias=True)\n",
        "o = d_plus_H(x)\n",
        "print(f'Primeira camada: {o.shape}\\n')\n",
        "\n",
        "# Passa pela ativação\n",
        "print('Passa pela ativação')\n",
        "o = nn.ReLU()(o)\n",
        "print(f'Ativação: {o.shape}\\n')\n",
        "\n",
        "# Última camada\n",
        "print('Última camada')\n",
        "b_plus_U = nn.Linear(in_features=h, out_features=vocab_size, bias=True)\n",
        "o = b_plus_U(o)\n",
        "print(f'Última camada: {o.shape}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQJo6AivZSzl",
        "outputId": "aac75eb8-5347-4a7f-af43-db5a091b21d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O batch tem tamanho:  torch.Size([2, 9])\n",
            "São torch.Size([2]) amostras e 9 entradas (tamando do contexto)\n",
            "Cria um nn.Embedding de tamanho (3001, 64)\n",
            "Embedding(3001, 64)  => (tamanho do vocabulário, dimensão dos embeddings) \n",
            "\n",
            "x = C(inputs)\n",
            "Dimensões de x: torch.Size([2, 9, 64])\n",
            "Tamanho do batch:\t\tB = 2\n",
            "Tamanho da sequência:\t\tL = 9\n",
            "Dimensão dos embeddings:\tD = 64\n",
            "\n",
            "Cria matrizes WQ, WK, WV de tamanhos (64, 64) -> dimensões dos embeddings\n",
            "WQ = torch.Size([64, 64])\n",
            "WK = torch.Size([64, 64])\n",
            "WV = torch.Size([64, 64])\n",
            "\n",
            "Aplica as matrizes WQ, WK e WV em x para criar Q, K e V\n",
            "Q = torch.Size([2, 9, 64])\n",
            "K = torch.Size([2, 9, 64])\n",
            "V = torch.Size([2, 9, 64])\n",
            "\n",
            "Calcula matmul(Q, K.t())\n",
            "As dimensões de Q e K são (batch, context_size, embeddings)\n",
            "As dimensões de Q e K.t() são (batch, context_size, embeddings) x (batch, embeddings, context_size)\n",
            "As dimensões finais devem ser (batch, context_size, context_size)\n",
            "qkt = torch.Size([2, 9, 9])\n",
            "\n",
            "Aplica o softmax em qkt. Dimensões permanecem\n",
            "qkt_softmax = torch.Size([2, 9, 9])\n",
            "\n",
            "Calcula o QKV\n",
            "As dimensões de qkt_softmax são (batch, context_size, context_size)\n",
            "As dimensões de v são (batch, context_size, embeddings)\n",
            "As dimensões resultantes devem ser (batch, context_size, embeddings)\n",
            "qkv = torch.Size([2, 9, 64])\n",
            "\n",
            "Achata a entrada:\n",
            "x = torch.Size([2, 576])\n",
            "Cria a primeira camada: d_plus_H\n",
            "Primeira camada: torch.Size([2, 50])\n",
            "\n",
            "Passa pela ativação\n",
            "Ativação: torch.Size([2, 50])\n",
            "\n",
            "Última camada\n",
            "Última camada: torch.Size([2, 3001])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testes de implementação em laço vs matricial"
      ],
      "metadata": {
        "id": "HrAHo_j39J9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Já temos disponível:\n",
        "#   - Matrizes C, WQ, WK e WV\n",
        "#   - Batch\n",
        "#   - Resultados intermediários:\n",
        "#       - Q = x * WQ\n",
        "#       - K = x * WK\n",
        "#       - V = x * WV\n",
        "#       - Q x K_T / sqrt(m)\n",
        "#       - [Q x K_T / sqrt(m)] x V <- mecanismo de atenção\n",
        "\n",
        "\n",
        "############################################################\n",
        "# A ideia desse teste é validar a implementação matricial,\n",
        "# ver se ela está batendo com uma implementação em laço\n",
        "# (mais ineficiente, porém mais simples de entender).\n",
        "# Por isso, faremos passo a passo aqui, sem preocupar em\n",
        "# otimizar os laços. A ideia é comparar os passos intermediários\n",
        "############################################################\n",
        "\n",
        "# Passa o batch pela matriz C\n",
        "# x tem tamanho (B, context_size, m)\n",
        "x = C(inputs)\n",
        "\n",
        "# O tamanho do batch (B) não é recebido como parâmetro, então vamos pegar ele aqui:\n",
        "B, _, _ = x.shape\n",
        "\n",
        "# Implementa a operação matricial x * WQ, x * WK, x * WV\n",
        "# WQ, WK e WX tem tamanho (m, m)\n",
        "# A saída dessa operação tem que ter tamanho (B, context_size, m) * (m, m) = (B, context_size, m)\n",
        "# A saída dessa operação tem que ter o tamanho\n",
        "if comparacoes_loop['projecoes']:\n",
        "  Q_laco = torch.zeros(B, context_size, m)\n",
        "  K_laco = torch.zeros(B, context_size, m)\n",
        "  V_laco = torch.zeros(B, context_size, m)\n",
        "\n",
        "  for b in range(B):\n",
        "    for i in range(context_size):\n",
        "      for j in range(m):\n",
        "        for aux in range(m):\n",
        "          Q_laco[b, i, j] += x[b, i, aux] * WQ[aux, j]\n",
        "          K_laco[b, i, j] += x[b, i, aux] * WK[aux, j]\n",
        "          V_laco[b, i, j] += x[b, i, aux] * WV[aux, j]\n",
        "\n",
        "  # Considerando precisão de 1e-6. Não dá precisão maior do que isso...\n",
        "  print('Testando cálculo das projeções usando for loops:')\n",
        "  print('Q == Q_laco:', torch.allclose(Q, Q_laco, atol=1e-6))\n",
        "  print('K == K_laco:', torch.allclose(K, K_laco, atol=1e-6))\n",
        "  print('V == V_laco:', torch.allclose(V, V_laco, atol=1e-6), '\\n')\n",
        "\n",
        "if comparacoes_loop['qkt']:\n",
        "  qkt_laco = torch.zeros(B, context_size, context_size)\n",
        "\n",
        "  for b in range(B):\n",
        "    for i in range(context_size):\n",
        "      for j in range(context_size):\n",
        "        for aux in range(m):\n",
        "          qkt_laco[b, i, j] += Q[b, i, aux] * K[b, j, aux]\n",
        "  qkt_laco = qkt_laco / math.sqrt(m)\n",
        "  print('Testando cálculo de Q * K.t() usando for loops:')\n",
        "  print('qkt == qkt_laco:', torch.allclose(qkt, qkt_laco, atol=1e-6), '\\n')\n",
        "\n",
        "if comparacoes_loop['softmax']:\n",
        "  qkt_softmax_laco = torch.zeros(B, context_size, context_size)\n",
        "\n",
        "  for b in range(B):\n",
        "    for i in range(context_size):\n",
        "      qkt_softmax_laco[b, i, :] = torch.softmax(qkt[b, i, :], dim=0)\n",
        "  print('Testando cálculo do softmax usando for loops:')\n",
        "  print('qkt_softmax == qkt_softmax_laco:', torch.allclose(qkt_softmax, qkt_softmax_laco, atol=1e-6), '\\n')\n",
        "\n",
        "if comparacoes_loop['qkv']:\n",
        "  qkv_laco = torch.zeros(B, context_size, m)\n",
        "\n",
        "  for b in range(B):\n",
        "    for i in range(context_size):\n",
        "      for j in range(m):\n",
        "        for aux in range(context_size):\n",
        "          qkv_laco[b, i, j] += qkt_softmax[b, i, aux] * V[b, aux, j]\n",
        "\n",
        "  print('Testando cálculo de QKV usando for loops:')\n",
        "  print('qkv == qkv_laco:', torch.allclose(qkv, qkv_laco, atol=1e-6), '\\n')"
      ],
      "metadata": {
        "id": "ONbFa1_ZpR3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d58ca7aa-96d3-4f8f-fda9-ddf57af2e18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testando cálculo de Q * K.t() usando for loops:\n",
            "qkt == qkt_laco: True \n",
            "\n",
            "Testando cálculo do softmax usando for loops:\n",
            "qkt_softmax == qkt_softmax_laco: True \n",
            "\n",
            "Testando cálculo de QKV usando for loops:\n",
            "qkv == qkv_laco: True \n",
            "\n",
            "CPU times: user 1.82 s, sys: 231 ms, total: 2.05 s\n",
            "Wall time: 3.38 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criação de um layer para implementar o mecanismo de atenção"
      ],
      "metadata": {
        "id": "GGLKGMeh71im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoAtencao(torch.nn.Module):\n",
        "  def __init__(self, m):\n",
        "    super(AutoAtencao, self).__init__()\n",
        "    self.WQ = nn.Parameter(torch.randn(m, m))\n",
        "    self.WK = nn.Parameter(torch.randn(m, m))\n",
        "    self.WV = nn.Parameter(torch.randn(m, m))\n",
        "    torch.nn.init.xavier_uniform_(self.WQ)\n",
        "    torch.nn.init.xavier_uniform_(self.WK)\n",
        "    torch.nn.init.xavier_uniform_(self.WV)\n",
        "\n",
        "  def forward(self, x):\n",
        "    Q = torch.matmul(x, self.WQ)\n",
        "    K = torch.matmul(x, self.WK)\n",
        "    V = torch.matmul(x, self.WV)\n",
        "\n",
        "    qkt = torch.bmm(Q, K.transpose(1, 2)) / math.sqrt(m)\n",
        "    qkt_softmax = torch.softmax(qkt, dim=2)\n",
        "    qkv = torch.bmm(qkt_softmax, V)\n",
        "\n",
        "    return qkv"
      ],
      "metadata": {
        "id": "tFgUsWmMcgYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding de posição"
      ],
      "metadata": {
        "id": "FlZP94t9PPaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseado no código disponível no [tutorial de Transformers do PyTorch](https://pytorch.org/tutorials/beginner/transformer_tutorial.html), mas adaptado para usar a equação do artigo [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)."
      ],
      "metadata": {
        "id": "edzq60hHP45B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncodingPosicao(torch.nn.Module):\n",
        "  def __init__(self, context_size, m, device):\n",
        "    super(EncodingPosicao, self).__init__()\n",
        "\n",
        "    # Os encodings posicionais serão somados à entrada. Devem ter, portanto,\n",
        "    # o mesmo tamanho da entrada. A entrada desse layer é a saída de um\n",
        "    # self.C (os embeddings). Logo, tem tamanho (B, context_size, m).\n",
        "    # Mas como a matriz é única por batch, podemos criar apenas como\n",
        "    # (context_size, m) e, depois, fazer o broadcasting na primeira dimensão\n",
        "    self.pe = torch.zeros(context_size, m)\n",
        "\n",
        "    position = torch.arange(context_size).unsqueeze(1)\n",
        "    dois_i = torch.arange(0, m, 2) # = [0, 2, 4, 6, ...., 62]\n",
        "    div_term = torch.pow(10000, dois_i / m)\n",
        "\n",
        "    # pe(pos, 2i) -> índices pares\n",
        "    self.pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "    # pe(pos, 2i+1) -> índices ímpares\n",
        "    # ESSE CÓDIGO SÓ VAI FUNCIONAR SE m FOR PAR\n",
        "    self.pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    self.pe = self.pe.to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Faz o broadcasting de self.pe (context_size, m) para (B, context_size, m)\n",
        "    return x + self.pe"
      ],
      "metadata": {
        "id": "wHFtqixYPRlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LanguageModel"
      ],
      "metadata": {
        "id": "xPPTs1rC786O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "  def __init__(self, vocab_size, context_size, m, h, device, ativacao='relu', zerar_w=True):\n",
        "    super(LanguageModel, self).__init__()\n",
        "\n",
        "    self.C = nn.Embedding(vocab_size, m)\n",
        "    self.encoding_posicao = EncodingPosicao(context_size, m, device)\n",
        "    self.auto_atencao = AutoAtencao(m)\n",
        "    self.d_plus_H = nn.Linear(in_features=context_size*m, out_features=h, bias=True)\n",
        "    self.ativacao = nn.ReLU() if ativacao == 'relu' else nn.Tanh()\n",
        "    self.b_plus_U = nn.Linear(in_features=h, out_features=vocab_size, bias=True)\n",
        "\n",
        "    self.zerar_w = zerar_w\n",
        "    # Modelo do artigo:\n",
        "    if not self.zerar_w:\n",
        "      self.W = nn.Linear(in_features=context_size*m, out_features=vocab_size, bias=False)\n",
        "\n",
        "  def forward(self, w):\n",
        "    # w é uma entrada de tamanho context_size (no artigo é chamada de n)\n",
        "    # O primeiro passo é obter os embeddings de w\n",
        "    x = self.C(w)\n",
        "\n",
        "    # Insere encoding de posição\n",
        "    x = self.encoding_posicao(x)\n",
        "\n",
        "    # Agora passa pelo mecanismo de auto atenção\n",
        "    x = self.auto_atencao(x)\n",
        "\n",
        "    # E agora volta para o esquema normal do modelo do Bengio\n",
        "    # Planifica a entrada nas duas últimas dimensões\n",
        "    if x.dim() == 3: # Usando batchs\n",
        "      batch_size, _, _ = x.shape\n",
        "      x = x.view(batch_size, -1)\n",
        "    elif x.dim() == 2: # Calculando sem usar batch, usando um tensor direto\n",
        "      x = x.view(-1)\n",
        "\n",
        "    # Passa pela primeira camada\n",
        "    o = self.d_plus_H(x)\n",
        "\n",
        "    # Camada de ativação\n",
        "    o = self.ativacao(o)\n",
        "\n",
        "    # Segunda camada\n",
        "    o = self.b_plus_U(o)\n",
        "\n",
        "    if not self.zerar_w: # Modelo do artigo\n",
        "      o = o + self.W(x)\n",
        "\n",
        "    return o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wntaV50nzJ_L",
        "outputId": "f77daac8-a7b9-46da-a195-03c8d8f19bf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "  print('GPU:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "  print('using CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Função para calcular a loss e a perplexidade para um DataLoader de entrada"
      ],
      "metadata": {
        "id": "DV3o4YK4RiUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "def calcula_loss_e_perplexidade(model, loader):\n",
        "  criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "  with torch.no_grad(): # Garante que nenhum gradiente seja calculado\n",
        "    model.eval()  # Coloca o modelo no modo de avaliação (não treinamento)\n",
        "    loss = 0.0\n",
        "    acc = 0\n",
        "    for inputs, targets in tqdm(loader, desc='Calculando loss e perplexidade'):\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      # Acumula a perda\n",
        "      loss += criterion(outputs, targets)\n",
        "      acc += len(targets)\n",
        "\n",
        "    loss = loss/acc\n",
        "    ppl = math.exp(loss)\n",
        "\n",
        "    return loss, ppl"
      ],
      "metadata": {
        "id": "k9tjtNxPRVM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_loss_ppl(msg, loss, ppl):\n",
        "  print(f'{msg}. Loss: {loss:.2f}. Perplexidade: {ppl:.2f}\\n')"
      ],
      "metadata": {
        "id": "HUQoy1V6RoDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model instantiation\n",
        "model = LanguageModel(vocab_size, context_size, m, 10, device)\n",
        "model.to(device)\n",
        "\n",
        "dataset_pequeno = ParagrafosDataset(paragrafos[0:10], vocab, context_size)\n",
        "loader_pequeno = DataLoader(dataset_pequeno, batch_size=2, shuffle=False)\n",
        "\n",
        "loss, ppl = calcula_loss_e_perplexidade(model, loader_pequeno)\n",
        "print_loss_ppl('\\nAntes de iniciar o treinamento', loss, ppl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CAMt9dvRrNV",
        "outputId": "8fe635a7-56bb-44fb-8de5-894f9e66f781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 12/12 [00:02<00:00,  5.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Antes de iniciar o treinamento. Loss: 8.09. Perplexidade: 3249.09\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Função para treinar um modelo"
      ],
      "metadata": {
        "id": "CvzIXPc4Rvp7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRwSPiwizJ_L"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def treina_modelo(model, optimizer, train_loader, val_loader, num_epochs=num_epochs):\n",
        "  print(f'------------------ ANTES DE INICIAR O TREINAMENTO ------------------')\n",
        "  loss, ppl = calcula_loss_e_perplexidade(model, train_loader)\n",
        "  print_loss_ppl(f'[TRAIN]', loss, ppl)\n",
        "\n",
        "  loss, ppl = calcula_loss_e_perplexidade(model, val_loader)\n",
        "  print_loss_ppl(f'[EVAL]', loss, ppl)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "  model.to(device)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    print(f'------------------ [ÉPOCA {epoch+1}/{num_epochs}] ------------------')\n",
        "    estimativa_loss_epoca_i = 0\n",
        "    acc_dados = 0\n",
        "\n",
        "    for inputs, targets in tqdm(train_loader, desc='Treinando modelo'):\n",
        "      # Envia inputs e target para o mesmo dispositivo\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      # Calcula loss no batch\n",
        "      loss = criterion(outputs, targets)\n",
        "\n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Acumula a loss pra época atual\n",
        "      # Obs.: Isso é só uma estimativa para a loss de treino após a época i.\n",
        "      # Como os pesos são atualizados após rodar cada batch, ao final da época\n",
        "      # é esperado que a loss no conjunto de treinamento seja menor\n",
        "      # do que o calculado dessa forma (o ajuste em cada batch tende a ir\n",
        "      # convergindo e, consequentemente, diminuindo a loss)\n",
        "      estimativa_loss_epoca_i += loss.item() * len(train_loader)\n",
        "      acc_dados += len(train_loader)\n",
        "\n",
        "    estimativa_loss_epoca_i = estimativa_loss_epoca_i / acc_dados\n",
        "\n",
        "    # Imprime a estimativa da loss/ppl de treino e a loss no dataloader de avaliação:\n",
        "    print_loss_ppl(f'[TRAIN ESTIMATIVA]', estimativa_loss_epoca_i, math.exp(estimativa_loss_epoca_i))\n",
        "    loss, ppl = calcula_loss_e_perplexidade(model, val_loader)\n",
        "    print_loss_ppl(f'[EVAL]', loss, ppl)\n",
        "\n",
        "    # Salva os modelos intermediários\n",
        "    checkpoint_path = f\"modelo_epoca_{epoch+1}.pth\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Função para escrever uma frase (recebe um modelo como entrada)"
      ],
      "metadata": {
        "id": "-XI5lKKpUcDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Cria um dataset sem target para guardar apenas uma frase\n",
        "class UmaFraseDataset(Dataset):\n",
        "  def __init__(self, frase, vocab, context_size):\n",
        "    # Salva o vocabulário\n",
        "    self.vocab = vocab\n",
        "    # Cria os inputs\n",
        "    frase_encodada = encode_sentence(frase, self.vocab)\n",
        "    self.inputs = torch.tensor(frase_encodada[len(frase_encodada)-context_size:len(frase_encodada)])\n",
        "\n",
        "  def __len__(self):\n",
        "    return 1\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw-hXSJNugw8",
        "outputId": "e7f8a46f-f6dd-45ab-9e88-6c11a9e12d80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 38 µs, sys: 7 µs, total: 45 µs\n",
            "Wall time: 48.4 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def escrever_frase(modelo, vocab, most_frequent_words, entrada, context_size, n_proximas_palavras, descartar_ukn=True):\n",
        "  if (n_proximas_palavras == 0):\n",
        "    return entrada\n",
        "  else:\n",
        "    # Cria um dataloader com esse input\n",
        "    loader_uma_frase = DataLoader(UmaFraseDataset(entrada, vocab, context_size), batch_size=1, shuffle=False)\n",
        "    batch_uma_frase = next(iter(loader_uma_frase)).to(device)\n",
        "\n",
        "    # Pega só as context_size últimas\n",
        "    with torch.no_grad():\n",
        "      output = model(batch_uma_frase)\n",
        "      softmax = nn.functional.softmax(output, dim=1)\n",
        "\n",
        "      if descartar_ukn:\n",
        "        valores, indices = softmax.topk(2, dim=1)\n",
        "        melhor_not_ukn = indices[0][0].item() if indices[0][0].item() != 0 else indices[0][1].item()\n",
        "        predicao = most_frequent_words[melhor_not_ukn]\n",
        "      else:\n",
        "        argmax = softmax.argmax(dim=0)\n",
        "        predicao = most_frequent_words[argmax[0]]\n",
        "\n",
        "    # Substitui símbolos que foram trocados manualmente\n",
        "    predicao = predicao.replace('SUBSTITUIRPORTRESPONTOS', '...')\n",
        "\n",
        "  return escrever_frase(modelo, vocab, most_frequent_words, f'{entrada} {predicao}', context_size, n_proximas_palavras-1)"
      ],
      "metadata": {
        "id": "as20X1WBUiEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inicializa um modelo e vê que frase ele gera (sem treinar):"
      ],
      "metadata": {
        "id": "hok6vCbPUk2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Como esse é o modelo que será treinado, seta a seed aqui\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "model = LanguageModel(vocab_size, context_size, m, h, device, ativacao, zerar_w)\n",
        "model.to(device)\n",
        "\n",
        "# Escreve uma frase com o modelo sem estar treinado\n",
        "frase = \"O espectaculo que se ofereceu aos seus olhos causou\"\n",
        "print(escrever_frase(model, vocab, most_frequent_words, frase, context_size, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oae1eBjiTUtq",
        "outputId": "04acf7e5-9c2d-48bb-9698-45aef9a05763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O espectaculo que se ofereceu aos seus olhos causou áquelles cascata ironia pallidos áquelles dedicação pallidos graciosas guerra era\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treina o modelo"
      ],
      "metadata": {
        "id": "4IBewbV6Uwcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "treina_modelo(model, optimizer, train_loader, val_loader, num_epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeVpLk1lUzuM",
        "outputId": "409f35e0-d260-4dd0-bf61-547c4489c2e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------ ANTES DE INICIAR O TREINAMENTO ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 595/595 [00:01<00:00, 422.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN]. Loss: 8.03. Perplexidade: 3078.86\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 148/148 [00:00<00:00, 542.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL]. Loss: 8.03. Perplexidade: 3076.87\n",
            "\n",
            "------------------ [ÉPOCA 1/10] ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinando modelo: 100%|██████████| 595/595 [00:01<00:00, 303.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN ESTIMATIVA]. Loss: 5.44. Perplexidade: 229.76\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 148/148 [00:00<00:00, 758.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL]. Loss: 4.95. Perplexidade: 140.59\n",
            "\n",
            "------------------ [ÉPOCA 2/10] ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinando modelo: 100%|██████████| 595/595 [00:01<00:00, 349.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN ESTIMATIVA]. Loss: 4.96. Perplexidade: 142.39\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 148/148 [00:00<00:00, 764.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL]. Loss: 4.76. Perplexidade: 116.43\n",
            "\n",
            "------------------ [ÉPOCA 3/10] ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinando modelo: 100%|██████████| 595/595 [00:01<00:00, 346.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN ESTIMATIVA]. Loss: 4.77. Perplexidade: 118.33\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 148/148 [00:00<00:00, 800.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL]. Loss: 4.67. Perplexidade: 106.65\n",
            "\n",
            "------------------ [ÉPOCA 4/10] ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinando modelo: 100%|██████████| 595/595 [00:01<00:00, 349.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN ESTIMATIVA]. Loss: 4.66. Perplexidade: 105.57\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 148/148 [00:00<00:00, 773.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL]. Loss: 4.62. Perplexidade: 101.52\n",
            "\n",
            "------------------ [ÉPOCA 5/10] ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinando modelo: 100%|██████████| 595/595 [00:02<00:00, 294.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN ESTIMATIVA]. Loss: 4.57. Perplexidade: 96.75\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 148/148 [00:00<00:00, 468.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL]. Loss: 4.60. Perplexidade: 99.39\n",
            "\n",
            "------------------ [ÉPOCA 6/10] ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinando modelo: 100%|██████████| 595/595 [00:01<00:00, 319.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN ESTIMATIVA]. Loss: 4.50. Perplexidade: 89.73\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 148/148 [00:00<00:00, 756.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL]. Loss: 4.61. Perplexidade: 100.48\n",
            "\n",
            "------------------ [ÉPOCA 7/10] ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinando modelo: 100%|██████████| 595/595 [00:01<00:00, 351.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN ESTIMATIVA]. Loss: 4.43. Perplexidade: 84.33\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 148/148 [00:00<00:00, 772.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL]. Loss: 4.59. Perplexidade: 98.96\n",
            "\n",
            "------------------ [ÉPOCA 8/10] ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinando modelo: 100%|██████████| 595/595 [00:01<00:00, 353.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN ESTIMATIVA]. Loss: 4.38. Perplexidade: 79.51\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 148/148 [00:00<00:00, 750.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL]. Loss: 4.60. Perplexidade: 99.87\n",
            "\n",
            "------------------ [ÉPOCA 9/10] ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinando modelo: 100%|██████████| 595/595 [00:01<00:00, 353.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN ESTIMATIVA]. Loss: 4.33. Perplexidade: 76.09\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 148/148 [00:00<00:00, 706.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL]. Loss: 4.64. Perplexidade: 103.65\n",
            "\n",
            "------------------ [ÉPOCA 10/10] ------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinando modelo: 100%|██████████| 595/595 [00:01<00:00, 356.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN ESTIMATIVA]. Loss: 4.28. Perplexidade: 72.60\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando loss e perplexidade: 100%|██████████| 148/148 [00:00<00:00, 731.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL]. Loss: 4.64. Perplexidade: 103.15\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação"
      ],
      "metadata": {
        "id": "PSXfwYISDoPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recupera_modelo(model, epoca):\n",
        "  # Recupera o modelo salvo na época x\n",
        "  checkpoint_path = f\"modelo_epoca_{epoca}.pth\"\n",
        "  # Carregar o estado do checkpoint\n",
        "  checkpoint = torch.load(checkpoint_path)\n",
        "  # Aplicar o estado do modelo e otimizador carregados\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "id": "nXXO78GSDqPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Completa frases\n",
        "def completa_frase_do_conjunto(context_size, paragrafos, idx_para_completar, epoca_modelo_salvo, descartar_ukn=False):\n",
        "  for i in idx_para_completar:\n",
        "    frase_esperada = paragrafos[i]\n",
        "    palavras_na_frase = frase_esperada.split(' ')\n",
        "    palavras_na_frase = palavras_na_frase[0:context_size]\n",
        "\n",
        "    if len(palavras_na_frase) == context_size:\n",
        "      frase = ' '.join(palavras_na_frase)\n",
        "      recupera_modelo(model, epoca_modelo_salvo)\n",
        "      print('-----------------------------------------------------------------')\n",
        "      print(f'Testando para o índice {i}')\n",
        "      print(f'Modelo da epoca {epoca_modelo_salvo}:')\n",
        "      print('Início:  ', frase)\n",
        "      print('Correta: ', frase_esperada)\n",
        "      print('Gerada:  ', escrever_frase(model, vocab, most_frequent_words, frase, context_size, 30, descartar_ukn=descartar_ukn))"
      ],
      "metadata": {
        "id": "xvBP7-qPVPpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Avaliação no conjunto de treinamento"
      ],
      "metadata": {
        "id": "tIqMS6sUV9pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver como ficou NO CONJUNTO DE TREINAMENTO depois de rodar por muitas épocas (overfit no modelo)\n",
        "completa_frase_do_conjunto(context_size, train_paragrafos, [0, 1, 2, 3, 55, 61, 76, 78, 388, 555, 1000], epoca_modelo_salvo=num_epochs, descartar_ukn=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz-WXyLdV_WZ",
        "outputId": "63e1e3d5-bf82-4859-d5ad-d4cb54a52b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------\n",
            "Testando para o índice 0\n",
            "Modelo da epoca 10:\n",
            "Início:   --Não; estou tão bem aqui! Não foste tu que\n",
            "Correta:  --Não; estou tão bem aqui! Não foste tu que me trouxeste?\n",
            "Gerada:   --Não; estou tão bem aqui! Não foste tu que não ha pouco tendes , e a que o indio que o indio de sua senhora , que não se tivesse de sua senhora . não vos se não te\n",
            "-----------------------------------------------------------------\n",
            "Testando para o índice 1\n",
            "Modelo da epoca 10:\n",
            "Início:   --Como vos illudis! Quem o julgará criminoso? Vós? Pois\n",
            "Correta:  --Como vos illudis! Quem o julgará criminoso? Vós? Pois bem; outros o julgarão innocente e o defenderão; e não tereis remedio senão curvar a cabeça e calar-vos.\n",
            "Gerada:   --Como vos illudis! Quem o julgará criminoso? Vós? Pois que o indio que o indio que o indio que o indio que o indio que o indio que o indio que o indio que o indio que o indio\n",
            "-----------------------------------------------------------------\n",
            "Testando para o índice 2\n",
            "Modelo da epoca 10:\n",
            "Início:   Nem um gemido escapou da massa inerte que se\n",
            "Correta:  Nem um gemido escapou da massa inerte que se estorceu um momento e quedou de encontro ao muro.\n",
            "Gerada:   Nem um gemido escapou da massa inerte que se passava o braço de sua prima . antonio de mariz que o indio que o indio de sua senhora , que não se tivesse de sua senhora . não vos\n",
            "-----------------------------------------------------------------\n",
            "Testando para o índice 3\n",
            "Modelo da epoca 10:\n",
            "Início:   A menina tirou do peito uma pequena cruz de\n",
            "Correta:  A menina tirou do peito uma pequena cruz de ouro presa a uma fita preta, e deitou-a no pescoço do indio:\n",
            "Gerada:   A menina tirou do peito uma pequena cruz de sua senhora , e a sua vida , e a sua coragem , e a sua coragem , e a sua coragem , e a sua coragem , e a\n",
            "-----------------------------------------------------------------\n",
            "Testando para o índice 55\n",
            "Modelo da epoca 10:\n",
            "Início:   Todas as pessoas reunidas na esplanada sentião mais ou\n",
            "Correta:  Todas as pessoas reunidas na esplanada sentião mais ou menos a impressão poderosa desta hora solemne, e cedião involuntariamente a esse sentimento vago, que não é bem tristeza, mas respeito misturado de um certo temor.\n",
            "Gerada:   Todas as pessoas reunidas na esplanada sentião mais ou , e não se animou , e que o indio que o indio que o indio que o indio que o indio que o indio que o indio que o\n",
            "-----------------------------------------------------------------\n",
            "Testando para o índice 61\n",
            "Modelo da epoca 10:\n",
            "Início:   --Ide, Ruy, e fallai á guela despregada para vêr\n",
            "Correta:  --Ide, Ruy, e fallai á guela despregada para vêr se Loredano ouve uma palavra sequer.\n",
            "Gerada:   --Ide, Ruy, e fallai á guela despregada para vêr a sua vida , e a sua vez . a sua coragem , e a sua alma , e a sua coragem , e a sua coragem , e a\n",
            "-----------------------------------------------------------------\n",
            "Testando para o índice 76\n",
            "Modelo da epoca 10:\n",
            "Início:   Alvaro fascinado a admirava; nunca a vira tão bella;\n",
            "Correta:  Alvaro fascinado a admirava; nunca a vira tão bella; o moreno suave do rosto e do collo da moça illuminava-se de reflexos doces e tinha ondulações tão suaves, que o pensamento ia, sem querer, enleiar-se nas curvas graciosas como para sentir-lhe o contacto, espreguiçar-se pelas fórmas palpitantes.\n",
            "Gerada:   Alvaro fascinado a admirava; nunca a vira tão bella; e a sua alma , e a sua coragem , e a sua coragem , e a sua coragem , e a sua coragem , e a sua coragem ,\n",
            "-----------------------------------------------------------------\n",
            "Testando para o índice 388\n",
            "Modelo da epoca 10:\n",
            "Início:   Erguendo-se, apinhou em botão de rosa os labios vermelhos\n",
            "Correta:  Erguendo-se, apinhou em botão de rosa os labios vermelhos e imitou com uma graça encantadora os arrulhos doces da jurity; immediatamente a rola saltou dos galhos da acacia, e veio aninhar-se no seu seio, estremecendo de prazer ao contacto da mãozinha que alisava a sua penugem macia.\n",
            "Gerada:   Erguendo-se, apinhou em botão de rosa os labios vermelhos e de sua senhora . lisboa - se de sua senhora , e não se animou , e não vos o indio que se tratava o indio que o indio\n",
            "-----------------------------------------------------------------\n",
            "Testando para o índice 555\n",
            "Modelo da epoca 10:\n",
            "Início:   Alvaro ergueu-se como se os labios da moça tivessem\n",
            "Correta:  Alvaro ergueu-se como se os labios da moça tivessem lançado nas suas veias uma gota do veneno subtil dos selvagens que matava com um atomo.\n",
            "Gerada:   Alvaro ergueu-se como se os labios da moça tivessem . não se mais que o que não vos te que o indio . que o indio que o cavalheiro que o indio que o indio que o indio que\n",
            "-----------------------------------------------------------------\n",
            "Testando para o índice 1000\n",
            "Modelo da epoca 10:\n",
            "Início:   Sabia a violencia e o effeito prompto daquella arma\n",
            "Correta:  Sabia a violencia e o effeito prompto daquella arma que seu pai lhe confiára na hora da morte; sabia que bastava uma pequena parcella desse pó subtil para destruir em algumas horas a organisação a mais forte e a mais robusta. O indio resolveu pois usar desse poder que na sua mão heroica ia tornar-se um instrumento de salvação, e o agente de um sacrificio tremendo feito á amizade.\n",
            "Gerada:   Sabia a violencia e o effeito prompto daquella arma que o indio que o indio que o indio que o indio que o indio que o indio que o indio que o indio que o indio que o indio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Avaliação no conjunto de teste"
      ],
      "metadata": {
        "id": "2y8IXpsxWAhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoca_melhor_modelo = 7\n",
        "\n",
        "# Ver como ficou NO CONJUNTO DE TREINAMENTO\n",
        "completa_frase_do_conjunto(context_size, val_paragrafos, range(100,110), epoca_modelo_salvo=epoca_melhor_modelo, descartar_ukn=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVHcp4IVWHjP",
        "outputId": "25a42aae-71d7-4a94-cbc2-25de530cdcd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------\n",
            "Testando para o índice 100\n",
            "Modelo da epoca 7:\n",
            "Início:   --Aqui me tendes! repetio o cavalheiro. Dizei o que\n",
            "Correta:  --Aqui me tendes! repetio o cavalheiro. Dizei o que quereis de D. Antonio de Mariz, e dizei-o claro e breve. Se fôr de justiça, sereis satisfeitos; se fôr uma falta, tereis a punição que merecerdes.\n",
            "Gerada:   --Aqui me tendes! repetio o cavalheiro. Dizei o que se não se o seu amor . antonio o seu amor ; o seu amor , e a sua vida , a sua vida , e a sua vida ,\n",
            "-----------------------------------------------------------------\n",
            "Testando para o índice 101\n",
            "Modelo da epoca 7:\n",
            "Início:   --Então devo tirar o meu; já não estamos irmãs.\n",
            "Correta:  --Então devo tirar o meu; já não estamos irmãs.\n",
            "Gerada:   --Então devo tirar o meu; já não estamos irmãs. que o seu amor , não o que o seu amor , e o seu amor ; o italiano que o seu amor , e a sua vida , a\n",
            "-----------------------------------------------------------------\n",
            "Testando para o índice 102\n",
            "Modelo da epoca 7:\n",
            "Início:   O italiano fez um esforço supremo, e passando a\n",
            "Correta:  O italiano fez um esforço supremo, e passando a mão pelos olhos como para arrancar uma visão importuna, encaminhou-se a um bofete e acendeu uma vela de cera côr de rosa.\n",
            "Gerada:   O italiano fez um esforço supremo, e passando a sua mãi , e a sua vida , e a sua vida , a sua vida , e a sua vida , a sua vida , e a sua vida\n",
            "-----------------------------------------------------------------\n",
            "Testando para o índice 103\n",
            "Modelo da epoca 7:\n",
            "Início:   --Néscios!... disse o italiano cobrindo-os com um olhar de\n",
            "Correta:  --Néscios!... disse o italiano cobrindo-os com um olhar de desprezo e de piedade ao mesmo tempo. Não vedes que quando um homem traz um segredo como o meu, a menos que esse homem não seja um truão da vossa laia, elle deve ter tomado as suas precauções contra estes pequenos incidentes!\n",
            "Gerada:   --Néscios!... disse o italiano cobrindo-os com um olhar de sua mãi , e a sua vida , e a sua vida , a sua vida , e a sua vida , a sua vida , e a sua vida\n",
            "-----------------------------------------------------------------\n",
            "Testando para o índice 104\n",
            "Modelo da epoca 7:\n",
            "Início:   Dentro de um segundo a frialdade acordaria todos os\n",
            "Correta:  Dentro de um segundo a frialdade acordaria todos os homens adormecidos, e os obrigaria a sahir do alpendre; era o que Pery esperava.\n",
            "Gerada:   Dentro de um segundo a frialdade acordaria todos os seus companheiros , e a sua vida , o seu amor , e a sua vida , a sua vida , e a sua vida , a sua vida ,\n",
            "-----------------------------------------------------------------\n",
            "Testando para o índice 106\n",
            "Modelo da epoca 7:\n",
            "Início:   Duas vezes Cecilia viera até á porta, escutára e\n",
            "Correta:  Duas vezes Cecilia viera até á porta, escutára e nada ouvira; por fim resolveu-se a bater, a fallar a Isabel, e não teve a menor resposta. Chamou Pery e contou-lhe o que se passava; o indio, tomado de um presentimento metteu o hombro á porta e abrio-a.\n",
            "Gerada:   Duas vezes Cecilia viera até á porta, escutára e a sua mãi , e a sua vida , e a sua vida , a sua vida , e a sua vida , a sua vida , e a sua\n",
            "-----------------------------------------------------------------\n",
            "Testando para o índice 108\n",
            "Modelo da epoca 7:\n",
            "Início:   --Pery sabe porque viu o pai e o irmão\n",
            "Correta:  --Pery sabe porque viu o pai e o irmão da india, que teu filho matou sem querer, olharem tua casa de longe, soltarem o grito da vingança, e caminharem para sua tribu.\n",
            "Gerada:   --Pery sabe porque viu o pai e o irmão , e a sua mãi , e a sua vida , e a sua vida , a sua vida , e a sua vida , a sua vida , e\n",
            "-----------------------------------------------------------------\n",
            "Testando para o índice 109\n",
            "Modelo da epoca 7:\n",
            "Início:   Ahi, como no resto da casa, tudo estava calmo\n",
            "Correta:  Ahi, como no resto da casa, tudo estava calmo e tranquillo; apenas via-se luzir na soleira da porta do aposento de Ayres Gomes a claridade de uma luz.\n",
            "Gerada:   Ahi, como no resto da casa, tudo estava calmo , e o seu amor , e a sua vida , a sua vida , e a sua vida , a sua vida , e a sua vida , a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PExkoWOzJ_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f65100c-8b15-4f46-cdc8-cde585f447b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo da epoca 7:\n",
            "Se se tratasse de sua vida, Pery teria sangue\n",
            "Se se tratasse de sua vida, Pery teria sangue o seu amor , e o seu amor ; o italiano que o seu amor , e a sua vida , a sua vida , e a sua vida ,\n"
          ]
        }
      ],
      "source": [
        "recupera_modelo(model, epoca=epoca_melhor_modelo)\n",
        "\n",
        "frase = \"Se se tratasse de sua vida, Pery teria sangue\"\n",
        "print(f'Modelo da epoca {epoca_melhor_modelo}:')\n",
        "print(frase)\n",
        "print(escrever_frase(model, vocab, most_frequent_words, frase, context_size, 30, descartar_ukn=True))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}